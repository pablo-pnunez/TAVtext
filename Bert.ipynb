{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "token_dict = pd.DataFrame(tk_dbrt.vocab.items(), columns=[\"token\", \"id\"]).sort_values(\"id\").reset_index(drop=True).set_index(\"id\")\n",
    "\n",
    "texto = \"No Hola a todos\"\n",
    "tokens = tokenizer.tokenize(texto)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "token_dict.loc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.text_datasets.old.tripadvisor.W2Vdataset import W2Vdataset\n",
    "from src.models.text_models.W2V import W2V\n",
    "\n",
    "base_path = \"/media/nas/datasets/tripadvisor/restaurants/\"\n",
    "cities = [\"gijon\", \"barcelona\", \"madrid\"]\n",
    "\n",
    "remove_numbers = True\n",
    "remove_accents = True\n",
    "remove_plurals = False\n",
    "lemmatization = True\n",
    "stemming = False\n",
    "\n",
    "w2v_dimen = 128\n",
    "seed = 100\n",
    "gpu = 1\n",
    "\n",
    "w2v_dts = W2Vdataset({\"cities\": cities, \"city\": \"multi\", \"seed\": seed, \"data_path\": base_path, \"save_path\": \"data/\",  # base_path + \"Datasets/\",\n",
    "                        \"remove_plurals\": remove_plurals, \"stemming\": stemming, \"lemmatization\": lemmatization,\n",
    "                        \"remove_accents\": remove_accents, \"remove_numbers\": remove_numbers,\n",
    "                        }, load=[])\n",
    "w2v_dts.CONFIG[\"dataset\"]=\"multi\"\n",
    "w2v_dts.CONFIG[\"subset\"]=\"-\".join(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dts.DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mdl = W2V({\"model\": {\"train_set\": \"ALL_TEXTS\", \"min_count\": 100, \"window\": 5, \"n_dimensions\": w2v_dimen, \"seed\": seed},\n",
    "                \"session\": {\"gpu\": gpu, \"mixed_precision\": True, \"in_md5\": False}}, w2v_dts)\n",
    "\n",
    "w2v_mdl.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir los tokens\n",
    "for token, idt in tk_dbrt.vocab.items():\n",
    "    print(idt,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepESP/gpt2-spanish\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"DeepESP/gpt2-spanish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hola mundo\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.load( \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "\n",
    "# Step 1: tokenize batches of text inputs.\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) # This SavedModel accepts up to 2 text inputs.\n",
    "tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "tokenized_inputs = [tokenize(text_input)]\n",
    "\n",
    "# Step 2: pack input sequences for the Transformer encoder.\n",
    "seq_length = 200  # Your choice here.\n",
    "bert_pack_inputs = hub.KerasLayer(preprocessor.bert_pack_inputs, arguments=dict(seq_length=seq_length))  # Optional argument.\n",
    "encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "\n",
    "# Model\n",
    "encoder = hub.KerasLayer( \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\", trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 128].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 128]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = tf.keras.Model(text_input, sequence_output)\n",
    "sentences = tf.constant([\"Hello world\"])\n",
    "\n",
    "res = embedding_model(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy dependency parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a350f840731942e2bc4c48e4d0ed857e-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\"><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Pizza</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">with</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADP</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">cheese</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">but</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">CCONJ</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">no</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan></text><text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\"><tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">tomato.</tspan><tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan></text><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-a350f840731942e2bc4c48e4d0ed857e-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-a350f840731942e2bc4c48e4d0ed857e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath></text><path class=\"displacy-arrowhead\" d=\"M215.0,266.5 L223.0,254.5 207.0,254.5\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-a350f840731942e2bc4c48e4d0ed857e-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-a350f840731942e2bc4c48e4d0ed857e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath></text><path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-a350f840731942e2bc4c48e4d0ed857e-0-2\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-a350f840731942e2bc4c48e4d0ed857e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath></text><path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-a350f840731942e2bc4c48e4d0ed857e-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-a350f840731942e2bc4c48e4d0ed857e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath></text><path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/></g><g class=\"displacy-arrow\"><path class=\"displacy-arc\" id=\"arrow-a350f840731942e2bc4c48e4d0ed857e-0-4\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/><text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\"><textPath xlink:href=\"#arrow-a350f840731942e2bc4c48e4d0ed857e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath></text><path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/></g></svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frase de ejemplo\n",
    "sentence = \"Pizza with cheese but no tomato.\"\n",
    "\n",
    "# Procesar la frase con el modelo\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Generar el gráfico de dependencia\n",
    "displacy.render(doc, style='dep', jupyter=True, minify=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in doc:\n",
    "    if token.dep_ != 'ROOT':  # Ignorar la raíz del árbol de dependencias\n",
    "        head = token.head.text\n",
    "        relation = token.dep_\n",
    "        child = token.text\n",
    "        print(f'{head} --{relation}--> {child}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza\n",
      "--prep--> with\n",
      "--cc--> but\n",
      "--conj--> tomato\n",
      "--punct--> .\n",
      "\n",
      "with\n",
      "--pobj--> cheese\n",
      "\n",
      "tomato\n",
      "--det--> no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construir el diccionario de relaciones\n",
    "relations_dict = {}\n",
    "for token in doc:\n",
    "    if token.dep_ != 'ROOT':  # Ignorar la raíz del árbol de dependencias\n",
    "        head = token.head.text\n",
    "        relation = token.dep_\n",
    "        child = token.text\n",
    "        \n",
    "        # Agregar la relación al diccionario\n",
    "        if head in relations_dict:\n",
    "            relations_dict[head].append((relation, child))\n",
    "        else:\n",
    "            relations_dict[head] = [(relation, child)]\n",
    "\n",
    "# Imprimir el diccionario de relaciones\n",
    "for word, relations in relations_dict.items():\n",
    "    print(word)\n",
    "    for relation in relations:\n",
    "        rel, child = relation\n",
    "        print(f'--{rel}--> {child}')\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
