{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "models = [\"ATT2ITM\", \"BOW2ITM\", \"USEM2ITM\", \"BERT2ITM\"]\n",
    "models = [\"MOSTPOP2ITM\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar nuestros modelos solo en los casos de cold-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.Common import load_best_model\n",
    "import pandas as pd\n",
    "\n",
    "cold_start_res = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        for model in models:\n",
    "            # Cargar configuración mejor modelo\n",
    "            model_class = load_best_model(model=model, dataset=dataset, subset=subset)\n",
    "            # Cargar el modelo entrenado\n",
    "            model_class.train(dev=False, save_model=True)\n",
    "            # Eliminar usuarios que aparecen en TRAIN\n",
    "            train_dev_users = model_class.DATASET.DATA[\"TRAIN_DEV\"].userId.unique()\n",
    "            # original_test_len = len(model_class.DATASET.DATA[\"TEST\"])\n",
    "            model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"][~model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)]\n",
    "            model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"].drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False)\n",
    "            # Evaluar en el conjunto de test\n",
    "            test_ret = model_class.evaluate(test=True)\n",
    "            cold_start_res.append([dataset, subset, model]+test_ret.values.tolist()[0][1:]+[len(model_class.DATASET.DATA[\"TEST\"])])\n",
    "\n",
    "results = pd.DataFrame(cold_start_res, columns=[\"Set\", \"Subset\", \"Model\"] + test_ret.columns.tolist()[1:]+[\"cold_items\"])\n",
    "# cold_start_res.to_excel(\"cold_start_evaluation.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar tabla Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_excel(\"/media/nas/pperez/code/TAVtext/cold_start_evaluation.xlsx\")\n",
    "# Poner nombres decentes para el artículo\n",
    "results[\"Set\"].replace({\"restaurants\": \"TAV-RSTS\", \"pois\":\"TAV-POIS\", \"amazon\":\"AM\"}, inplace=True)\n",
    "results[\"Subset\"].replace({\"digital_music\": \"Music\", \"fashion\":\"Fashion\", \"gijon\": \"Gijón\", \"barcelona\":\"Barcelona\", \"madrid\": \"Madrid\", \"paris\": \"Paris\", \"newyorkcity\": \"New York\", \"london\": \"London\"}, inplace=True)\n",
    "results[\"Model\"].replace({\"USEM2ITM\":\"USEM\", \"BERT2ITM\":\"BERT\", \"BOW2ITM\":\"TRecX\", \"ATT2ITM\": \"AITRecX\", \"MOSTPOP2ITM\": \"M.Pop\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Recall@5\", \"Recall@10\", \"NDCG@10\"]\n",
    "models = ['TRecX', 'AITRecX', 'USEM', 'BERT', \"M.Pop\"]\n",
    "\n",
    "result_df = []\n",
    "\n",
    "for metric in metrics:\n",
    "    mres = results.pivot_table(index=[ \"Set\", \"Subset\"], columns=[\"Model\"])[metric].reset_index()\n",
    "    mres[\"Metric\"] = metric\n",
    "    result_df.extend(mres.values)\n",
    "\n",
    "result_df = pd.DataFrame(result_df, columns=mres.columns)\n",
    "\n",
    "# Especificar el orden de los datos\n",
    "result_df['Set'] = pd.Categorical(result_df['Set'], [\"AM\", \"TAV-POIS\", \"TAV-RSTS\"])\n",
    "result_df['Subset'] = pd.Categorical(result_df['Subset'], [\"Music\", \"Fashion\", \"Gijón\", \"Barcelona\", \"Madrid\", \"New York\", \"Paris\", \"London\"])\n",
    "result_df['Metric'] = pd.Categorical(result_df['Metric'], metrics)\n",
    "\n",
    "result_df = result_df.pivot_table(index=[\"Set\", \"Subset\", \"Metric\"])\n",
    "print(result_df[models].to_latex())\n",
    "result_df.to_excel(\"cold_start_results.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas de evolución de nuestros modelos y baselines en función del usuarios en Train\n",
    "No parece verse mejora clara en los baselines, por tanto no se mostrarán gráficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "top = 5\n",
    "cumulate = True\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "datasets = {\"restaurants\":[\"barcelona\"]}\n",
    "\n",
    "models = [\"ATT2ITM\", \"BOW2ITM\", \"USEM2ITM\"]\n",
    "\n",
    "best_model = pd.read_csv(\"models/best_models.csv\")\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax2 = ax.twinx()\n",
    "        subset_results = pd.DataFrame(range(top+1), columns=[\"train_dev_items\"])\n",
    "        \n",
    "        baseline_path = f\"models/Baselines/{dataset}/{subset}/user_results.csv\"\n",
    "        if os.path.exists(baseline_path): \n",
    "            baselines = pd.read_csv(baseline_path).rename(columns={\"cold\":\"train_dev_items\"})\n",
    "            baselines_names = baselines.columns.to_list(); baselines_names.remove(\"userId\"); baselines_names.remove(\"train_dev_items\")\n",
    "            baselines = baselines.groupby(\"train_dev_items\")[baselines_names].mean().reset_index()\n",
    "            subset_results = subset_results.merge(baselines, how=\"left\")\n",
    "\n",
    "            for bl in baselines_names: # [\"GridSearch_BPR\", \"GridSearch_EASEᴿ\", \"MostPop\"]: \n",
    "                if cumulate: subset_results[bl] = subset_results[bl].expanding().mean()\n",
    "                ax.plot(subset_results[\"train_dev_items\"], subset_results[bl], \"--\", label=bl)\n",
    "\n",
    "        else: raise ValueError\n",
    "\n",
    "        for model in models:\n",
    "            md5 = best_model.loc[(best_model.dataset == dataset) & (best_model.subset == subset) & (best_model.model == model)][\"model_md5\"].values[0]\n",
    "            results_path = f\"models/{model}/{dataset}/{subset}/{md5}/final_user_eval.csv\"\n",
    "            if os.path.exists(results_path):\n",
    "                results = pd.read_csv(results_path)\n",
    "                results = results.groupby(\"cold\").agg(mn=(\"NDCG\", \"mean\"), n_samples=(\"NDCG\", \"count\")).reset_index().rename(columns={\"cold\":\"train_dev_items\", \"mn\":model})\n",
    "                if cumulate: results[model] = results[model].expanding().mean()\n",
    "                subset_results = subset_results.merge(results, how=\"left\")\n",
    "                ax.plot(subset_results[\"train_dev_items\"], subset_results[model], \"-\", label=model)\n",
    "            else:\n",
    "                raise ValueError\n",
    "        \n",
    "        subset_results[\"n_samples\"] = subset_results[\"n_samples\"].fillna(0)\n",
    "\n",
    "        ax2.plot(subset_results[\"train_dev_items\"], subset_results[\"n_samples\"], \":\",label=\"\")\n",
    "\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax2.set_ylim([0, subset_results[\"n_samples\"].max()])\n",
    "        ax.set_ylabel('AVG NDCG')\n",
    "        plt.title(f\"{dataset.title()} - {subset.title()}\")\n",
    "        ax.set_xlabel(\"Number of reviews in Train set\")\n",
    "        # ax.set_xticks(range(top+1))\n",
    "        # ax.set_xlim([0, top])\n",
    "        ax2.set_ylabel('Number of Test cases')\n",
    "\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
