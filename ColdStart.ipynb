{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "models = [\"ATT2ITM\", \"BOW2ITM\", \"USEM2ITM\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar nuestros modelos solo en los casos de cold-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.Common import load_best_model\n",
    "import pandas as pd\n",
    "\n",
    "cold_start_res = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        for model in models:\n",
    "            # Cargar el modelo con mejores hiperparámetros\n",
    "            model_class = load_best_model(model=model, dataset=dataset, subset=subset)\n",
    "            model_class.train(dev=True, save_model=True) # Cargar el modelo\n",
    "            # Eliminar usuarios que aparecen en TRAIN\n",
    "            train_dev_users = model_class.DATASET.DATA[\"TRAIN_DEV\"].userId.unique()\n",
    "            # original_test_len = len(model_class.DATASET.DATA[\"TEST\"])\n",
    "            model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"][~model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)]\n",
    "            model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"].drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False)\n",
    "            # Evaluar en el conjunto de test\n",
    "            test_ret = model_class.evaluate(test=True)\n",
    "            cold_start_res.append([dataset, subset, model]+test_ret.values.tolist()[0][1:]+[len(model_class.DATASET.DATA[\"TEST\"])])\n",
    "\n",
    "cold_start_res = pd.DataFrame(cold_start_res, columns=[\"dataset\", \"subset\", \"model\"] + test_ret.columns.tolist()[1:]+[\"cold_items\"])\n",
    "cold_start_res.to_excel(\"cold_start_evaluation.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas de evolución de nuestros modelos y baselines en función del usuarios en Train\n",
    "No parece verse mejora clara en los baselines, por tanto no se mostrarán gráficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "top = 5\n",
    "cumulate = True\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "datasets = {\"restaurants\":[\"barcelona\"]}\n",
    "\n",
    "models = [\"ATT2ITM\", \"BOW2ITM\", \"USEM2ITM\"]\n",
    "\n",
    "best_model = pd.read_csv(\"models/best_models.csv\")\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax2 = ax.twinx()\n",
    "        subset_results = pd.DataFrame(range(top+1), columns=[\"train_dev_items\"])\n",
    "        \n",
    "        baseline_path = f\"models/Baselines/{dataset}/{subset}/user_results.csv\"\n",
    "        if os.path.exists(baseline_path): \n",
    "            baselines = pd.read_csv(baseline_path).rename(columns={\"cold\":\"train_dev_items\"})\n",
    "            baselines_names = baselines.columns.to_list(); baselines_names.remove(\"userId\"); baselines_names.remove(\"train_dev_items\")\n",
    "            baselines = baselines.groupby(\"train_dev_items\")[baselines_names].mean().reset_index()\n",
    "            subset_results = subset_results.merge(baselines, how=\"left\")\n",
    "\n",
    "            for bl in baselines_names: # [\"GridSearch_BPR\", \"GridSearch_EASEᴿ\", \"MostPop\"]: \n",
    "                if cumulate: subset_results[bl] = subset_results[bl].expanding().mean()\n",
    "                ax.plot(subset_results[\"train_dev_items\"], subset_results[bl], \"--\", label=bl)\n",
    "\n",
    "        else: raise ValueError\n",
    "\n",
    "        for model in models:\n",
    "            md5 = best_model.loc[(best_model.dataset == dataset) & (best_model.subset == subset) & (best_model.model == model)][\"model_md5\"].values[0]\n",
    "            results_path = f\"models/{model}/{dataset}/{subset}/{md5}/final_user_eval.csv\"\n",
    "            if os.path.exists(results_path):\n",
    "                results = pd.read_csv(results_path)\n",
    "                results = results.groupby(\"cold\").agg(mn=(\"NDCG\", \"mean\"), n_samples=(\"NDCG\", \"count\")).reset_index().rename(columns={\"cold\":\"train_dev_items\", \"mn\":model})\n",
    "                if cumulate: results[model] = results[model].expanding().mean()\n",
    "                subset_results = subset_results.merge(results, how=\"left\")\n",
    "                ax.plot(subset_results[\"train_dev_items\"], subset_results[model], \"-\", label=model)\n",
    "            else:\n",
    "                raise ValueError\n",
    "        \n",
    "        subset_results[\"n_samples\"] = subset_results[\"n_samples\"].fillna(0)\n",
    "\n",
    "        ax2.plot(subset_results[\"train_dev_items\"], subset_results[\"n_samples\"], \":\",label=\"\")\n",
    "\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax2.set_ylim([0, subset_results[\"n_samples\"].max()])\n",
    "        ax.set_ylabel('AVG NDCG')\n",
    "        plt.title(f\"{dataset.title()} - {subset.title()}\")\n",
    "        ax.set_xlabel(\"Number of reviews in Train set\")\n",
    "        # ax.set_xticks(range(top+1))\n",
    "        # ax.set_xlim([0, top])\n",
    "        ax2.set_ylabel('Number of Test cases')\n",
    "\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
