Model: "ATT2ITM_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 172)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1322)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 172, 384)     3389952     ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 1322, 384)    507648      ['input_2[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 172, 256)     98560       ['embedding[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 1322, 256)    98560       ['in_rsts[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 172)         0           ['input_1[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 172, 128)     32896       ['dense[0][0]']                  
                                                                                                  
 dense_3 (Dense)                (None, 1322, 128)    32896       ['dense_2[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 172)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 172, 128)     0           ['dense_1[0][0]']                
                                                                                                  
 rest_emb (Lambda)              (None, 1322, 128)    0           ['dense_3[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 172, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 172, 1322)    0           ['word_emb[0][0]',               
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 172, 1322)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 172, 1322)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 172, 1322)    0           ['lambda_1[0][0]']               
                                                                                                  
 dropout (Dropout)              (None, 172, 1322)    0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 1322)         0           ['dropout[0][0]']                
                                                                                                  
 out (Activation)               (None, 1322)         0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 4,160,512
Trainable params: 4,160,512
Non-trainable params: 0
__________________________________________________________________________________________________
None
Epoch 1/1000
502/502 - 27s - loss: 0.0059 - r1: 0.0058 - r5: 0.0207 - r10: 0.0352 - p5: 0.0041 - p10: 0.0035 - val_loss: 0.0037 - val_r1: 0.0141 - val_r5: 0.0390 - val_r10: 0.0584 - val_p5: 0.0078 - val_p10: 0.0058 - lr: 4.9951e-04 - 27s/epoch - 53ms/step
Epoch 2/1000
502/502 - 25s - loss: 0.0037 - r1: 0.0313 - r5: 0.0760 - r10: 0.1068 - p5: 0.0152 - p10: 0.0107 - val_loss: 0.0032 - val_r1: 0.0561 - val_r5: 0.1414 - val_r10: 0.1977 - val_p5: 0.0282 - val_p10: 0.0198 - lr: 4.9901e-04 - 25s/epoch - 49ms/step
Epoch 3/1000
502/502 - 25s - loss: 0.0033 - r1: 0.0753 - r5: 0.1609 - r10: 0.2123 - p5: 0.0322 - p10: 0.0212 - val_loss: 0.0030 - val_r1: 0.0872 - val_r5: 0.2025 - val_r10: 0.2725 - val_p5: 0.0405 - val_p10: 0.0273 - lr: 4.9852e-04 - 25s/epoch - 49ms/step
Epoch 4/1000
502/502 - 25s - loss: 0.0031 - r1: 0.1045 - r5: 0.2126 - r10: 0.2744 - p5: 0.0425 - p10: 0.0274 - val_loss: 0.0028 - val_r1: 0.1353 - val_r5: 0.2686 - val_r10: 0.3487 - val_p5: 0.0538 - val_p10: 0.0349 - lr: 4.9802e-04 - 25s/epoch - 49ms/step
Epoch 5/1000
502/502 - 25s - loss: 0.0030 - r1: 0.1260 - r5: 0.2477 - r10: 0.3132 - p5: 0.0495 - p10: 0.0313 - val_loss: 0.0028 - val_r1: 0.1371 - val_r5: 0.2779 - val_r10: 0.3545 - val_p5: 0.0556 - val_p10: 0.0355 - lr: 4.9753e-04 - 25s/epoch - 49ms/step
Epoch 6/1000
502/502 - 25s - loss: 0.0029 - r1: 0.1412 - r5: 0.2711 - r10: 0.3395 - p5: 0.0542 - p10: 0.0340 - val_loss: 0.0028 - val_r1: 0.1556 - val_r5: 0.3051 - val_r10: 0.3859 - val_p5: 0.0610 - val_p10: 0.0386 - lr: 4.9703e-04 - 25s/epoch - 49ms/step
Epoch 7/1000
502/502 - 25s - loss: 0.0028 - r1: 0.1545 - r5: 0.2907 - r10: 0.3613 - p5: 0.0581 - p10: 0.0361 - val_loss: 0.0027 - val_r1: 0.1548 - val_r5: 0.3067 - val_r10: 0.3901 - val_p5: 0.0614 - val_p10: 0.0390 - lr: 4.9654e-04 - 25s/epoch - 49ms/step
Epoch 8/1000
502/502 - 25s - loss: 0.0027 - r1: 0.1650 - r5: 0.3049 - r10: 0.3757 - p5: 0.0610 - p10: 0.0376 - val_loss: 0.0027 - val_r1: 0.1837 - val_r5: 0.3462 - val_r10: 0.4283 - val_p5: 0.0692 - val_p10: 0.0428 - lr: 4.9604e-04 - 25s/epoch - 49ms/step
Epoch 9/1000
502/502 - 25s - loss: 0.0027 - r1: 0.1739 - r5: 0.3185 - r10: 0.3903 - p5: 0.0637 - p10: 0.0390 - val_loss: 0.0026 - val_r1: 0.1897 - val_r5: 0.3564 - val_r10: 0.4408 - val_p5: 0.0713 - val_p10: 0.0441 - lr: 4.9555e-04 - 25s/epoch - 49ms/step
Epoch 10/1000
502/502 - 24s - loss: 0.0027 - r1: 0.1815 - r5: 0.3283 - r10: 0.4011 - p5: 0.0657 - p10: 0.0401 - val_loss: 0.0027 - val_r1: 0.1906 - val_r5: 0.3540 - val_r10: 0.4353 - val_p5: 0.0708 - val_p10: 0.0435 - lr: 4.9505e-04 - 24s/epoch - 48ms/step
Epoch 11/1000
502/502 - 25s - loss: 0.0026 - r1: 0.1889 - r5: 0.3388 - r10: 0.4120 - p5: 0.0678 - p10: 0.0412 - val_loss: 0.0026 - val_r1: 0.1949 - val_r5: 0.3551 - val_r10: 0.4327 - val_p5: 0.0710 - val_p10: 0.0433 - lr: 4.9456e-04 - 25s/epoch - 49ms/step
Epoch 12/1000
502/502 - 24s - loss: 0.0026 - r1: 0.1948 - r5: 0.3466 - r10: 0.4202 - p5: 0.0693 - p10: 0.0420 - val_loss: 0.0027 - val_r1: 0.1927 - val_r5: 0.3518 - val_r10: 0.4308 - val_p5: 0.0703 - val_p10: 0.0431 - lr: 4.9406e-04 - 24s/epoch - 48ms/step
Epoch 13/1000
502/502 - 25s - loss: 0.0025 - r1: 0.1996 - r5: 0.3529 - r10: 0.4271 - p5: 0.0706 - p10: 0.0427 - val_loss: 0.0026 - val_r1: 0.2167 - val_r5: 0.3839 - val_r10: 0.4659 - val_p5: 0.0768 - val_p10: 0.0466 - lr: 4.9357e-04 - 25s/epoch - 49ms/step
Epoch 14/1000
502/502 - 24s - loss: 0.0025 - r1: 0.2049 - r5: 0.3595 - r10: 0.4327 - p5: 0.0719 - p10: 0.0433 - val_loss: 0.0026 - val_r1: 0.2091 - val_r5: 0.3753 - val_r10: 0.4548 - val_p5: 0.0751 - val_p10: 0.0455 - lr: 4.9307e-04 - 24s/epoch - 48ms/step
Epoch 15/1000
502/502 - 24s - loss: 0.0025 - r1: 0.2102 - r5: 0.3670 - r10: 0.4422 - p5: 0.0734 - p10: 0.0442 - val_loss: 0.0027 - val_r1: 0.2200 - val_r5: 0.3813 - val_r10: 0.4613 - val_p5: 0.0763 - val_p10: 0.0461 - lr: 4.9258e-04 - 24s/epoch - 48ms/step
Epoch 16/1000
502/502 - 24s - loss: 0.0025 - r1: 0.2139 - r5: 0.3718 - r10: 0.4475 - p5: 0.0744 - p10: 0.0448 - val_loss: 0.0027 - val_r1: 0.2186 - val_r5: 0.3805 - val_r10: 0.4600 - val_p5: 0.0761 - val_p10: 0.0460 - lr: 4.9208e-04 - 24s/epoch - 48ms/step
Epoch 17/1000
502/502 - 24s - loss: 0.0025 - r1: 0.2173 - r5: 0.3765 - r10: 0.4516 - p5: 0.0753 - p10: 0.0452 - val_loss: 0.0027 - val_r1: 0.2311 - val_r5: 0.3950 - val_r10: 0.4743 - val_p5: 0.0790 - val_p10: 0.0474 - lr: 4.9159e-04 - 24s/epoch - 48ms/step
Epoch 18/1000
502/502 - 24s - loss: 0.0024 - r1: 0.2205 - r5: 0.3814 - r10: 0.4576 - p5: 0.0763 - p10: 0.0458 - val_loss: 0.0026 - val_r1: 0.2290 - val_r5: 0.3928 - val_r10: 0.4704 - val_p5: 0.0786 - val_p10: 0.0470 - lr: 4.9109e-04 - 24s/epoch - 48ms/step
Epoch 19/1000
502/502 - 25s - loss: 0.0024 - r1: 0.2241 - r5: 0.3863 - r10: 0.4615 - p5: 0.0773 - p10: 0.0461 - val_loss: 0.0026 - val_r1: 0.2324 - val_r5: 0.4027 - val_r10: 0.4827 - val_p5: 0.0806 - val_p10: 0.0483 - lr: 4.9060e-04 - 25s/epoch - 49ms/step
Epoch 20/1000
502/502 - 24s - loss: 0.0024 - r1: 0.2278 - r5: 0.3909 - r10: 0.4669 - p5: 0.0782 - p10: 0.0467 - val_loss: 0.0027 - val_r1: 0.2334 - val_r5: 0.3982 - val_r10: 0.4792 - val_p5: 0.0796 - val_p10: 0.0479 - lr: 4.9010e-04 - 24s/epoch - 48ms/step
Epoch 21/1000
502/502 - 25s - loss: 0.0024 - r1: 0.2299 - r5: 0.3944 - r10: 0.4705 - p5: 0.0789 - p10: 0.0471 - val_loss: 0.0026 - val_r1: 0.2422 - val_r5: 0.4134 - val_r10: 0.4956 - val_p5: 0.0827 - val_p10: 0.0496 - lr: 4.8961e-04 - 25s/epoch - 49ms/step
Epoch 22/1000
502/502 - 24s - loss: 0.0024 - r1: 0.2332 - r5: 0.3983 - r10: 0.4738 - p5: 0.0797 - p10: 0.0474 - val_loss: 0.0027 - val_r1: 0.2428 - val_r5: 0.4103 - val_r10: 0.4905 - val_p5: 0.0821 - val_p10: 0.0491 - lr: 4.8911e-04 - 24s/epoch - 48ms/step
Epoch 23/1000
502/502 - 24s - loss: 0.0024 - r1: 0.2354 - r5: 0.4017 - r10: 0.4781 - p5: 0.0803 - p10: 0.0478 - val_loss: 0.0027 - val_r1: 0.2363 - val_r5: 0.3981 - val_r10: 0.4777 - val_p5: 0.0796 - val_p10: 0.0478 - lr: 4.8862e-04 - 24s/epoch - 48ms/step
Epoch 24/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2385 - r5: 0.4045 - r10: 0.4811 - p5: 0.0809 - p10: 0.0481 - val_loss: 0.0027 - val_r1: 0.2336 - val_r5: 0.3974 - val_r10: 0.4771 - val_p5: 0.0795 - val_p10: 0.0477 - lr: 4.8812e-04 - 24s/epoch - 48ms/step
Epoch 25/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2412 - r5: 0.4087 - r10: 0.4847 - p5: 0.0817 - p10: 0.0485 - val_loss: 0.0027 - val_r1: 0.2323 - val_r5: 0.3935 - val_r10: 0.4728 - val_p5: 0.0787 - val_p10: 0.0473 - lr: 4.8763e-04 - 24s/epoch - 48ms/step
Epoch 26/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2419 - r5: 0.4093 - r10: 0.4862 - p5: 0.0819 - p10: 0.0486 - val_loss: 0.0026 - val_r1: 0.2480 - val_r5: 0.4160 - val_r10: 0.4977 - val_p5: 0.0832 - val_p10: 0.0498 - lr: 4.8713e-04 - 24s/epoch - 48ms/step
Epoch 27/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2444 - r5: 0.4133 - r10: 0.4897 - p5: 0.0827 - p10: 0.0490 - val_loss: 0.0027 - val_r1: 0.2455 - val_r5: 0.4119 - val_r10: 0.4927 - val_p5: 0.0824 - val_p10: 0.0493 - lr: 4.8664e-04 - 24s/epoch - 48ms/step
Epoch 28/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2465 - r5: 0.4156 - r10: 0.4930 - p5: 0.0831 - p10: 0.0493 - val_loss: 0.0026 - val_r1: 0.2500 - val_r5: 0.4203 - val_r10: 0.4991 - val_p5: 0.0841 - val_p10: 0.0499 - lr: 4.8614e-04 - 24s/epoch - 48ms/step
Epoch 29/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2493 - r5: 0.4189 - r10: 0.4956 - p5: 0.0838 - p10: 0.0496 - val_loss: 0.0027 - val_r1: 0.2441 - val_r5: 0.4159 - val_r10: 0.4951 - val_p5: 0.0832 - val_p10: 0.0495 - lr: 4.8565e-04 - 24s/epoch - 48ms/step
Epoch 30/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2514 - r5: 0.4219 - r10: 0.4989 - p5: 0.0844 - p10: 0.0499 - val_loss: 0.0026 - val_r1: 0.2500 - val_r5: 0.4173 - val_r10: 0.4965 - val_p5: 0.0835 - val_p10: 0.0496 - lr: 4.8515e-04 - 24s/epoch - 48ms/step
Epoch 31/1000
502/502 - 25s - loss: 0.0023 - r1: 0.2532 - r5: 0.4245 - r10: 0.5018 - p5: 0.0849 - p10: 0.0502 - val_loss: 0.0026 - val_r1: 0.2518 - val_r5: 0.4221 - val_r10: 0.5027 - val_p5: 0.0844 - val_p10: 0.0503 - lr: 4.8466e-04 - 25s/epoch - 49ms/step
Epoch 32/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2543 - r5: 0.4261 - r10: 0.5035 - p5: 0.0852 - p10: 0.0504 - val_loss: 0.0026 - val_r1: 0.2470 - val_r5: 0.4128 - val_r10: 0.4924 - val_p5: 0.0826 - val_p10: 0.0492 - lr: 4.8416e-04 - 24s/epoch - 48ms/step
Epoch 33/1000
502/502 - 24s - loss: 0.0023 - r1: 0.2560 - r5: 0.4276 - r10: 0.5041 - p5: 0.0855 - p10: 0.0504 - val_loss: 0.0026 - val_r1: 0.2549 - val_r5: 0.4250 - val_r10: 0.5044 - val_p5: 0.0850 - val_p10: 0.0504 - lr: 4.8367e-04 - 24s/epoch - 48ms/step
Epoch 34/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2570 - r5: 0.4307 - r10: 0.5079 - p5: 0.0862 - p10: 0.0508 - val_loss: 0.0026 - val_r1: 0.2537 - val_r5: 0.4251 - val_r10: 0.5042 - val_p5: 0.0850 - val_p10: 0.0504 - lr: 4.8317e-04 - 24s/epoch - 48ms/step
Epoch 35/1000
502/502 - 25s - loss: 0.0022 - r1: 0.2589 - r5: 0.4314 - r10: 0.5090 - p5: 0.0863 - p10: 0.0509 - val_loss: 0.0026 - val_r1: 0.2572 - val_r5: 0.4280 - val_r10: 0.5069 - val_p5: 0.0856 - val_p10: 0.0507 - lr: 4.8268e-04 - 25s/epoch - 49ms/step
Epoch 36/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2617 - r5: 0.4343 - r10: 0.5108 - p5: 0.0869 - p10: 0.0511 - val_loss: 0.0027 - val_r1: 0.2617 - val_r5: 0.4334 - val_r10: 0.5107 - val_p5: 0.0867 - val_p10: 0.0511 - lr: 4.8218e-04 - 24s/epoch - 48ms/step
Epoch 37/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2614 - r5: 0.4361 - r10: 0.5129 - p5: 0.0872 - p10: 0.0513 - val_loss: 0.0027 - val_r1: 0.2542 - val_r5: 0.4207 - val_r10: 0.5003 - val_p5: 0.0841 - val_p10: 0.0500 - lr: 4.8169e-04 - 24s/epoch - 48ms/step
Epoch 38/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2635 - r5: 0.4376 - r10: 0.5142 - p5: 0.0875 - p10: 0.0514 - val_loss: 0.0027 - val_r1: 0.2448 - val_r5: 0.4134 - val_r10: 0.4914 - val_p5: 0.0827 - val_p10: 0.0491 - lr: 4.8119e-04 - 24s/epoch - 48ms/step
Epoch 39/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2657 - r5: 0.4386 - r10: 0.5167 - p5: 0.0877 - p10: 0.0517 - val_loss: 0.0026 - val_r1: 0.2646 - val_r5: 0.4372 - val_r10: 0.5143 - val_p5: 0.0874 - val_p10: 0.0514 - lr: 4.8070e-04 - 24s/epoch - 48ms/step
Epoch 40/1000
502/502 - 25s - loss: 0.0022 - r1: 0.2659 - r5: 0.4404 - r10: 0.5174 - p5: 0.0881 - p10: 0.0517 - val_loss: 0.0025 - val_r1: 0.2782 - val_r5: 0.4533 - val_r10: 0.5309 - val_p5: 0.0906 - val_p10: 0.0531 - lr: 4.8020e-04 - 25s/epoch - 49ms/step
Epoch 41/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2668 - r5: 0.4427 - r10: 0.5201 - p5: 0.0885 - p10: 0.0520 - val_loss: 0.0026 - val_r1: 0.2665 - val_r5: 0.4384 - val_r10: 0.5160 - val_p5: 0.0877 - val_p10: 0.0516 - lr: 4.7971e-04 - 24s/epoch - 48ms/step
Epoch 42/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2684 - r5: 0.4440 - r10: 0.5223 - p5: 0.0888 - p10: 0.0522 - val_loss: 0.0026 - val_r1: 0.2726 - val_r5: 0.4424 - val_r10: 0.5193 - val_p5: 0.0885 - val_p10: 0.0519 - lr: 4.7921e-04 - 24s/epoch - 48ms/step
Epoch 43/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2698 - r5: 0.4450 - r10: 0.5222 - p5: 0.0890 - p10: 0.0522 - val_loss: 0.0026 - val_r1: 0.2612 - val_r5: 0.4290 - val_r10: 0.5057 - val_p5: 0.0858 - val_p10: 0.0506 - lr: 4.7872e-04 - 24s/epoch - 48ms/step
Epoch 44/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2724 - r5: 0.4480 - r10: 0.5254 - p5: 0.0896 - p10: 0.0525 - val_loss: 0.0026 - val_r1: 0.2730 - val_r5: 0.4425 - val_r10: 0.5180 - val_p5: 0.0885 - val_p10: 0.0518 - lr: 4.7822e-04 - 24s/epoch - 48ms/step
Epoch 45/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2716 - r5: 0.4476 - r10: 0.5254 - p5: 0.0895 - p10: 0.0525 - val_loss: 0.0026 - val_r1: 0.2649 - val_r5: 0.4337 - val_r10: 0.5116 - val_p5: 0.0867 - val_p10: 0.0512 - lr: 4.7773e-04 - 24s/epoch - 48ms/step
Epoch 46/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2739 - r5: 0.4502 - r10: 0.5278 - p5: 0.0901 - p10: 0.0528 - val_loss: 0.0026 - val_r1: 0.2560 - val_r5: 0.4239 - val_r10: 0.5004 - val_p5: 0.0848 - val_p10: 0.0500 - lr: 4.7723e-04 - 24s/epoch - 48ms/step
Epoch 47/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2748 - r5: 0.4507 - r10: 0.5279 - p5: 0.0901 - p10: 0.0528 - val_loss: 0.0026 - val_r1: 0.2754 - val_r5: 0.4489 - val_r10: 0.5264 - val_p5: 0.0898 - val_p10: 0.0526 - lr: 4.7674e-04 - 24s/epoch - 48ms/step
Epoch 48/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2757 - r5: 0.4541 - r10: 0.5316 - p5: 0.0908 - p10: 0.0532 - val_loss: 0.0026 - val_r1: 0.2716 - val_r5: 0.4438 - val_r10: 0.5230 - val_p5: 0.0887 - val_p10: 0.0523 - lr: 4.7624e-04 - 24s/epoch - 48ms/step
Epoch 49/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2769 - r5: 0.4547 - r10: 0.5314 - p5: 0.0909 - p10: 0.0531 - val_loss: 0.0026 - val_r1: 0.2752 - val_r5: 0.4483 - val_r10: 0.5237 - val_p5: 0.0897 - val_p10: 0.0523 - lr: 4.7575e-04 - 24s/epoch - 48ms/step
Epoch 50/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2777 - r5: 0.4556 - r10: 0.5327 - p5: 0.0911 - p10: 0.0533 - val_loss: 0.0026 - val_r1: 0.2728 - val_r5: 0.4495 - val_r10: 0.5239 - val_p5: 0.0899 - val_p10: 0.0524 - lr: 4.7525e-04 - 24s/epoch - 48ms/step
Epoch 51/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2784 - r5: 0.4565 - r10: 0.5345 - p5: 0.0913 - p10: 0.0535 - val_loss: 0.0026 - val_r1: 0.2700 - val_r5: 0.4386 - val_r10: 0.5166 - val_p5: 0.0877 - val_p10: 0.0517 - lr: 4.7476e-04 - 24s/epoch - 48ms/step
Epoch 52/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2791 - r5: 0.4557 - r10: 0.5338 - p5: 0.0911 - p10: 0.0534 - val_loss: 0.0026 - val_r1: 0.2725 - val_r5: 0.4432 - val_r10: 0.5195 - val_p5: 0.0886 - val_p10: 0.0520 - lr: 4.7426e-04 - 24s/epoch - 48ms/step
Epoch 53/1000
502/502 - 24s - loss: 0.0022 - r1: 0.2809 - r5: 0.4582 - r10: 0.5360 - p5: 0.0916 - p10: 0.0536 - val_loss: 0.0026 - val_r1: 0.2753 - val_r5: 0.4476 - val_r10: 0.5240 - val_p5: 0.0895 - val_p10: 0.0524 - lr: 4.7377e-04 - 24s/epoch - 48ms/step
Epoch 54/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2817 - r5: 0.4593 - r10: 0.5367 - p5: 0.0919 - p10: 0.0537 - val_loss: 0.0026 - val_r1: 0.2691 - val_r5: 0.4408 - val_r10: 0.5192 - val_p5: 0.0882 - val_p10: 0.0519 - lr: 4.7327e-04 - 24s/epoch - 48ms/step
Epoch 55/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2813 - r5: 0.4593 - r10: 0.5373 - p5: 0.0919 - p10: 0.0537 - val_loss: 0.0026 - val_r1: 0.2670 - val_r5: 0.4397 - val_r10: 0.5153 - val_p5: 0.0879 - val_p10: 0.0515 - lr: 4.7278e-04 - 24s/epoch - 48ms/step
Epoch 56/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2827 - r5: 0.4604 - r10: 0.5387 - p5: 0.0921 - p10: 0.0539 - val_loss: 0.0026 - val_r1: 0.2721 - val_r5: 0.4439 - val_r10: 0.5228 - val_p5: 0.0888 - val_p10: 0.0523 - lr: 4.7228e-04 - 24s/epoch - 48ms/step
Epoch 57/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2827 - r5: 0.4623 - r10: 0.5400 - p5: 0.0925 - p10: 0.0540 - val_loss: 0.0026 - val_r1: 0.2687 - val_r5: 0.4392 - val_r10: 0.5177 - val_p5: 0.0879 - val_p10: 0.0518 - lr: 4.7179e-04 - 24s/epoch - 48ms/step
Epoch 58/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2827 - r5: 0.4610 - r10: 0.5394 - p5: 0.0922 - p10: 0.0539 - val_loss: 0.0026 - val_r1: 0.2712 - val_r5: 0.4427 - val_r10: 0.5191 - val_p5: 0.0885 - val_p10: 0.0519 - lr: 4.7129e-04 - 24s/epoch - 48ms/step
Epoch 59/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2839 - r5: 0.4627 - r10: 0.5408 - p5: 0.0925 - p10: 0.0541 - val_loss: 0.0026 - val_r1: 0.2794 - val_r5: 0.4560 - val_r10: 0.5319 - val_p5: 0.0912 - val_p10: 0.0532 - lr: 4.7080e-04 - 24s/epoch - 48ms/step
Epoch 60/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2851 - r5: 0.4647 - r10: 0.5417 - p5: 0.0929 - p10: 0.0542 - val_loss: 0.0026 - val_r1: 0.2704 - val_r5: 0.4431 - val_r10: 0.5186 - val_p5: 0.0886 - val_p10: 0.0519 - lr: 4.7030e-04 - 24s/epoch - 48ms/step
Epoch 61/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2870 - r5: 0.4665 - r10: 0.5435 - p5: 0.0933 - p10: 0.0544 - val_loss: 0.0026 - val_r1: 0.2785 - val_r5: 0.4485 - val_r10: 0.5249 - val_p5: 0.0897 - val_p10: 0.0525 - lr: 4.6981e-04 - 24s/epoch - 48ms/step
Epoch 62/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2869 - r5: 0.4669 - r10: 0.5448 - p5: 0.0934 - p10: 0.0545 - val_loss: 0.0026 - val_r1: 0.2845 - val_r5: 0.4566 - val_r10: 0.5321 - val_p5: 0.0913 - val_p10: 0.0532 - lr: 4.6931e-04 - 24s/epoch - 48ms/step
Epoch 63/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2879 - r5: 0.4674 - r10: 0.5448 - p5: 0.0935 - p10: 0.0545 - val_loss: 0.0026 - val_r1: 0.2766 - val_r5: 0.4494 - val_r10: 0.5273 - val_p5: 0.0899 - val_p10: 0.0527 - lr: 4.6882e-04 - 24s/epoch - 48ms/step
Epoch 64/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2863 - r5: 0.4664 - r10: 0.5446 - p5: 0.0933 - p10: 0.0545 - val_loss: 0.0025 - val_r1: 0.2731 - val_r5: 0.4465 - val_r10: 0.5231 - val_p5: 0.0893 - val_p10: 0.0523 - lr: 4.6832e-04 - 24s/epoch - 48ms/step
Epoch 65/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2878 - r5: 0.4689 - r10: 0.5461 - p5: 0.0938 - p10: 0.0546 - val_loss: 0.0026 - val_r1: 0.2777 - val_r5: 0.4484 - val_r10: 0.5248 - val_p5: 0.0897 - val_p10: 0.0525 - lr: 4.6783e-04 - 24s/epoch - 48ms/step
Epoch 66/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2878 - r5: 0.4687 - r10: 0.5471 - p5: 0.0937 - p10: 0.0547 - val_loss: 0.0026 - val_r1: 0.2866 - val_r5: 0.4606 - val_r10: 0.5380 - val_p5: 0.0921 - val_p10: 0.0538 - lr: 4.6733e-04 - 24s/epoch - 48ms/step
Epoch 67/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2895 - r5: 0.4706 - r10: 0.5482 - p5: 0.0941 - p10: 0.0548 - val_loss: 0.0026 - val_r1: 0.2851 - val_r5: 0.4601 - val_r10: 0.5342 - val_p5: 0.0920 - val_p10: 0.0534 - lr: 4.6684e-04 - 24s/epoch - 48ms/step
Epoch 68/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2884 - r5: 0.4697 - r10: 0.5474 - p5: 0.0939 - p10: 0.0547 - val_loss: 0.0026 - val_r1: 0.2801 - val_r5: 0.4497 - val_r10: 0.5258 - val_p5: 0.0899 - val_p10: 0.0526 - lr: 4.6634e-04 - 24s/epoch - 48ms/step
Epoch 69/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2907 - r5: 0.4705 - r10: 0.5481 - p5: 0.0941 - p10: 0.0548 - val_loss: 0.0026 - val_r1: 0.2712 - val_r5: 0.4450 - val_r10: 0.5206 - val_p5: 0.0890 - val_p10: 0.0521 - lr: 4.6585e-04 - 24s/epoch - 48ms/step
Epoch 70/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2918 - r5: 0.4718 - r10: 0.5497 - p5: 0.0943 - p10: 0.0550 - val_loss: 0.0026 - val_r1: 0.2741 - val_r5: 0.4457 - val_r10: 0.5227 - val_p5: 0.0891 - val_p10: 0.0523 - lr: 4.6535e-04 - 24s/epoch - 48ms/step
Epoch 71/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2924 - r5: 0.4730 - r10: 0.5498 - p5: 0.0946 - p10: 0.0550 - val_loss: 0.0026 - val_r1: 0.2838 - val_r5: 0.4560 - val_r10: 0.5341 - val_p5: 0.0912 - val_p10: 0.0534 - lr: 4.6486e-04 - 24s/epoch - 48ms/step
Epoch 72/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2916 - r5: 0.4734 - r10: 0.5506 - p5: 0.0947 - p10: 0.0551 - val_loss: 0.0026 - val_r1: 0.2767 - val_r5: 0.4486 - val_r10: 0.5239 - val_p5: 0.0897 - val_p10: 0.0524 - lr: 4.6436e-04 - 24s/epoch - 48ms/step
Epoch 73/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2931 - r5: 0.4741 - r10: 0.5516 - p5: 0.0948 - p10: 0.0552 - val_loss: 0.0026 - val_r1: 0.2798 - val_r5: 0.4514 - val_r10: 0.5288 - val_p5: 0.0903 - val_p10: 0.0529 - lr: 4.6387e-04 - 24s/epoch - 48ms/step
Epoch 74/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2936 - r5: 0.4738 - r10: 0.5517 - p5: 0.0948 - p10: 0.0552 - val_loss: 0.0026 - val_r1: 0.2866 - val_r5: 0.4607 - val_r10: 0.5364 - val_p5: 0.0922 - val_p10: 0.0537 - lr: 4.6337e-04 - 24s/epoch - 48ms/step
Epoch 75/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2935 - r5: 0.4741 - r10: 0.5522 - p5: 0.0948 - p10: 0.0552 - val_loss: 0.0026 - val_r1: 0.2833 - val_r5: 0.4562 - val_r10: 0.5306 - val_p5: 0.0912 - val_p10: 0.0531 - lr: 4.6288e-04 - 24s/epoch - 48ms/step
Epoch 76/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2950 - r5: 0.4768 - r10: 0.5544 - p5: 0.0953 - p10: 0.0554 - val_loss: 0.0026 - val_r1: 0.2812 - val_r5: 0.4530 - val_r10: 0.5283 - val_p5: 0.0906 - val_p10: 0.0528 - lr: 4.6238e-04 - 24s/epoch - 48ms/step
Epoch 77/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2945 - r5: 0.4760 - r10: 0.5536 - p5: 0.0952 - p10: 0.0554 - val_loss: 0.0026 - val_r1: 0.2764 - val_r5: 0.4510 - val_r10: 0.5296 - val_p5: 0.0902 - val_p10: 0.0529 - lr: 4.6189e-04 - 24s/epoch - 48ms/step
Epoch 78/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2957 - r5: 0.4764 - r10: 0.5545 - p5: 0.0953 - p10: 0.0555 - val_loss: 0.0026 - val_r1: 0.2798 - val_r5: 0.4540 - val_r10: 0.5294 - val_p5: 0.0908 - val_p10: 0.0529 - lr: 4.6139e-04 - 24s/epoch - 48ms/step
Epoch 79/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2949 - r5: 0.4769 - r10: 0.5545 - p5: 0.0954 - p10: 0.0555 - val_loss: 0.0026 - val_r1: 0.2828 - val_r5: 0.4571 - val_r10: 0.5356 - val_p5: 0.0914 - val_p10: 0.0536 - lr: 4.6090e-04 - 24s/epoch - 48ms/step
Epoch 80/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2955 - r5: 0.4777 - r10: 0.5553 - p5: 0.0955 - p10: 0.0555 - val_loss: 0.0026 - val_r1: 0.2838 - val_r5: 0.4585 - val_r10: 0.5351 - val_p5: 0.0917 - val_p10: 0.0535 - lr: 4.6040e-04 - 24s/epoch - 48ms/step
Epoch 81/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2969 - r5: 0.4788 - r10: 0.5559 - p5: 0.0958 - p10: 0.0556 - val_loss: 0.0026 - val_r1: 0.2833 - val_r5: 0.4589 - val_r10: 0.5354 - val_p5: 0.0918 - val_p10: 0.0535 - lr: 4.5991e-04 - 24s/epoch - 48ms/step
Epoch 82/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2971 - r5: 0.4790 - r10: 0.5574 - p5: 0.0958 - p10: 0.0557 - val_loss: 0.0026 - val_r1: 0.2830 - val_r5: 0.4598 - val_r10: 0.5373 - val_p5: 0.0919 - val_p10: 0.0537 - lr: 4.5941e-04 - 24s/epoch - 48ms/step
Epoch 83/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2968 - r5: 0.4792 - r10: 0.5567 - p5: 0.0958 - p10: 0.0557 - val_loss: 0.0026 - val_r1: 0.2846 - val_r5: 0.4583 - val_r10: 0.5344 - val_p5: 0.0917 - val_p10: 0.0535 - lr: 4.5892e-04 - 24s/epoch - 48ms/step
Epoch 84/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2990 - r5: 0.4813 - r10: 0.5588 - p5: 0.0963 - p10: 0.0559 - val_loss: 0.0026 - val_r1: 0.2850 - val_r5: 0.4606 - val_r10: 0.5372 - val_p5: 0.0921 - val_p10: 0.0537 - lr: 4.5842e-04 - 24s/epoch - 48ms/step
Epoch 85/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2991 - r5: 0.4815 - r10: 0.5592 - p5: 0.0963 - p10: 0.0559 - val_loss: 0.0026 - val_r1: 0.2788 - val_r5: 0.4548 - val_r10: 0.5325 - val_p5: 0.0910 - val_p10: 0.0533 - lr: 4.5793e-04 - 24s/epoch - 48ms/step
Epoch 86/1000
502/502 - 24s - loss: 0.0021 - r1: 0.3001 - r5: 0.4804 - r10: 0.5585 - p5: 0.0961 - p10: 0.0559 - val_loss: 0.0026 - val_r1: 0.2829 - val_r5: 0.4564 - val_r10: 0.5345 - val_p5: 0.0913 - val_p10: 0.0534 - lr: 4.5743e-04 - 24s/epoch - 48ms/step
Epoch 87/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2986 - r5: 0.4816 - r10: 0.5596 - p5: 0.0963 - p10: 0.0560 - val_loss: 0.0026 - val_r1: 0.2839 - val_r5: 0.4595 - val_r10: 0.5353 - val_p5: 0.0919 - val_p10: 0.0535 - lr: 4.5694e-04 - 24s/epoch - 48ms/step
Epoch 88/1000
502/502 - 24s - loss: 0.0021 - r1: 0.3004 - r5: 0.4826 - r10: 0.5596 - p5: 0.0965 - p10: 0.0560 - val_loss: 0.0026 - val_r1: 0.2803 - val_r5: 0.4554 - val_r10: 0.5298 - val_p5: 0.0911 - val_p10: 0.0530 - lr: 4.5644e-04 - 24s/epoch - 48ms/step
Epoch 89/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2996 - r5: 0.4823 - r10: 0.5596 - p5: 0.0964 - p10: 0.0560 - val_loss: 0.0026 - val_r1: 0.2834 - val_r5: 0.4561 - val_r10: 0.5318 - val_p5: 0.0912 - val_p10: 0.0532 - lr: 4.5595e-04 - 24s/epoch - 48ms/step
Epoch 90/1000
502/502 - 24s - loss: 0.0021 - r1: 0.2989 - r5: 0.4810 - r10: 0.5602 - p5: 0.0962 - p10: 0.0560 - val_loss: 0.0026 - val_r1: 0.2806 - val_r5: 0.4525 - val_r10: 0.5288 - val_p5: 0.0905 - val_p10: 0.0529 - lr: 4.5545e-04 - 24s/epoch - 48ms/step
Epoch 90: early stopping
[92m[INFO] Loading best model...[0m
[93m[WARNING] Model folder already exists...[0m
Model: "ATT2ITM_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 172)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1322)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 172, 384)     3389952     ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 1322, 384)    507648      ['input_2[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 172, 256)     98560       ['embedding[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 1322, 256)    98560       ['in_rsts[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 172)         0           ['input_1[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 172, 128)     32896       ['dense[0][0]']                  
                                                                                                  
 dense_3 (Dense)                (None, 1322, 128)    32896       ['dense_2[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 172)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 172, 128)     0           ['dense_1[0][0]']                
                                                                                                  
 rest_emb (Lambda)              (None, 1322, 128)    0           ['dense_3[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 172, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 172, 1322)    0           ['word_emb[0][0]',               
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 172, 1322)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 172, 1322)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 172, 1322)    0           ['lambda_1[0][0]']               
                                                                                                  
 dropout (Dropout)              (None, 172, 1322)    0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 1322)         0           ['dropout[0][0]']                
                                                                                                  
 out (Activation)               (None, 1322)         0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 4,160,512
Trainable params: 4,160,512
Non-trainable params: 0
__________________________________________________________________________________________________
None
[92m[INFO] Model already trained. Loading weights...[0m
[93m[WARNING] Model folder already exists...[0m
Model: "ATT2ITM_1"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 172)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1322)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 172, 384)     3389952     ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 1322, 384)    507648      ['input_2[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 172, 256)     98560       ['embedding[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 1322, 256)    98560       ['in_rsts[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 172)         0           ['input_1[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 172, 128)     32896       ['dense[0][0]']                  
                                                                                                  
 dense_3 (Dense)                (None, 1322, 128)    32896       ['dense_2[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 172)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 172, 128)     0           ['dense_1[0][0]']                
                                                                                                  
 rest_emb (Lambda)              (None, 1322, 128)    0           ['dense_3[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 172, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 172, 1322)    0           ['word_emb[0][0]',               
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 172, 1322)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 172, 1322)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 172, 1322)    0           ['lambda_1[0][0]']               
                                                                                                  
 dropout (Dropout)              (None, 172, 1322)    0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 1322)         0           ['dropout[0][0]']                
                                                                                                  
 out (Activation)               (None, 1322)         0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 4,160,512
Trainable params: 4,160,512
Non-trainable params: 0
__________________________________________________________________________________________________
None
[92m[INFO] Model already trained. Loading weights...[0m
