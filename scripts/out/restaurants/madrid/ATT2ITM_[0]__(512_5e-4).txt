Model: "ATT2ITM_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 179)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1634)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 179, 384)     4201728     ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 1634, 384)    627456      ['input_2[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 179, 256)     98560       ['embedding[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 1634, 256)    98560       ['in_rsts[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 179)         0           ['input_1[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 179, 128)     32896       ['dense[0][0]']                  
                                                                                                  
 dense_3 (Dense)                (None, 1634, 128)    32896       ['dense_2[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 179)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 179, 128)     0           ['dense_1[0][0]']                
                                                                                                  
 rest_emb (Lambda)              (None, 1634, 128)    0           ['dense_3[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 179, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 179, 1634)    0           ['word_emb[0][0]',               
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 179, 1634)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 179, 1634)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 179, 1634)    0           ['lambda_1[0][0]']               
                                                                                                  
 dropout (Dropout)              (None, 179, 1634)    0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 1634)         0           ['dropout[0][0]']                
                                                                                                  
 out (Activation)               (None, 1634)         0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 5,092,096
Trainable params: 5,092,096
Non-trainable params: 0
__________________________________________________________________________________________________
None
Epoch 1/1000
745/745 - 46s - loss: 5.6173 - r1: 0.1122 - r5: 0.2306 - r10: 0.2963 - p5: 0.0461 - p10: 0.0296 - val_loss: 4.8815 - val_r1: 0.1923 - val_r5: 0.3640 - val_r10: 0.4505 - val_p5: 0.0728 - val_p10: 0.0450 - lr: 4.9951e-04 - 46s/epoch - 61ms/step
Epoch 2/1000
745/745 - 43s - loss: 4.6960 - r1: 0.1912 - r5: 0.3646 - r10: 0.4486 - p5: 0.0729 - p10: 0.0449 - val_loss: 5.2462 - val_r1: 0.1547 - val_r5: 0.3074 - val_r10: 0.3871 - val_p5: 0.0615 - val_p10: 0.0387 - lr: 4.9901e-04 - 43s/epoch - 58ms/step
Epoch 3/1000
745/745 - 43s - loss: 4.4275 - r1: 0.2226 - r5: 0.4083 - r10: 0.4937 - p5: 0.0817 - p10: 0.0494 - val_loss: 5.3669 - val_r1: 0.1471 - val_r5: 0.3012 - val_r10: 0.3847 - val_p5: 0.0602 - val_p10: 0.0385 - lr: 4.9852e-04 - 43s/epoch - 58ms/step
Epoch 4/1000
745/745 - 43s - loss: 4.2648 - r1: 0.2425 - r5: 0.4353 - r10: 0.5204 - p5: 0.0871 - p10: 0.0520 - val_loss: 4.8248 - val_r1: 0.2057 - val_r5: 0.3742 - val_r10: 0.4560 - val_p5: 0.0749 - val_p10: 0.0456 - lr: 4.9802e-04 - 43s/epoch - 58ms/step
Epoch 5/1000
745/745 - 44s - loss: 4.1477 - r1: 0.2562 - r5: 0.4526 - r10: 0.5386 - p5: 0.0905 - p10: 0.0539 - val_loss: 4.8130 - val_r1: 0.2085 - val_r5: 0.3817 - val_r10: 0.4645 - val_p5: 0.0763 - val_p10: 0.0464 - lr: 4.9753e-04 - 44s/epoch - 58ms/step
Epoch 6/1000
745/745 - 44s - loss: 4.0506 - r1: 0.2696 - r5: 0.4670 - r10: 0.5520 - p5: 0.0934 - p10: 0.0552 - val_loss: 4.6560 - val_r1: 0.2271 - val_r5: 0.4022 - val_r10: 0.4842 - val_p5: 0.0804 - val_p10: 0.0484 - lr: 4.9703e-04 - 44s/epoch - 59ms/step
Epoch 7/1000
745/745 - 43s - loss: 3.9746 - r1: 0.2779 - r5: 0.4791 - r10: 0.5642 - p5: 0.0958 - p10: 0.0564 - val_loss: 4.6778 - val_r1: 0.2223 - val_r5: 0.4037 - val_r10: 0.4852 - val_p5: 0.0807 - val_p10: 0.0485 - lr: 4.9654e-04 - 43s/epoch - 58ms/step
Epoch 8/1000
745/745 - 44s - loss: 3.9098 - r1: 0.2856 - r5: 0.4891 - r10: 0.5738 - p5: 0.0978 - p10: 0.0574 - val_loss: 4.6086 - val_r1: 0.2339 - val_r5: 0.4109 - val_r10: 0.4933 - val_p5: 0.0822 - val_p10: 0.0493 - lr: 4.9604e-04 - 44s/epoch - 59ms/step
Epoch 9/1000
745/745 - 43s - loss: 3.8493 - r1: 0.2931 - r5: 0.4989 - r10: 0.5826 - p5: 0.0998 - p10: 0.0583 - val_loss: 4.6198 - val_r1: 0.2327 - val_r5: 0.4091 - val_r10: 0.4909 - val_p5: 0.0818 - val_p10: 0.0491 - lr: 4.9555e-04 - 43s/epoch - 58ms/step
Epoch 10/1000
745/745 - 44s - loss: 3.8001 - r1: 0.2998 - r5: 0.5057 - r10: 0.5901 - p5: 0.1011 - p10: 0.0590 - val_loss: 4.3636 - val_r1: 0.2632 - val_r5: 0.4484 - val_r10: 0.5306 - val_p5: 0.0897 - val_p10: 0.0531 - lr: 4.9505e-04 - 44s/epoch - 59ms/step
Epoch 11/1000
745/745 - 43s - loss: 3.7546 - r1: 0.3061 - r5: 0.5124 - r10: 0.5962 - p5: 0.1025 - p10: 0.0596 - val_loss: 4.5240 - val_r1: 0.2454 - val_r5: 0.4228 - val_r10: 0.5036 - val_p5: 0.0846 - val_p10: 0.0504 - lr: 4.9456e-04 - 43s/epoch - 58ms/step
Epoch 12/1000
745/745 - 43s - loss: 3.7103 - r1: 0.3114 - r5: 0.5197 - r10: 0.6033 - p5: 0.1039 - p10: 0.0603 - val_loss: 4.5423 - val_r1: 0.2446 - val_r5: 0.4216 - val_r10: 0.5026 - val_p5: 0.0843 - val_p10: 0.0503 - lr: 4.9406e-04 - 43s/epoch - 58ms/step
Epoch 13/1000
745/745 - 43s - loss: 3.6743 - r1: 0.3153 - r5: 0.5251 - r10: 0.6091 - p5: 0.1050 - p10: 0.0609 - val_loss: 4.3641 - val_r1: 0.2636 - val_r5: 0.4473 - val_r10: 0.5298 - val_p5: 0.0895 - val_p10: 0.0530 - lr: 4.9357e-04 - 43s/epoch - 58ms/step
Epoch 14/1000
745/745 - 43s - loss: 3.6334 - r1: 0.3214 - r5: 0.5315 - r10: 0.6149 - p5: 0.1063 - p10: 0.0615 - val_loss: 4.4318 - val_r1: 0.2558 - val_r5: 0.4370 - val_r10: 0.5181 - val_p5: 0.0874 - val_p10: 0.0518 - lr: 4.9307e-04 - 43s/epoch - 58ms/step
Epoch 15/1000
745/745 - 43s - loss: 3.6080 - r1: 0.3243 - r5: 0.5354 - r10: 0.6182 - p5: 0.1071 - p10: 0.0618 - val_loss: 4.3923 - val_r1: 0.2619 - val_r5: 0.4439 - val_r10: 0.5246 - val_p5: 0.0888 - val_p10: 0.0525 - lr: 4.9258e-04 - 43s/epoch - 58ms/step
Epoch 16/1000
745/745 - 44s - loss: 3.5759 - r1: 0.3291 - r5: 0.5401 - r10: 0.6224 - p5: 0.1080 - p10: 0.0622 - val_loss: 4.1503 - val_r1: 0.2898 - val_r5: 0.4793 - val_r10: 0.5589 - val_p5: 0.0959 - val_p10: 0.0559 - lr: 4.9208e-04 - 44s/epoch - 59ms/step
Epoch 17/1000
745/745 - 43s - loss: 3.5462 - r1: 0.3325 - r5: 0.5447 - r10: 0.6274 - p5: 0.1089 - p10: 0.0627 - val_loss: 4.4936 - val_r1: 0.2526 - val_r5: 0.4340 - val_r10: 0.5135 - val_p5: 0.0868 - val_p10: 0.0514 - lr: 4.9159e-04 - 43s/epoch - 58ms/step
Epoch 18/1000
745/745 - 43s - loss: 3.5232 - r1: 0.3348 - r5: 0.5494 - r10: 0.6310 - p5: 0.1099 - p10: 0.0631 - val_loss: 4.3577 - val_r1: 0.2662 - val_r5: 0.4491 - val_r10: 0.5303 - val_p5: 0.0898 - val_p10: 0.0530 - lr: 4.9109e-04 - 43s/epoch - 58ms/step
Epoch 19/1000
745/745 - 43s - loss: 3.4972 - r1: 0.3383 - r5: 0.5532 - r10: 0.6352 - p5: 0.1106 - p10: 0.0635 - val_loss: 4.1802 - val_r1: 0.2882 - val_r5: 0.4738 - val_r10: 0.5533 - val_p5: 0.0948 - val_p10: 0.0553 - lr: 4.9060e-04 - 43s/epoch - 58ms/step
Epoch 20/1000
745/745 - 43s - loss: 3.4771 - r1: 0.3420 - r5: 0.5555 - r10: 0.6374 - p5: 0.1111 - p10: 0.0637 - val_loss: 4.3581 - val_r1: 0.2670 - val_r5: 0.4502 - val_r10: 0.5297 - val_p5: 0.0900 - val_p10: 0.0530 - lr: 4.9010e-04 - 43s/epoch - 58ms/step
Epoch 21/1000
745/745 - 43s - loss: 3.4542 - r1: 0.3447 - r5: 0.5592 - r10: 0.6413 - p5: 0.1118 - p10: 0.0641 - val_loss: 4.3283 - val_r1: 0.2691 - val_r5: 0.4532 - val_r10: 0.5331 - val_p5: 0.0906 - val_p10: 0.0533 - lr: 4.8961e-04 - 43s/epoch - 58ms/step
Epoch 22/1000
745/745 - 44s - loss: 3.4389 - r1: 0.3467 - r5: 0.5618 - r10: 0.6438 - p5: 0.1124 - p10: 0.0644 - val_loss: 4.0634 - val_r1: 0.3018 - val_r5: 0.4922 - val_r10: 0.5715 - val_p5: 0.0984 - val_p10: 0.0571 - lr: 4.8911e-04 - 44s/epoch - 59ms/step
Epoch 23/1000
745/745 - 43s - loss: 3.4119 - r1: 0.3490 - r5: 0.5661 - r10: 0.6471 - p5: 0.1132 - p10: 0.0647 - val_loss: 4.4775 - val_r1: 0.2580 - val_r5: 0.4361 - val_r10: 0.5169 - val_p5: 0.0872 - val_p10: 0.0517 - lr: 4.8862e-04 - 43s/epoch - 58ms/step
Epoch 24/1000
745/745 - 43s - loss: 3.4010 - r1: 0.3509 - r5: 0.5667 - r10: 0.6490 - p5: 0.1134 - p10: 0.0649 - val_loss: 4.3273 - val_r1: 0.2706 - val_r5: 0.4555 - val_r10: 0.5342 - val_p5: 0.0911 - val_p10: 0.0534 - lr: 4.8812e-04 - 43s/epoch - 58ms/step
Epoch 25/1000
745/745 - 43s - loss: 3.3870 - r1: 0.3529 - r5: 0.5692 - r10: 0.6510 - p5: 0.1138 - p10: 0.0651 - val_loss: 4.3652 - val_r1: 0.2684 - val_r5: 0.4503 - val_r10: 0.5309 - val_p5: 0.0900 - val_p10: 0.0531 - lr: 4.8763e-04 - 43s/epoch - 58ms/step
Epoch 26/1000
745/745 - 43s - loss: 3.3675 - r1: 0.3555 - r5: 0.5713 - r10: 0.6537 - p5: 0.1143 - p10: 0.0654 - val_loss: 4.2437 - val_r1: 0.2804 - val_r5: 0.4648 - val_r10: 0.5477 - val_p5: 0.0930 - val_p10: 0.0548 - lr: 4.8713e-04 - 43s/epoch - 58ms/step
Epoch 27/1000
745/745 - 43s - loss: 3.3537 - r1: 0.3570 - r5: 0.5736 - r10: 0.6555 - p5: 0.1147 - p10: 0.0656 - val_loss: 4.2461 - val_r1: 0.2790 - val_r5: 0.4675 - val_r10: 0.5474 - val_p5: 0.0935 - val_p10: 0.0547 - lr: 4.8664e-04 - 43s/epoch - 58ms/step
Epoch 28/1000
745/745 - 44s - loss: 3.3402 - r1: 0.3595 - r5: 0.5764 - r10: 0.6578 - p5: 0.1153 - p10: 0.0658 - val_loss: 4.0602 - val_r1: 0.3012 - val_r5: 0.4941 - val_r10: 0.5723 - val_p5: 0.0988 - val_p10: 0.0572 - lr: 4.8614e-04 - 44s/epoch - 59ms/step
Epoch 29/1000
745/745 - 43s - loss: 3.3257 - r1: 0.3616 - r5: 0.5785 - r10: 0.6591 - p5: 0.1157 - p10: 0.0659 - val_loss: 4.3325 - val_r1: 0.2712 - val_r5: 0.4576 - val_r10: 0.5348 - val_p5: 0.0915 - val_p10: 0.0535 - lr: 4.8565e-04 - 43s/epoch - 58ms/step
Epoch 30/1000
745/745 - 43s - loss: 3.3125 - r1: 0.3633 - r5: 0.5805 - r10: 0.6611 - p5: 0.1161 - p10: 0.0661 - val_loss: 4.2974 - val_r1: 0.2741 - val_r5: 0.4597 - val_r10: 0.5402 - val_p5: 0.0920 - val_p10: 0.0540 - lr: 4.8515e-04 - 43s/epoch - 58ms/step
Epoch 31/1000
745/745 - 43s - loss: 3.3086 - r1: 0.3636 - r5: 0.5809 - r10: 0.6618 - p5: 0.1162 - p10: 0.0662 - val_loss: 4.0830 - val_r1: 0.2999 - val_r5: 0.4896 - val_r10: 0.5692 - val_p5: 0.0979 - val_p10: 0.0569 - lr: 4.8466e-04 - 43s/epoch - 58ms/step
Epoch 32/1000
745/745 - 43s - loss: 3.2849 - r1: 0.3674 - r5: 0.5851 - r10: 0.6655 - p5: 0.1170 - p10: 0.0666 - val_loss: 4.3561 - val_r1: 0.2680 - val_r5: 0.4513 - val_r10: 0.5330 - val_p5: 0.0903 - val_p10: 0.0533 - lr: 4.8416e-04 - 43s/epoch - 58ms/step
Epoch 33/1000
745/745 - 43s - loss: 3.2781 - r1: 0.3682 - r5: 0.5847 - r10: 0.6661 - p5: 0.1169 - p10: 0.0666 - val_loss: 4.2991 - val_r1: 0.2753 - val_r5: 0.4621 - val_r10: 0.5414 - val_p5: 0.0924 - val_p10: 0.0541 - lr: 4.8367e-04 - 43s/epoch - 58ms/step
Epoch 34/1000
745/745 - 43s - loss: 3.2681 - r1: 0.3697 - r5: 0.5879 - r10: 0.6682 - p5: 0.1176 - p10: 0.0668 - val_loss: 4.1083 - val_r1: 0.2973 - val_r5: 0.4874 - val_r10: 0.5652 - val_p5: 0.0975 - val_p10: 0.0565 - lr: 4.8317e-04 - 43s/epoch - 58ms/step
Epoch 35/1000
745/745 - 43s - loss: 3.2585 - r1: 0.3699 - r5: 0.5878 - r10: 0.6682 - p5: 0.1176 - p10: 0.0668 - val_loss: 4.2870 - val_r1: 0.2744 - val_r5: 0.4612 - val_r10: 0.5405 - val_p5: 0.0922 - val_p10: 0.0541 - lr: 4.8268e-04 - 43s/epoch - 58ms/step
Epoch 36/1000
745/745 - 43s - loss: 3.2506 - r1: 0.3725 - r5: 0.5901 - r10: 0.6700 - p5: 0.1180 - p10: 0.0670 - val_loss: 4.2607 - val_r1: 0.2785 - val_r5: 0.4653 - val_r10: 0.5440 - val_p5: 0.0931 - val_p10: 0.0544 - lr: 4.8218e-04 - 43s/epoch - 58ms/step
Epoch 37/1000
745/745 - 43s - loss: 3.2397 - r1: 0.3737 - r5: 0.5908 - r10: 0.6718 - p5: 0.1182 - p10: 0.0672 - val_loss: 4.0844 - val_r1: 0.2992 - val_r5: 0.4912 - val_r10: 0.5692 - val_p5: 0.0982 - val_p10: 0.0569 - lr: 4.8169e-04 - 43s/epoch - 58ms/step
Epoch 38/1000
745/745 - 43s - loss: 3.2247 - r1: 0.3764 - r5: 0.5939 - r10: 0.6731 - p5: 0.1188 - p10: 0.0673 - val_loss: 4.2882 - val_r1: 0.2736 - val_r5: 0.4616 - val_r10: 0.5414 - val_p5: 0.0923 - val_p10: 0.0541 - lr: 4.8119e-04 - 43s/epoch - 58ms/step
Epoch 39/1000
745/745 - 43s - loss: 3.2248 - r1: 0.3744 - r5: 0.5933 - r10: 0.6735 - p5: 0.1187 - p10: 0.0674 - val_loss: 4.2871 - val_r1: 0.2763 - val_r5: 0.4629 - val_r10: 0.5419 - val_p5: 0.0926 - val_p10: 0.0542 - lr: 4.8070e-04 - 43s/epoch - 58ms/step
Epoch 40/1000
745/745 - 44s - loss: 3.2120 - r1: 0.3769 - r5: 0.5957 - r10: 0.6754 - p5: 0.1191 - p10: 0.0675 - val_loss: 3.9789 - val_r1: 0.3142 - val_r5: 0.5057 - val_r10: 0.5832 - val_p5: 0.1011 - val_p10: 0.0583 - lr: 4.8020e-04 - 44s/epoch - 59ms/step
Epoch 41/1000
745/745 - 43s - loss: 3.2026 - r1: 0.3784 - r5: 0.5973 - r10: 0.6775 - p5: 0.1195 - p10: 0.0678 - val_loss: 4.3310 - val_r1: 0.2719 - val_r5: 0.4574 - val_r10: 0.5356 - val_p5: 0.0915 - val_p10: 0.0536 - lr: 4.7971e-04 - 43s/epoch - 58ms/step
Epoch 42/1000
745/745 - 43s - loss: 3.1927 - r1: 0.3799 - r5: 0.5985 - r10: 0.6785 - p5: 0.1197 - p10: 0.0679 - val_loss: 4.2584 - val_r1: 0.2786 - val_r5: 0.4678 - val_r10: 0.5460 - val_p5: 0.0936 - val_p10: 0.0546 - lr: 4.7921e-04 - 43s/epoch - 58ms/step
Epoch 43/1000
745/745 - 43s - loss: 3.1871 - r1: 0.3801 - r5: 0.5988 - r10: 0.6790 - p5: 0.1198 - p10: 0.0679 - val_loss: 4.2878 - val_r1: 0.2773 - val_r5: 0.4647 - val_r10: 0.5421 - val_p5: 0.0929 - val_p10: 0.0542 - lr: 4.7872e-04 - 43s/epoch - 58ms/step
Epoch 44/1000
745/745 - 43s - loss: 3.1843 - r1: 0.3815 - r5: 0.5995 - r10: 0.6787 - p5: 0.1199 - p10: 0.0679 - val_loss: 4.2659 - val_r1: 0.2784 - val_r5: 0.4649 - val_r10: 0.5460 - val_p5: 0.0930 - val_p10: 0.0546 - lr: 4.7822e-04 - 43s/epoch - 58ms/step
Epoch 45/1000
745/745 - 43s - loss: 3.1712 - r1: 0.3834 - r5: 0.6022 - r10: 0.6816 - p5: 0.1204 - p10: 0.0682 - val_loss: 4.2811 - val_r1: 0.2771 - val_r5: 0.4626 - val_r10: 0.5426 - val_p5: 0.0925 - val_p10: 0.0542 - lr: 4.7773e-04 - 43s/epoch - 58ms/step
Epoch 46/1000
745/745 - 43s - loss: 3.1681 - r1: 0.3830 - r5: 0.6025 - r10: 0.6819 - p5: 0.1205 - p10: 0.0682 - val_loss: 4.0709 - val_r1: 0.3041 - val_r5: 0.4910 - val_r10: 0.5682 - val_p5: 0.0982 - val_p10: 0.0568 - lr: 4.7723e-04 - 43s/epoch - 58ms/step
Epoch 47/1000
745/745 - 43s - loss: 3.1560 - r1: 0.3859 - r5: 0.6039 - r10: 0.6835 - p5: 0.1208 - p10: 0.0683 - val_loss: 4.2692 - val_r1: 0.2785 - val_r5: 0.4651 - val_r10: 0.5453 - val_p5: 0.0930 - val_p10: 0.0545 - lr: 4.7674e-04 - 43s/epoch - 58ms/step
Epoch 48/1000
745/745 - 43s - loss: 3.1502 - r1: 0.3858 - r5: 0.6052 - r10: 0.6843 - p5: 0.1210 - p10: 0.0684 - val_loss: 4.2689 - val_r1: 0.2785 - val_r5: 0.4666 - val_r10: 0.5447 - val_p5: 0.0933 - val_p10: 0.0545 - lr: 4.7624e-04 - 43s/epoch - 58ms/step
Epoch 49/1000
745/745 - 43s - loss: 3.1433 - r1: 0.3868 - r5: 0.6060 - r10: 0.6849 - p5: 0.1212 - p10: 0.0685 - val_loss: 4.0876 - val_r1: 0.3008 - val_r5: 0.4898 - val_r10: 0.5680 - val_p5: 0.0979 - val_p10: 0.0568 - lr: 4.7575e-04 - 43s/epoch - 58ms/step
Epoch 50/1000
745/745 - 43s - loss: 3.1418 - r1: 0.3870 - r5: 0.6061 - r10: 0.6848 - p5: 0.1212 - p10: 0.0685 - val_loss: 4.2378 - val_r1: 0.2849 - val_r5: 0.4710 - val_r10: 0.5497 - val_p5: 0.0942 - val_p10: 0.0550 - lr: 4.7525e-04 - 43s/epoch - 58ms/step
Epoch 51/1000
745/745 - 43s - loss: 3.1339 - r1: 0.3878 - r5: 0.6078 - r10: 0.6868 - p5: 0.1216 - p10: 0.0687 - val_loss: 4.3068 - val_r1: 0.2758 - val_r5: 0.4610 - val_r10: 0.5403 - val_p5: 0.0922 - val_p10: 0.0540 - lr: 4.7476e-04 - 43s/epoch - 58ms/step
Epoch 52/1000
745/745 - 43s - loss: 3.1326 - r1: 0.3881 - r5: 0.6084 - r10: 0.6872 - p5: 0.1217 - p10: 0.0687 - val_loss: 4.0654 - val_r1: 0.3041 - val_r5: 0.4940 - val_r10: 0.5690 - val_p5: 0.0988 - val_p10: 0.0569 - lr: 4.7426e-04 - 43s/epoch - 58ms/step
Epoch 53/1000
745/745 - 43s - loss: 3.1219 - r1: 0.3894 - r5: 0.6100 - r10: 0.6889 - p5: 0.1220 - p10: 0.0689 - val_loss: 4.2505 - val_r1: 0.2827 - val_r5: 0.4695 - val_r10: 0.5479 - val_p5: 0.0939 - val_p10: 0.0548 - lr: 4.7377e-04 - 43s/epoch - 58ms/step
Epoch 54/1000
745/745 - 43s - loss: 3.1178 - r1: 0.3901 - r5: 0.6094 - r10: 0.6884 - p5: 0.1219 - p10: 0.0688 - val_loss: 4.2394 - val_r1: 0.2815 - val_r5: 0.4706 - val_r10: 0.5474 - val_p5: 0.0941 - val_p10: 0.0547 - lr: 4.7327e-04 - 43s/epoch - 58ms/step
Epoch 55/1000
745/745 - 43s - loss: 3.1140 - r1: 0.3907 - r5: 0.6100 - r10: 0.6892 - p5: 0.1220 - p10: 0.0689 - val_loss: 4.2140 - val_r1: 0.2850 - val_r5: 0.4727 - val_r10: 0.5513 - val_p5: 0.0945 - val_p10: 0.0551 - lr: 4.7278e-04 - 43s/epoch - 58ms/step
Epoch 56/1000
745/745 - 43s - loss: 3.1049 - r1: 0.3929 - r5: 0.6123 - r10: 0.6902 - p5: 0.1225 - p10: 0.0690 - val_loss: 4.1820 - val_r1: 0.2899 - val_r5: 0.4782 - val_r10: 0.5559 - val_p5: 0.0956 - val_p10: 0.0556 - lr: 4.7228e-04 - 43s/epoch - 58ms/step
Epoch 57/1000
745/745 - 43s - loss: 3.0995 - r1: 0.3928 - r5: 0.6129 - r10: 0.6912 - p5: 0.1226 - p10: 0.0691 - val_loss: 4.2177 - val_r1: 0.2843 - val_r5: 0.4730 - val_r10: 0.5519 - val_p5: 0.0946 - val_p10: 0.0552 - lr: 4.7179e-04 - 43s/epoch - 58ms/step
Epoch 58/1000
745/745 - 43s - loss: 3.1033 - r1: 0.3920 - r5: 0.6117 - r10: 0.6907 - p5: 0.1223 - p10: 0.0691 - val_loss: 4.0816 - val_r1: 0.3023 - val_r5: 0.4919 - val_r10: 0.5702 - val_p5: 0.0984 - val_p10: 0.0570 - lr: 4.7129e-04 - 43s/epoch - 58ms/step
Epoch 59/1000
745/745 - 43s - loss: 3.0932 - r1: 0.3941 - r5: 0.6131 - r10: 0.6921 - p5: 0.1226 - p10: 0.0692 - val_loss: 4.1976 - val_r1: 0.2886 - val_r5: 0.4769 - val_r10: 0.5546 - val_p5: 0.0954 - val_p10: 0.0555 - lr: 4.7080e-04 - 43s/epoch - 58ms/step
Epoch 60/1000
745/745 - 43s - loss: 3.0927 - r1: 0.3933 - r5: 0.6144 - r10: 0.6932 - p5: 0.1229 - p10: 0.0693 - val_loss: 4.1808 - val_r1: 0.2903 - val_r5: 0.4781 - val_r10: 0.5562 - val_p5: 0.0956 - val_p10: 0.0556 - lr: 4.7030e-04 - 43s/epoch - 58ms/step
Epoch 61/1000
745/745 - 43s - loss: 3.0845 - r1: 0.3952 - r5: 0.6152 - r10: 0.6930 - p5: 0.1230 - p10: 0.0693 - val_loss: 4.2439 - val_r1: 0.2817 - val_r5: 0.4687 - val_r10: 0.5497 - val_p5: 0.0937 - val_p10: 0.0550 - lr: 4.6981e-04 - 43s/epoch - 58ms/step
Epoch 62/1000
745/745 - 43s - loss: 3.0827 - r1: 0.3957 - r5: 0.6149 - r10: 0.6934 - p5: 0.1230 - p10: 0.0693 - val_loss: 4.2254 - val_r1: 0.2856 - val_r5: 0.4714 - val_r10: 0.5499 - val_p5: 0.0943 - val_p10: 0.0550 - lr: 4.6931e-04 - 43s/epoch - 58ms/step
Epoch 63/1000
745/745 - 43s - loss: 3.0727 - r1: 0.3960 - r5: 0.6162 - r10: 0.6947 - p5: 0.1232 - p10: 0.0695 - val_loss: 4.2048 - val_r1: 0.2862 - val_r5: 0.4737 - val_r10: 0.5532 - val_p5: 0.0948 - val_p10: 0.0553 - lr: 4.6882e-04 - 43s/epoch - 58ms/step
Epoch 64/1000
745/745 - 43s - loss: 3.0766 - r1: 0.3969 - r5: 0.6156 - r10: 0.6944 - p5: 0.1231 - p10: 0.0694 - val_loss: 4.0856 - val_r1: 0.3011 - val_r5: 0.4924 - val_r10: 0.5705 - val_p5: 0.0985 - val_p10: 0.0571 - lr: 4.6832e-04 - 43s/epoch - 58ms/step
Epoch 65/1000
745/745 - 43s - loss: 3.0690 - r1: 0.3977 - r5: 0.6176 - r10: 0.6954 - p5: 0.1235 - p10: 0.0695 - val_loss: 4.2221 - val_r1: 0.2845 - val_r5: 0.4725 - val_r10: 0.5511 - val_p5: 0.0945 - val_p10: 0.0551 - lr: 4.6783e-04 - 43s/epoch - 58ms/step
Epoch 66/1000
745/745 - 43s - loss: 3.0649 - r1: 0.3981 - r5: 0.6174 - r10: 0.6959 - p5: 0.1235 - p10: 0.0696 - val_loss: 4.2135 - val_r1: 0.2851 - val_r5: 0.4746 - val_r10: 0.5539 - val_p5: 0.0949 - val_p10: 0.0554 - lr: 4.6733e-04 - 43s/epoch - 58ms/step
Epoch 67/1000
745/745 - 43s - loss: 3.0568 - r1: 0.4000 - r5: 0.6181 - r10: 0.6961 - p5: 0.1236 - p10: 0.0696 - val_loss: 4.0699 - val_r1: 0.3029 - val_r5: 0.4928 - val_r10: 0.5721 - val_p5: 0.0986 - val_p10: 0.0572 - lr: 4.6684e-04 - 43s/epoch - 58ms/step
Epoch 68/1000
745/745 - 43s - loss: 3.0555 - r1: 0.3999 - r5: 0.6194 - r10: 0.6971 - p5: 0.1239 - p10: 0.0697 - val_loss: 4.2056 - val_r1: 0.2886 - val_r5: 0.4769 - val_r10: 0.5551 - val_p5: 0.0954 - val_p10: 0.0555 - lr: 4.6634e-04 - 43s/epoch - 58ms/step
Epoch 69/1000
745/745 - 43s - loss: 3.0543 - r1: 0.3995 - r5: 0.6191 - r10: 0.6971 - p5: 0.1238 - p10: 0.0697 - val_loss: 4.2281 - val_r1: 0.2834 - val_r5: 0.4745 - val_r10: 0.5520 - val_p5: 0.0949 - val_p10: 0.0552 - lr: 4.6585e-04 - 43s/epoch - 58ms/step
Epoch 70/1000
745/745 - 43s - loss: 3.0508 - r1: 0.4005 - r5: 0.6209 - r10: 0.6982 - p5: 0.1242 - p10: 0.0698 - val_loss: 4.0661 - val_r1: 0.3063 - val_r5: 0.4964 - val_r10: 0.5732 - val_p5: 0.0993 - val_p10: 0.0573 - lr: 4.6535e-04 - 43s/epoch - 58ms/step
Epoch 71/1000
745/745 - 43s - loss: 3.0418 - r1: 0.4021 - r5: 0.6217 - r10: 0.6997 - p5: 0.1243 - p10: 0.0700 - val_loss: 4.2143 - val_r1: 0.2855 - val_r5: 0.4744 - val_r10: 0.5541 - val_p5: 0.0949 - val_p10: 0.0554 - lr: 4.6486e-04 - 43s/epoch - 58ms/step
Epoch 72/1000
745/745 - 43s - loss: 3.0449 - r1: 0.4007 - r5: 0.6201 - r10: 0.6980 - p5: 0.1240 - p10: 0.0698 - val_loss: 4.1903 - val_r1: 0.2897 - val_r5: 0.4779 - val_r10: 0.5567 - val_p5: 0.0956 - val_p10: 0.0557 - lr: 4.6436e-04 - 43s/epoch - 58ms/step
Epoch 73/1000
745/745 - 43s - loss: 3.0383 - r1: 0.4018 - r5: 0.6217 - r10: 0.6997 - p5: 0.1243 - p10: 0.0700 - val_loss: 4.0402 - val_r1: 0.3065 - val_r5: 0.4984 - val_r10: 0.5750 - val_p5: 0.0997 - val_p10: 0.0575 - lr: 4.6387e-04 - 43s/epoch - 58ms/step
Epoch 74/1000
745/745 - 43s - loss: 3.0320 - r1: 0.4035 - r5: 0.6228 - r10: 0.7002 - p5: 0.1246 - p10: 0.0700 - val_loss: 4.2380 - val_r1: 0.2863 - val_r5: 0.4718 - val_r10: 0.5514 - val_p5: 0.0943 - val_p10: 0.0551 - lr: 4.6337e-04 - 43s/epoch - 58ms/step
Epoch 75/1000
745/745 - 43s - loss: 3.0330 - r1: 0.4032 - r5: 0.6231 - r10: 0.7002 - p5: 0.1246 - p10: 0.0700 - val_loss: 4.1899 - val_r1: 0.2893 - val_r5: 0.4782 - val_r10: 0.5563 - val_p5: 0.0956 - val_p10: 0.0556 - lr: 4.6288e-04 - 43s/epoch - 58ms/step
Epoch 76/1000
745/745 - 43s - loss: 3.0311 - r1: 0.4033 - r5: 0.6228 - r10: 0.7003 - p5: 0.1246 - p10: 0.0700 - val_loss: 4.0516 - val_r1: 0.3076 - val_r5: 0.4975 - val_r10: 0.5743 - val_p5: 0.0995 - val_p10: 0.0574 - lr: 4.6238e-04 - 43s/epoch - 58ms/step
Epoch 77/1000
745/745 - 43s - loss: 3.0208 - r1: 0.4047 - r5: 0.6242 - r10: 0.7012 - p5: 0.1248 - p10: 0.0701 - val_loss: 4.2179 - val_r1: 0.2865 - val_r5: 0.4729 - val_r10: 0.5519 - val_p5: 0.0946 - val_p10: 0.0552 - lr: 4.6189e-04 - 43s/epoch - 58ms/step
Epoch 78/1000
745/745 - 43s - loss: 3.0271 - r1: 0.4030 - r5: 0.6239 - r10: 0.7007 - p5: 0.1248 - p10: 0.0701 - val_loss: 4.2192 - val_r1: 0.2880 - val_r5: 0.4738 - val_r10: 0.5519 - val_p5: 0.0948 - val_p10: 0.0552 - lr: 4.6139e-04 - 43s/epoch - 58ms/step
Epoch 79/1000
745/745 - 43s - loss: 3.0258 - r1: 0.4035 - r5: 0.6239 - r10: 0.7014 - p5: 0.1248 - p10: 0.0701 - val_loss: 4.0593 - val_r1: 0.3059 - val_r5: 0.4968 - val_r10: 0.5726 - val_p5: 0.0993 - val_p10: 0.0573 - lr: 4.6090e-04 - 43s/epoch - 58ms/step
Epoch 80/1000
745/745 - 43s - loss: 3.0155 - r1: 0.4056 - r5: 0.6255 - r10: 0.7028 - p5: 0.1251 - p10: 0.0703 - val_loss: 4.2052 - val_r1: 0.2863 - val_r5: 0.4760 - val_r10: 0.5538 - val_p5: 0.0952 - val_p10: 0.0554 - lr: 4.6040e-04 - 43s/epoch - 58ms/step
Epoch 81/1000
745/745 - 43s - loss: 3.0108 - r1: 0.4065 - r5: 0.6249 - r10: 0.7026 - p5: 0.1250 - p10: 0.0703 - val_loss: 4.1693 - val_r1: 0.2914 - val_r5: 0.4800 - val_r10: 0.5587 - val_p5: 0.0960 - val_p10: 0.0559 - lr: 4.5991e-04 - 43s/epoch - 58ms/step
Epoch 82/1000
745/745 - 43s - loss: 3.0152 - r1: 0.4054 - r5: 0.6261 - r10: 0.7029 - p5: 0.1252 - p10: 0.0703 - val_loss: 4.0446 - val_r1: 0.3049 - val_r5: 0.4983 - val_r10: 0.5745 - val_p5: 0.0997 - val_p10: 0.0575 - lr: 4.5941e-04 - 43s/epoch - 58ms/step
Epoch 83/1000
745/745 - 43s - loss: 3.0071 - r1: 0.4074 - r5: 0.6270 - r10: 0.7033 - p5: 0.1254 - p10: 0.0703 - val_loss: 4.2231 - val_r1: 0.2859 - val_r5: 0.4731 - val_r10: 0.5516 - val_p5: 0.0946 - val_p10: 0.0552 - lr: 4.5892e-04 - 43s/epoch - 58ms/step
Epoch 84/1000
745/745 - 43s - loss: 3.0072 - r1: 0.4072 - r5: 0.6262 - r10: 0.7031 - p5: 0.1252 - p10: 0.0703 - val_loss: 4.1877 - val_r1: 0.2906 - val_r5: 0.4781 - val_r10: 0.5575 - val_p5: 0.0956 - val_p10: 0.0557 - lr: 4.5842e-04 - 43s/epoch - 58ms/step
Epoch 85/1000
745/745 - 43s - loss: 3.0054 - r1: 0.4065 - r5: 0.6269 - r10: 0.7037 - p5: 0.1254 - p10: 0.0704 - val_loss: 4.0581 - val_r1: 0.3034 - val_r5: 0.4947 - val_r10: 0.5726 - val_p5: 0.0989 - val_p10: 0.0573 - lr: 4.5793e-04 - 43s/epoch - 58ms/step
Epoch 86/1000
745/745 - 43s - loss: 2.9989 - r1: 0.4079 - r5: 0.6282 - r10: 0.7051 - p5: 0.1257 - p10: 0.0705 - val_loss: 4.2278 - val_r1: 0.2864 - val_r5: 0.4724 - val_r10: 0.5515 - val_p5: 0.0945 - val_p10: 0.0551 - lr: 4.5743e-04 - 43s/epoch - 58ms/step
Epoch 87/1000
745/745 - 43s - loss: 2.9954 - r1: 0.4086 - r5: 0.6278 - r10: 0.7050 - p5: 0.1256 - p10: 0.0705 - val_loss: 4.1387 - val_r1: 0.2963 - val_r5: 0.4835 - val_r10: 0.5636 - val_p5: 0.0967 - val_p10: 0.0564 - lr: 4.5694e-04 - 43s/epoch - 58ms/step
Epoch 88/1000
745/745 - 43s - loss: 2.9927 - r1: 0.4095 - r5: 0.6296 - r10: 0.7054 - p5: 0.1259 - p10: 0.0705 - val_loss: 4.0357 - val_r1: 0.3096 - val_r5: 0.5004 - val_r10: 0.5772 - val_p5: 0.1001 - val_p10: 0.0577 - lr: 4.5644e-04 - 43s/epoch - 58ms/step
Epoch 89/1000
745/745 - 43s - loss: 2.9936 - r1: 0.4086 - r5: 0.6283 - r10: 0.7047 - p5: 0.1257 - p10: 0.0705 - val_loss: 4.2253 - val_r1: 0.2874 - val_r5: 0.4731 - val_r10: 0.5511 - val_p5: 0.0946 - val_p10: 0.0551 - lr: 4.5595e-04 - 43s/epoch - 58ms/step
Epoch 90/1000
745/745 - 43s - loss: 2.9928 - r1: 0.4083 - r5: 0.6285 - r10: 0.7051 - p5: 0.1257 - p10: 0.0705 - val_loss: 4.2035 - val_r1: 0.2878 - val_r5: 0.4763 - val_r10: 0.5539 - val_p5: 0.0953 - val_p10: 0.0554 - lr: 4.5545e-04 - 43s/epoch - 58ms/step
Epoch 90: early stopping
[92m[INFO] Loading best model...[0m
[93m[WARNING] Model folder already exists...[0m
Model: "ATT2ITM_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 179)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1634)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 179, 384)     4201728     ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 1634, 384)    627456      ['input_2[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 179, 256)     98560       ['embedding[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 1634, 256)    98560       ['in_rsts[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 179)         0           ['input_1[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 179, 128)     32896       ['dense[0][0]']                  
                                                                                                  
 dense_3 (Dense)                (None, 1634, 128)    32896       ['dense_2[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 179)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 179, 128)     0           ['dense_1[0][0]']                
                                                                                                  
 rest_emb (Lambda)              (None, 1634, 128)    0           ['dense_3[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 179, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 179, 1634)    0           ['word_emb[0][0]',               
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 179, 1634)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 179, 1634)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 179, 1634)    0           ['lambda_1[0][0]']               
                                                                                                  
 dropout (Dropout)              (None, 179, 1634)    0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 1634)         0           ['dropout[0][0]']                
                                                                                                  
 out (Activation)               (None, 1634)         0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 5,092,096
Trainable params: 5,092,096
Non-trainable params: 0
__________________________________________________________________________________________________
None
[92m[INFO] Model already trained. Loading weights...[0m
[93m[WARNING] Model folder already exists...[0m
Model: "ATT2ITM_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 179)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1634)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 179, 384)     4201728     ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 1634, 384)    627456      ['input_2[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 179, 256)     98560       ['embedding[0][0]']              
                                                                                                  
 dense_2 (Dense)                (None, 1634, 256)    98560       ['in_rsts[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 179)         0           ['input_1[0][0]']                
                                                                                                  
 dense_1 (Dense)                (None, 179, 128)     32896       ['dense[0][0]']                  
                                                                                                  
 dense_3 (Dense)                (None, 1634, 128)    32896       ['dense_2[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 179)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 179, 128)     0           ['dense_1[0][0]']                
                                                                                                  
 rest_emb (Lambda)              (None, 1634, 128)    0           ['dense_3[0][0]']                
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 179, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 179, 1634)    0           ['word_emb[0][0]',               
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 179, 1634)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 179, 1634)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 179, 1634)    0           ['lambda_1[0][0]']               
                                                                                                  
 dropout (Dropout)              (None, 179, 1634)    0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 1634)         0           ['dropout[0][0]']                
                                                                                                  
 out (Activation)               (None, 1634)         0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 5,092,096
Trainable params: 5,092,096
Non-trainable params: 0
__________________________________________________________________________________________________
None
[92m[INFO] Model already trained. Loading weights...[0m
