Model: "ATT2ITM_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 162)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 149)]        0           []                               
                                                                                                  
 embedding (Embedding)          (None, 162, 45)      108630      ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 149, 45)      6705        ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 162)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 162, 45)      0           ['embedding[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 149, 45)      0           ['in_rsts[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 162)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 162, 45)      0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 149, 45)      0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 162, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 162, 149)     0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 162, 149)     0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 162, 149)     0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 162, 149)     0           ['lambda_1[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 149)          0           ['dotprod[0][0]']                
                                                                                                  
 normalization (Normalization)  (None, 149)          299         ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 149)          0           ['normalization[0][0]']          
                                                                                                  
==================================================================================================
Total params: 115,634
Trainable params: 115,335
Non-trainable params: 299
__________________________________________________________________________________________________
None
Epoch 1/1000
252/252 - 4s - loss: 4.7684 - NDCG10: 0.1561 - AUC: 0.6810 - r1: 0.0683 - p1: 0.0684 - F1: 0.0038 - val_loss: 4.6081 - val_NDCG10: 0.1985 - val_AUC: 0.7255 - val_r1: 0.0912 - val_p1: 0.0910 - val_F1: 0.0100 - lr: 4.9951e-04 - e_time: 3.8101 - 4s/epoch - 15ms/step
Epoch 2/1000
252/252 - 1s - loss: 4.3882 - NDCG10: 0.2512 - AUC: 0.7712 - r1: 0.1395 - p1: 0.1395 - F1: 0.0251 - val_loss: 4.1834 - val_NDCG10: 0.3028 - val_AUC: 0.8088 - val_r1: 0.1802 - val_p1: 0.1802 - val_F1: 0.0386 - lr: 4.9901e-04 - e_time: 1.1683 - 1s/epoch - 5ms/step
Epoch 3/1000
252/252 - 1s - loss: 3.9459 - NDCG10: 0.3451 - AUC: 0.8441 - r1: 0.2067 - p1: 0.2067 - F1: 0.0598 - val_loss: 3.8169 - val_NDCG10: 0.3794 - val_AUC: 0.8570 - val_r1: 0.2368 - val_p1: 0.2374 - val_F1: 0.0780 - lr: 4.9852e-04 - e_time: 1.1359 - 1s/epoch - 5ms/step
Epoch 4/1000
252/252 - 1s - loss: 3.5942 - NDCG10: 0.4181 - AUC: 0.8763 - r1: 0.2623 - p1: 0.2623 - F1: 0.1039 - val_loss: 3.5493 - val_NDCG10: 0.4314 - val_AUC: 0.8768 - val_r1: 0.2762 - val_p1: 0.2765 - val_F1: 0.1164 - lr: 4.9802e-04 - e_time: 1.1542 - 1s/epoch - 5ms/step
Epoch 5/1000
252/252 - 1s - loss: 3.3345 - NDCG10: 0.4664 - AUC: 0.8920 - r1: 0.3014 - p1: 0.3014 - F1: 0.1435 - val_loss: 3.3608 - val_NDCG10: 0.4654 - val_AUC: 0.8873 - val_r1: 0.3019 - val_p1: 0.3017 - val_F1: 0.1487 - lr: 4.9753e-04 - e_time: 1.1454 - 1s/epoch - 5ms/step
Epoch 6/1000
252/252 - 1s - loss: 3.1438 - NDCG10: 0.5008 - AUC: 0.9009 - r1: 0.3328 - p1: 0.3327 - F1: 0.1815 - val_loss: 3.2270 - val_NDCG10: 0.4851 - val_AUC: 0.8935 - val_r1: 0.3199 - val_p1: 0.3202 - val_F1: 0.1672 - lr: 4.9703e-04 - e_time: 1.1473 - 1s/epoch - 5ms/step
Epoch 7/1000
252/252 - 1s - loss: 2.9908 - NDCG10: 0.5270 - AUC: 0.9080 - r1: 0.3562 - p1: 0.3562 - F1: 0.2094 - val_loss: 3.1288 - val_NDCG10: 0.5006 - val_AUC: 0.8978 - val_r1: 0.3344 - val_p1: 0.3342 - val_F1: 0.1919 - lr: 4.9654e-04 - e_time: 1.1349 - 1s/epoch - 5ms/step
Epoch 8/1000
252/252 - 1s - loss: 2.8712 - NDCG10: 0.5477 - AUC: 0.9127 - r1: 0.3787 - p1: 0.3787 - F1: 0.2400 - val_loss: 3.0543 - val_NDCG10: 0.5124 - val_AUC: 0.9007 - val_r1: 0.3469 - val_p1: 0.3466 - val_F1: 0.2091 - lr: 4.9604e-04 - e_time: 1.1334 - 1s/epoch - 5ms/step
Epoch 9/1000
252/252 - 1s - loss: 2.7738 - NDCG10: 0.5635 - AUC: 0.9171 - r1: 0.3923 - p1: 0.3921 - F1: 0.2568 - val_loss: 2.9975 - val_NDCG10: 0.5191 - val_AUC: 0.9033 - val_r1: 0.3504 - val_p1: 0.3507 - val_F1: 0.2209 - lr: 4.9555e-04 - e_time: 1.1269 - 1s/epoch - 5ms/step
Epoch 10/1000
252/252 - 1s - loss: 2.6889 - NDCG10: 0.5781 - AUC: 0.9205 - r1: 0.4076 - p1: 0.4075 - F1: 0.2781 - val_loss: 2.9519 - val_NDCG10: 0.5264 - val_AUC: 0.9053 - val_r1: 0.3558 - val_p1: 0.3555 - val_F1: 0.2302 - lr: 4.9505e-04 - e_time: 1.1468 - 1s/epoch - 5ms/step
Epoch 11/1000
252/252 - 1s - loss: 2.6119 - NDCG10: 0.5928 - AUC: 0.9235 - r1: 0.4228 - p1: 0.4228 - F1: 0.3002 - val_loss: 2.9156 - val_NDCG10: 0.5286 - val_AUC: 0.9067 - val_r1: 0.3588 - val_p1: 0.3588 - val_F1: 0.2377 - lr: 4.9456e-04 - e_time: 1.1419 - 1s/epoch - 5ms/step
Epoch 12/1000
252/252 - 1s - loss: 2.5449 - NDCG10: 0.6031 - AUC: 0.9263 - r1: 0.4343 - p1: 0.4343 - F1: 0.3157 - val_loss: 2.8868 - val_NDCG10: 0.5325 - val_AUC: 0.9080 - val_r1: 0.3644 - val_p1: 0.3642 - val_F1: 0.2454 - lr: 4.9406e-04 - e_time: 1.1906 - 1s/epoch - 5ms/step
Epoch 13/1000
252/252 - 1s - loss: 2.4858 - NDCG10: 0.6145 - AUC: 0.9289 - r1: 0.4468 - p1: 0.4467 - F1: 0.3315 - val_loss: 2.8624 - val_NDCG10: 0.5346 - val_AUC: 0.9092 - val_r1: 0.3662 - val_p1: 0.3662 - val_F1: 0.2503 - lr: 4.9357e-04 - e_time: 1.1668 - 1s/epoch - 5ms/step
Epoch 14/1000
252/252 - 1s - loss: 2.4317 - NDCG10: 0.6223 - AUC: 0.9310 - r1: 0.4535 - p1: 0.4535 - F1: 0.3414 - val_loss: 2.8410 - val_NDCG10: 0.5368 - val_AUC: 0.9106 - val_r1: 0.3687 - val_p1: 0.3687 - val_F1: 0.2577 - lr: 4.9307e-04 - e_time: 1.2687 - 1s/epoch - 5ms/step
Epoch 15/1000
252/252 - 1s - loss: 2.3830 - NDCG10: 0.6309 - AUC: 0.9328 - r1: 0.4649 - p1: 0.4650 - F1: 0.3586 - val_loss: 2.8245 - val_NDCG10: 0.5385 - val_AUC: 0.9112 - val_r1: 0.3720 - val_p1: 0.3718 - val_F1: 0.2651 - lr: 4.9258e-04 - e_time: 1.2325 - 1s/epoch - 5ms/step
Epoch 16/1000
252/252 - 1s - loss: 2.3386 - NDCG10: 0.6382 - AUC: 0.9347 - r1: 0.4711 - p1: 0.4710 - F1: 0.3676 - val_loss: 2.8085 - val_NDCG10: 0.5405 - val_AUC: 0.9117 - val_r1: 0.3753 - val_p1: 0.3748 - val_F1: 0.2690 - lr: 4.9208e-04 - e_time: 1.2348 - 1s/epoch - 5ms/step
Epoch 17/1000
252/252 - 1s - loss: 2.2946 - NDCG10: 0.6467 - AUC: 0.9365 - r1: 0.4838 - p1: 0.4837 - F1: 0.3820 - val_loss: 2.7965 - val_NDCG10: 0.5409 - val_AUC: 0.9128 - val_r1: 0.3741 - val_p1: 0.3733 - val_F1: 0.2701 - lr: 4.9159e-04 - e_time: 1.1510 - 1s/epoch - 5ms/step
Epoch 18/1000
252/252 - 1s - loss: 2.2578 - NDCG10: 0.6535 - AUC: 0.9380 - r1: 0.4903 - p1: 0.4904 - F1: 0.3931 - val_loss: 2.7854 - val_NDCG10: 0.5438 - val_AUC: 0.9131 - val_r1: 0.3792 - val_p1: 0.3789 - val_F1: 0.2768 - lr: 4.9109e-04 - e_time: 1.2002 - 1s/epoch - 5ms/step
Epoch 19/1000
252/252 - 1s - loss: 2.2268 - NDCG10: 0.6569 - AUC: 0.9396 - r1: 0.4944 - p1: 0.4942 - F1: 0.4006 - val_loss: 2.7769 - val_NDCG10: 0.5444 - val_AUC: 0.9139 - val_r1: 0.3802 - val_p1: 0.3807 - val_F1: 0.2794 - lr: 4.9060e-04 - e_time: 1.1425 - 1s/epoch - 5ms/step
Epoch 20/1000
252/252 - 1s - loss: 2.1840 - NDCG10: 0.6634 - AUC: 0.9414 - r1: 0.5014 - p1: 0.5015 - F1: 0.4079 - val_loss: 2.7683 - val_NDCG10: 0.5440 - val_AUC: 0.9147 - val_r1: 0.3761 - val_p1: 0.3766 - val_F1: 0.2794 - lr: 4.9010e-04 - e_time: 1.1512 - 1s/epoch - 5ms/step
Epoch 21/1000
252/252 - 1s - loss: 2.1541 - NDCG10: 0.6696 - AUC: 0.9427 - r1: 0.5093 - p1: 0.5094 - F1: 0.4208 - val_loss: 2.7615 - val_NDCG10: 0.5470 - val_AUC: 0.9151 - val_r1: 0.3814 - val_p1: 0.3814 - val_F1: 0.2827 - lr: 4.8961e-04 - e_time: 1.1334 - 1s/epoch - 5ms/step
Epoch 22/1000
252/252 - 1s - loss: 2.1240 - NDCG10: 0.6746 - AUC: 0.9438 - r1: 0.5160 - p1: 0.5160 - F1: 0.4271 - val_loss: 2.7570 - val_NDCG10: 0.5450 - val_AUC: 0.9158 - val_r1: 0.3784 - val_p1: 0.3789 - val_F1: 0.2832 - lr: 4.8911e-04 - e_time: 1.0208 - 1s/epoch - 4ms/step
Epoch 23/1000
252/252 - 1s - loss: 2.0941 - NDCG10: 0.6791 - AUC: 0.9451 - r1: 0.5197 - p1: 0.5195 - F1: 0.4324 - val_loss: 2.7518 - val_NDCG10: 0.5456 - val_AUC: 0.9160 - val_r1: 0.3789 - val_p1: 0.3789 - val_F1: 0.2857 - lr: 4.8862e-04 - e_time: 1.2128 - 1s/epoch - 5ms/step
Epoch 24/1000
252/252 - 1s - loss: 2.0669 - NDCG10: 0.6841 - AUC: 0.9464 - r1: 0.5257 - p1: 0.5260 - F1: 0.4429 - val_loss: 2.7479 - val_NDCG10: 0.5461 - val_AUC: 0.9173 - val_r1: 0.3789 - val_p1: 0.3789 - val_F1: 0.2861 - lr: 4.8812e-04 - e_time: 1.1723 - 1s/epoch - 5ms/step
Epoch 25/1000
252/252 - 1s - loss: 2.0410 - NDCG10: 0.6890 - AUC: 0.9476 - r1: 0.5329 - p1: 0.5329 - F1: 0.4509 - val_loss: 2.7445 - val_NDCG10: 0.5468 - val_AUC: 0.9173 - val_r1: 0.3807 - val_p1: 0.3807 - val_F1: 0.2887 - lr: 4.8763e-04 - e_time: 1.1803 - 1s/epoch - 5ms/step
Epoch 26/1000
252/252 - 1s - loss: 2.0180 - NDCG10: 0.6912 - AUC: 0.9485 - r1: 0.5357 - p1: 0.5357 - F1: 0.4555 - val_loss: 2.7406 - val_NDCG10: 0.5473 - val_AUC: 0.9176 - val_r1: 0.3814 - val_p1: 0.3817 - val_F1: 0.2924 - lr: 4.8713e-04 - e_time: 1.2241 - 1s/epoch - 5ms/step
Epoch 27/1000
252/252 - 1s - loss: 1.9946 - NDCG10: 0.6960 - AUC: 0.9494 - r1: 0.5420 - p1: 0.5421 - F1: 0.4646 - val_loss: 2.7389 - val_NDCG10: 0.5489 - val_AUC: 0.9177 - val_r1: 0.3853 - val_p1: 0.3853 - val_F1: 0.2977 - lr: 4.8664e-04 - e_time: 1.1976 - 1s/epoch - 5ms/step
Epoch 28/1000
252/252 - 1s - loss: 1.9758 - NDCG10: 0.6989 - AUC: 0.9507 - r1: 0.5444 - p1: 0.5447 - F1: 0.4692 - val_loss: 2.7368 - val_NDCG10: 0.5500 - val_AUC: 0.9180 - val_r1: 0.3860 - val_p1: 0.3860 - val_F1: 0.2949 - lr: 4.8614e-04 - e_time: 1.1674 - 1s/epoch - 5ms/step
Epoch 29/1000
252/252 - 1s - loss: 1.9501 - NDCG10: 0.7030 - AUC: 0.9517 - r1: 0.5469 - p1: 0.5467 - F1: 0.4712 - val_loss: 2.7356 - val_NDCG10: 0.5490 - val_AUC: 0.9187 - val_r1: 0.3845 - val_p1: 0.3840 - val_F1: 0.2961 - lr: 4.8565e-04 - e_time: 1.2140 - 1s/epoch - 5ms/step
Epoch 30/1000
252/252 - 1s - loss: 1.9313 - NDCG10: 0.7054 - AUC: 0.9527 - r1: 0.5509 - p1: 0.5507 - F1: 0.4761 - val_loss: 2.7331 - val_NDCG10: 0.5496 - val_AUC: 0.9186 - val_r1: 0.3853 - val_p1: 0.3858 - val_F1: 0.2995 - lr: 4.8515e-04 - e_time: 1.1444 - 1s/epoch - 5ms/step
Epoch 31/1000
252/252 - 1s - loss: 1.9100 - NDCG10: 0.7107 - AUC: 0.9535 - r1: 0.5584 - p1: 0.5585 - F1: 0.4877 - val_loss: 2.7327 - val_NDCG10: 0.5495 - val_AUC: 0.9187 - val_r1: 0.3858 - val_p1: 0.3858 - val_F1: 0.2957 - lr: 4.8466e-04 - e_time: 1.1477 - 1s/epoch - 5ms/step
Epoch 32/1000
252/252 - 1s - loss: 1.8963 - NDCG10: 0.7125 - AUC: 0.9540 - r1: 0.5608 - p1: 0.5608 - F1: 0.4925 - val_loss: 2.7339 - val_NDCG10: 0.5490 - val_AUC: 0.9190 - val_r1: 0.3830 - val_p1: 0.3830 - val_F1: 0.2947 - lr: 4.8416e-04 - e_time: 1.1667 - 1s/epoch - 5ms/step
Epoch 33/1000
252/252 - 1s - loss: 1.8800 - NDCG10: 0.7144 - AUC: 0.9549 - r1: 0.5632 - p1: 0.5630 - F1: 0.4920 - val_loss: 2.7333 - val_NDCG10: 0.5484 - val_AUC: 0.9195 - val_r1: 0.3837 - val_p1: 0.3832 - val_F1: 0.2972 - lr: 4.8367e-04 - e_time: 1.1644 - 1s/epoch - 5ms/step
Epoch 34/1000
252/252 - 1s - loss: 1.8608 - NDCG10: 0.7190 - AUC: 0.9557 - r1: 0.5692 - p1: 0.5691 - F1: 0.5020 - val_loss: 2.7358 - val_NDCG10: 0.5478 - val_AUC: 0.9196 - val_r1: 0.3812 - val_p1: 0.3812 - val_F1: 0.2896 - lr: 4.8317e-04 - e_time: 1.1502 - 1s/epoch - 5ms/step
Epoch 35/1000
252/252 - 1s - loss: 1.8415 - NDCG10: 0.7214 - AUC: 0.9565 - r1: 0.5704 - p1: 0.5705 - F1: 0.5032 - val_loss: 2.7343 - val_NDCG10: 0.5473 - val_AUC: 0.9198 - val_r1: 0.3825 - val_p1: 0.3822 - val_F1: 0.2933 - lr: 4.8268e-04 - e_time: 1.1627 - 1s/epoch - 5ms/step
Epoch 36/1000
252/252 - 1s - loss: 1.8285 - NDCG10: 0.7233 - AUC: 0.9570 - r1: 0.5732 - p1: 0.5730 - F1: 0.5070 - val_loss: 2.7336 - val_NDCG10: 0.5477 - val_AUC: 0.9199 - val_r1: 0.3817 - val_p1: 0.3820 - val_F1: 0.2943 - lr: 4.8218e-04 - e_time: 1.1600 - 1s/epoch - 5ms/step
Epoch 37/1000
252/252 - 1s - loss: 1.8142 - NDCG10: 0.7260 - AUC: 0.9579 - r1: 0.5779 - p1: 0.5777 - F1: 0.5149 - val_loss: 2.7370 - val_NDCG10: 0.5471 - val_AUC: 0.9200 - val_r1: 0.3814 - val_p1: 0.3812 - val_F1: 0.2971 - lr: 4.8169e-04 - e_time: 1.1837 - 1s/epoch - 5ms/step
Epoch 38/1000
252/252 - 1s - loss: 1.8020 - NDCG10: 0.7271 - AUC: 0.9583 - r1: 0.5773 - p1: 0.5772 - F1: 0.5132 - val_loss: 2.7408 - val_NDCG10: 0.5458 - val_AUC: 0.9202 - val_r1: 0.3787 - val_p1: 0.3787 - val_F1: 0.2923 - lr: 4.8119e-04 - e_time: 1.2190 - 1s/epoch - 5ms/step
Epoch 39/1000
252/252 - 1s - loss: 1.7828 - NDCG10: 0.7313 - AUC: 0.9592 - r1: 0.5820 - p1: 0.5821 - F1: 0.5187 - val_loss: 2.7412 - val_NDCG10: 0.5474 - val_AUC: 0.9200 - val_r1: 0.3830 - val_p1: 0.3830 - val_F1: 0.2970 - lr: 4.8070e-04 - e_time: 1.1678 - 1s/epoch - 5ms/step
Epoch 40/1000
252/252 - 1s - loss: 1.7746 - NDCG10: 0.7323 - AUC: 0.9595 - r1: 0.5838 - p1: 0.5838 - F1: 0.5227 - val_loss: 2.7458 - val_NDCG10: 0.5467 - val_AUC: 0.9198 - val_r1: 0.3814 - val_p1: 0.3812 - val_F1: 0.2955 - lr: 4.8020e-04 - e_time: 1.1554 - 1s/epoch - 5ms/step
Epoch 41/1000
252/252 - 1s - loss: 1.7712 - NDCG10: 0.7333 - AUC: 0.9596 - r1: 0.5856 - p1: 0.5857 - F1: 0.5248 - val_loss: 2.7456 - val_NDCG10: 0.5460 - val_AUC: 0.9200 - val_r1: 0.3822 - val_p1: 0.3822 - val_F1: 0.2941 - lr: 4.7971e-04 - e_time: 1.1551 - 1s/epoch - 5ms/step
Epoch 41: early stopping
[92m[INFO] Loading best model...[0m
[93m[WARNING] Model folder already exists...[0m
Model: "ATT2ITM_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 162)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 149)]        0           []                               
                                                                                                  
 embedding (Embedding)          (None, 162, 45)      108630      ['input_1[0][0]']                
                                                                                                  
 in_rsts (Embedding)            (None, 149, 45)      6705        ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 162)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 162, 45)      0           ['embedding[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 149, 45)      0           ['in_rsts[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 162)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 162, 45)      0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 149, 45)      0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 162, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 162, 149)     0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 162, 149)     0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 162, 149)     0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 162, 149)     0           ['lambda_1[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 149)          0           ['dotprod[0][0]']                
                                                                                                  
 normalization (Normalization)  (None, 149)          299         ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 149)          0           ['normalization[0][0]']          
                                                                                                  
==================================================================================================
Total params: 115,634
Trainable params: 115,335
Non-trainable params: 299
__________________________________________________________________________________________________
None
[92m[INFO] Model already trained. Loading weights...[0m
