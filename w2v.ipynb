{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.text_datasets.RestaurantDataset import RestaurantDataset\n",
    "from src.datasets.text_datasets.AmazonDataset import AmazonDataset\n",
    "from src.datasets.text_datasets.POIDataset import POIDataset\n",
    "\n",
    "dataset = \"restaurants\".lower().replace(\" \", \"\") \n",
    "subset = \"gijon\".lower().replace(\" \", \"\") \n",
    "\n",
    "seed = 100 \n",
    "l_rate = 1e-4\n",
    "n_epochs = 1000 \n",
    "b_size = 256 \n",
    "early_stop_patience = 10\n",
    "\n",
    "min_reviews_rst = 100\n",
    "min_reviews_usr = 1\n",
    "bow_pct_words = 10 \n",
    "language = \"es\" if subset in [\"gijon\", \"madrid\", \"barcelona\"] else \"fr\" if subset in [\"paris\"] else \"en\"\n",
    "\n",
    "remove_stopwords = 2  # 0, 1 o 2 (No quitar, quitar manual, quitar automático)\n",
    "lemmatization = True\n",
    "remove_accents = True\n",
    "remove_numbers = True\n",
    "truncate_padding = True\n",
    "\n",
    "if dataset == \"restaurants\":\n",
    "    base_path = \"/media/nas/datasets/tripadvisor/restaurants/\"\n",
    "elif dataset == \"pois\":\n",
    "    base_path = \"/media/nas/datasets/tripadvisor/pois/\"\n",
    "    language = \"es\"  # Están todas en español\n",
    "elif dataset == \"amazon\":\n",
    "    base_path = \"/media/nas/datasets/amazon/\"\n",
    "\n",
    "# DATASET CONFIG #######################################################################################################\n",
    "\n",
    "dts_cfg = {\"dataset\": dataset, \"subset\": subset, \"language\": language, \"seed\": seed, \"data_path\": base_path, \"save_path\": \"data/\",  # base_path + \"Datasets/\",\n",
    "            \"remove_stopwords\": remove_stopwords, \"remove_accents\": remove_accents, \"remove_numbers\": remove_numbers,\n",
    "            \"lemmatization\": lemmatization,\n",
    "            \"min_reviews_rst\": min_reviews_rst, \"min_reviews_usr\": min_reviews_usr,\n",
    "            \"min_df\": 5, \"bow_pct_words\": bow_pct_words, \"presencia\": False, \"text_column\": \"text\",  # BOW\n",
    "            \"n_max_words\": -50, \"test_dev_split\": .1, \"truncate_padding\": truncate_padding}\n",
    "\n",
    "if dataset == \"restaurants\": text_dataset = RestaurantDataset(dts_cfg)\n",
    "elif dataset == \"pois\": text_dataset = POIDataset(dts_cfg)\n",
    "elif dataset == \"amazon\": text_dataset = AmazonDataset(dts_cfg)\n",
    "else: raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes una lista de oraciones como datos de entrenamiento\n",
    "sentences = [\n",
    "    \"El gato está en el tejado\",\n",
    "    \"El perro está corriendo en el parque\",\n",
    "    \"Los pájaros vuelan en el cielo\"\n",
    "]\n",
    "\n",
    "# Tokenizar las oraciones en palabras individuales\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCallback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        print(f\"Epoch {self.epoch:03d} => Loss:\", loss)\n",
    "        self.epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-1204:\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread Thread-1205:\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread Thread-1206:\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/threading.py\", line 870, in run\n",
      "    self.run()\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/threading.py\", line 870, in run\n",
      "    self.run()\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1166, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1166, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1166, in _worker_loop\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 957, in _do_train_job\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 957, in _do_train_job\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 957, in _do_train_job\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 638, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 638, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "    tally += train_batch_cbow(self, sentences, alpha, work, neu1, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 638, in gensim.models.word2vec_inner.train_batch_cbow\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = text_dataset.DATA[\"TEXT_SEQUENCES\"]\n",
    "\n",
    "# Definir los parámetros del modelo\n",
    "embedding_dim = 100  # Dimensión del espacio de vectores\n",
    "window_size = 5      # Tamaño de la ventana de contexto\n",
    "epochs = 100         # Número de épocas de entrenamiento\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=embedding_dim, window=window_size, epochs=epochs, callbacks=[TrainingCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
