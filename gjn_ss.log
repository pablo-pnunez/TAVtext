2023-06-20 17:50:50.762473: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
[93m[WARNING] Model folder already exists...[0m
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
All model checkpoint layers were used when initializing TFBertModel.

All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
Model: "SSATT2ITM_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 162)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 149)]        0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  117653760   ['input_1[0][0]']                
                                thPoolingAndCrossAt                                               
                                tentions(last_hidde                                               
                                n_state=(None, 162,                                               
                                 384),                                                            
                                 pooler_output=(Non                                               
                                e, 384),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 in_rsts (Embedding)            (None, 149, 384)     57216       ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 162)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 162, 384)     0           ['tf_bert_model[0][0]']          
                                                                                                  
 dropout_38 (Dropout)           (None, 149, 384)     0           ['in_rsts[0][0]']                
                                                                                                  
 tf.cast (TFOpLambda)           (None, 162)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout_37 (Dropout)           (None, 162, 384)     0           ['word_emb[0][0]']               
                                                                                                  
 rest_emb (Lambda)              (None, 149, 384)     0           ['dropout_38[0][0]']             
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 162, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 162, 149)     0           ['dropout_37[0][0]',             
                                                                  'rest_emb[0][0]']               
                                                                                                  
 tf.tile (TFOpLambda)           (None, 162, 149)     0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 162, 149)     0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 162, 149)     0           ['lambda_1[0][0]']               
                                                                                                  
 dropout_39 (Dropout)           (None, 162, 149)     0           ['dotprod[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 149)          0           ['dropout_39[0][0]']             
                                                                                                  
 out (Activation)               (None, 149)          0           ['sum[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 117,710,976
Trainable params: 117,710,976
Non-trainable params: 0
__________________________________________________________________________________________________
None
Epoch 1/1000
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
252/252 - 97s - loss: 7.3299 - NDCG@10: 0.0683 - r1: 0.0205 - r5: 0.0802 - r10: 0.1376 - p5: 0.0160 - p10: 0.0138 - val_loss: 4.8220 - val_NDCG@10: 0.1506 - val_r1: 0.0584 - val_r5: 0.1825 - val_r10: 0.2658 - val_p5: 0.0365 - val_p10: 0.0266 - lr: 9.9901e-05 - e_time: 83.3743 - 97s/epoch - 385ms/step
Epoch 2/1000
252/252 - 85s - loss: 5.1256 - NDCG@10: 0.1446 - r1: 0.0631 - r5: 0.1738 - r10: 0.2515 - p5: 0.0348 - p10: 0.0252 - val_loss: 4.4081 - val_NDCG@10: 0.2301 - val_r1: 0.1260 - val_r5: 0.2709 - val_r10: 0.3609 - val_p5: 0.0542 - val_p10: 0.0361 - lr: 9.9802e-05 - e_time: 70.7027 - 85s/epoch - 336ms/step
Epoch 3/1000
252/252 - 85s - loss: 4.4979 - NDCG@10: 0.2309 - r1: 0.1219 - r5: 0.2749 - r10: 0.3679 - p5: 0.0550 - p10: 0.0368 - val_loss: 4.0720 - val_NDCG@10: 0.3045 - val_r1: 0.1789 - val_r5: 0.3545 - val_r10: 0.4640 - val_p5: 0.0710 - val_p10: 0.0463 - lr: 9.9703e-05 - e_time: 70.7366 - 85s/epoch - 337ms/step
Epoch 4/1000
252/252 - 85s - loss: 4.0638 - NDCG@10: 0.3013 - r1: 0.1715 - r5: 0.3577 - r10: 0.4620 - p5: 0.0715 - p10: 0.0462 - val_loss: 3.6268 - val_NDCG@10: 0.3782 - val_r1: 0.2330 - val_r5: 0.4399 - val_r10: 0.5598 - val_p5: 0.0880 - val_p10: 0.0560 - lr: 9.9604e-05 - e_time: 70.7234 - 85s/epoch - 336ms/step
Epoch 5/1000
252/252 - 85s - loss: 3.7151 - NDCG@10: 0.3634 - r1: 0.2180 - r5: 0.4312 - r10: 0.5395 - p5: 0.0863 - p10: 0.0540 - val_loss: 3.4252 - val_NDCG@10: 0.4182 - val_r1: 0.2712 - val_r5: 0.4762 - val_r10: 0.5992 - val_p5: 0.0952 - val_p10: 0.0599 - lr: 9.9505e-05 - e_time: 70.7335 - 85s/epoch - 337ms/step
Epoch 6/1000
252/252 - 85s - loss: 3.4452 - NDCG@10: 0.4105 - r1: 0.2555 - r5: 0.4850 - r10: 0.5945 - p5: 0.0970 - p10: 0.0594 - val_loss: 3.1577 - val_NDCG@10: 0.4732 - val_r1: 0.3128 - val_r5: 0.5487 - val_r10: 0.6602 - val_p5: 0.1098 - val_p10: 0.0660 - lr: 9.9406e-05 - e_time: 70.7070 - 85s/epoch - 337ms/step
Epoch 7/1000
252/252 - 85s - loss: 3.2321 - NDCG@10: 0.4500 - r1: 0.2906 - r5: 0.5292 - r10: 0.6366 - p5: 0.1058 - p10: 0.0637 - val_loss: 3.0622 - val_NDCG@10: 0.4816 - val_r1: 0.3235 - val_r5: 0.5520 - val_r10: 0.6686 - val_p5: 0.1102 - val_p10: 0.0669 - lr: 9.9307e-05 - e_time: 70.7552 - 85s/epoch - 336ms/step
Epoch 8/1000
252/252 - 85s - loss: 2.9883 - NDCG@10: 0.4931 - r1: 0.3316 - r5: 0.5738 - r10: 0.6786 - p5: 0.1148 - p10: 0.0679 - val_loss: 3.0398 - val_NDCG@10: 0.4930 - val_r1: 0.3380 - val_r5: 0.5693 - val_r10: 0.6724 - val_p5: 0.1137 - val_p10: 0.0672 - lr: 9.9208e-05 - e_time: 70.7203 - 85s/epoch - 337ms/step
Epoch 9/1000
252/252 - 85s - loss: 2.8138 - NDCG@10: 0.5237 - r1: 0.3624 - r5: 0.6063 - r10: 0.7086 - p5: 0.1213 - p10: 0.0709 - val_loss: 2.9016 - val_NDCG@10: 0.5187 - val_r1: 0.3586 - val_r5: 0.5980 - val_r10: 0.7039 - val_p5: 0.1196 - val_p10: 0.0704 - lr: 9.9109e-05 - e_time: 70.6643 - 85s/epoch - 336ms/step
Epoch 10/1000
252/252 - 71s - loss: 2.6174 - NDCG@10: 0.5581 - r1: 0.3955 - r5: 0.6394 - r10: 0.7426 - p5: 0.1279 - p10: 0.0742 - val_loss: 2.9194 - val_NDCG@10: 0.5112 - val_r1: 0.3502 - val_r5: 0.5939 - val_r10: 0.6978 - val_p5: 0.1188 - val_p10: 0.0698 - lr: 9.9010e-05 - e_time: 70.5779 - 71s/epoch - 280ms/step
Epoch 11/1000
252/252 - 71s - loss: 2.4553 - NDCG@10: 0.5856 - r1: 0.4255 - r5: 0.6690 - r10: 0.7645 - p5: 0.1338 - p10: 0.0764 - val_loss: 2.9649 - val_NDCG@10: 0.5162 - val_r1: 0.3614 - val_r5: 0.5957 - val_r10: 0.6958 - val_p5: 0.1191 - val_p10: 0.0696 - lr: 9.8911e-05 - e_time: 70.8045 - 71s/epoch - 281ms/step
Epoch 12/1000
252/252 - 71s - loss: 2.2754 - NDCG@10: 0.6185 - r1: 0.4606 - r5: 0.7054 - r10: 0.7925 - p5: 0.1411 - p10: 0.0793 - val_loss: 3.0012 - val_NDCG@10: 0.5156 - val_r1: 0.3611 - val_r5: 0.6000 - val_r10: 0.6910 - val_p5: 0.1200 - val_p10: 0.0691 - lr: 9.8812e-05 - e_time: 70.8057 - 71s/epoch - 281ms/step
Epoch 13/1000
252/252 - 71s - loss: 2.1218 - NDCG@10: 0.6450 - r1: 0.4912 - r5: 0.7288 - r10: 0.8136 - p5: 0.1457 - p10: 0.0814 - val_loss: 3.0011 - val_NDCG@10: 0.5335 - val_r1: 0.3812 - val_r5: 0.6091 - val_r10: 0.7055 - val_p5: 0.1218 - val_p10: 0.0705 - lr: 9.8713e-05 - e_time: 70.7875 - 71s/epoch - 281ms/step
Epoch 14/1000
252/252 - 71s - loss: 1.9531 - NDCG@10: 0.6751 - r1: 0.5228 - r5: 0.7592 - r10: 0.8377 - p5: 0.1518 - p10: 0.0838 - val_loss: 3.0306 - val_NDCG@10: 0.5304 - val_r1: 0.3802 - val_r5: 0.6069 - val_r10: 0.6989 - val_p5: 0.1214 - val_p10: 0.0699 - lr: 9.8614e-05 - e_time: 70.7819 - 71s/epoch - 281ms/step
Epoch 15/1000
252/252 - 71s - loss: 1.7977 - NDCG@10: 0.7026 - r1: 0.5545 - r5: 0.7845 - r10: 0.8583 - p5: 0.1570 - p10: 0.0858 - val_loss: 3.1274 - val_NDCG@10: 0.5265 - val_r1: 0.3840 - val_r5: 0.6013 - val_r10: 0.6836 - val_p5: 0.1202 - val_p10: 0.0684 - lr: 9.8515e-05 - e_time: 70.7796 - 71s/epoch - 281ms/step
Epoch 16/1000
252/252 - 71s - loss: 1.6476 - NDCG@10: 0.7273 - r1: 0.5836 - r5: 0.8078 - r10: 0.8756 - p5: 0.1616 - p10: 0.0876 - val_loss: 3.3441 - val_NDCG@10: 0.5365 - val_r1: 0.3980 - val_r5: 0.6079 - val_r10: 0.6968 - val_p5: 0.1216 - val_p10: 0.0697 - lr: 9.8416e-05 - e_time: 70.7900 - 71s/epoch - 281ms/step
Epoch 17/1000
252/252 - 71s - loss: 1.5192 - NDCG@10: 0.7495 - r1: 0.6084 - r5: 0.8313 - r10: 0.8912 - p5: 0.1662 - p10: 0.0891 - val_loss: 3.4729 - val_NDCG@10: 0.5209 - val_r1: 0.3797 - val_r5: 0.5886 - val_r10: 0.6785 - val_p5: 0.1177 - val_p10: 0.0679 - lr: 9.8317e-05 - e_time: 70.7857 - 71s/epoch - 281ms/step
Epoch 18/1000
252/252 - 71s - loss: 1.3921 - NDCG@10: 0.7728 - r1: 0.6395 - r5: 0.8496 - r10: 0.9064 - p5: 0.1699 - p10: 0.0906 - val_loss: 3.7117 - val_NDCG@10: 0.5168 - val_r1: 0.3753 - val_r5: 0.5942 - val_r10: 0.6760 - val_p5: 0.1188 - val_p10: 0.0676 - lr: 9.8218e-05 - e_time: 70.7433 - 71s/epoch - 281ms/step
Epoch 19/1000
252/252 - 71s - loss: 1.3046 - NDCG@10: 0.7883 - r1: 0.6587 - r5: 0.8631 - r10: 0.9155 - p5: 0.1726 - p10: 0.0915 - val_loss: 3.7201 - val_NDCG@10: 0.5164 - val_r1: 0.3797 - val_r5: 0.5891 - val_r10: 0.6734 - val_p5: 0.1178 - val_p10: 0.0673 - lr: 9.8119e-05 - e_time: 70.7659 - 71s/epoch - 281ms/step
Epoch 19: early stopping
[92m[INFO] Loading best model...[0m
