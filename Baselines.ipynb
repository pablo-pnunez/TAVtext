{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"models/Baselines\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "column_names = None\n",
    "all_data = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        # Definir el nombre del fichero\n",
    "        path = f\"/media/nas/pperez/code/TAVtext/{base_path}/{dataset}/{subset}/\"\n",
    "        # path+=[f for f in os.listdir(path) if \".log\" in f][0]\n",
    "        # Leer el fichero con pandas, saltando las primeras dos l√≠neas y usando el separador |\n",
    "        #df = pd.read_csv(path, skiprows=2, sep=\"|\", comment=\"-\", header=1)\n",
    "        path+=\"results.csv\"\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.sort_values(\"F1@1\", ascending=False).reset_index(drop=True)\n",
    "        df.insert(0, \"Position\", df.index+1)\n",
    "        df[\"Set\"] = dataset\n",
    "        df[\"Subset\"] = subset\n",
    "\n",
    "        if column_names is None: column_names = df.columns # [\"Model\"] + [c.strip() for c in df.columns[1:]]\n",
    "        \n",
    "        all_data.extend(df.to_records(index=False).tolist())\n",
    "\n",
    "all_data = pd.DataFrame(all_data, columns=column_names)\n",
    "all_data.to_excel(f\"/media/nas/pperez/code/TAVtext/{base_path}/all_results.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Common import print_b, print_e\n",
    "from src.datasets.text_datasets.RestaurantDataset import RestaurantDataset\n",
    "from src.datasets.text_datasets.AmazonDataset import AmazonDataset\n",
    "from src.datasets.text_datasets.POIDataset import POIDataset\n",
    "\n",
    "from cornac.eval_methods import BaseMethod\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import ReviewModality\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import json\n",
    "\n",
    "gpu = int(np.argmin(list(map(lambda x: x[\"mem_used_percent\"], nvgpu.gpu_info())))) \n",
    "\n",
    "def load_set(dataset, subset, model = \"ATT2ITM\"):\n",
    "    best_model = pd.read_csv(\"models/best_models.csv\")\n",
    "    best_model = best_model.loc[(best_model.dataset == dataset) & (best_model.subset == subset) & (best_model.model == model)][\"model_md5\"].values[0]\n",
    "    model_path = f\"models/{model}/{dataset}/{subset}/{best_model}\"\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    dts_cfg = model_config[\"dataset_config\"]\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    mdl_cfg = {\"model\": model_config[\"model\"], \"session\": {\"gpu\": gpu, \"mixed_precision\": False, \"in_md5\": False}}\n",
    "\n",
    "    print_b(f\"Loading best model: {best_model}\")\n",
    "\n",
    "    if dataset == \"restaurants\":\n",
    "        # text_dataset = RestaurantDataset(dts_cfg, load=[\"TRAIN_DEV\", \"TEXT_TOKENIZER\", \"TEXT_SEQUENCES\", \"WORD_INDEX\", \"VOCAB_SIZE\", \"MAX_LEN_PADDING\", \"N_ITEMS\", \"FEATURES_NAME\", \"BOW_SEQUENCES\"])\n",
    "        text_dataset = RestaurantDataset(dts_cfg)\n",
    "    elif dataset == \"pois\":\n",
    "        text_dataset = POIDataset(dts_cfg)\n",
    "    elif dataset == \"amazon\":\n",
    "        text_dataset = AmazonDataset(dts_cfg)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    all_data = pd.read_pickle(f\"{text_dataset.DATASET_PATH}ALL_DATA\")\n",
    "    all_data[\"rating\"]/=10\n",
    "    all_data=all_data[[\"userId\", \"id_item\", \"rating\", \"dev\", \"test\", \"text\"]]\n",
    "\n",
    "    # Eliminar usuarios desconocidos y dividir en 3 subconjuntos\n",
    "    train_data = all_data[(all_data[\"dev\"] == 0) & (all_data[\"test\"] == 0)]\n",
    "    train_users = train_data[\"userId\"].unique()\n",
    "    id_user, userId = pd.factorize(train_data[\"userId\"])\n",
    "    user_map = pd.DataFrame(zip(userId, id_user), columns=[\"userId\", \"id_user\"])\n",
    "    val_data = all_data[(all_data[\"dev\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "    test_data = all_data[(all_data[\"test\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "\n",
    "    train_data = train_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]]\n",
    "    val_data = val_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "    test_data = test_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "\n",
    "    # Instantiate a Base evaluation method using the provided train and test sets\n",
    "    eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=False, rating_threshold=3)\n",
    "    # Ojo, lo anterior elimina las repeticiones de USUARIO, ITEM\n",
    "\n",
    "    # max_vocab = 3000\n",
    "    # max_doc_freq = 0.5\n",
    "    # tokenizer = BaseTokenizer()\n",
    "    # reviews = all_data.drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False).merge(user_map)[[\"id_user\", \"id_item\", \"text\"]].to_records(index=False).tolist()\n",
    "    # eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), review_text=rm, val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=True, rating_threshold=3)\n",
    "\n",
    "    return text_dataset, eval_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "models = [\n",
    "    cornac.models.MostPop(),\n",
    "    cornac.models.BPR(seed=seed),\n",
    "    cornac.models.EASE(seed=seed)\n",
    "]\n",
    "\n",
    "model = \"ATT2ITM\"\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\"]}\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        text_dataset, eval_method = load_set(dataset, subset)\n",
    "        test_result = Experiment(\n",
    "            eval_method=eval_method,\n",
    "            show_validation=False,\n",
    "            models=models,\n",
    "            metrics=metrics,\n",
    "            save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "            verbose=True\n",
    "        ).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.hyperopt import GridSearch, Discrete\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "dataset = \"restaurants\"\n",
    "subset = \"barcelona\"\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "_, eval_method = load_set(dataset, subset)\n",
    "\n",
    "md_bpr = cornac.models.BPR(seed=seed, verbose=True) #  k=50, max_iter=200, learning_rate=0.001, lambda_reg=0.001, verbose=True\n",
    "md_ease = cornac.models.EASE(seed=seed, verbose=True) \n",
    "\n",
    "models = [\n",
    "    GridSearch(\n",
    "        model=md_bpr, space=[ \n",
    "            Discrete(\"k\", [25, 50, 75]), \n",
    "            Discrete(\"max_iter\", [50, 100, 200]), \n",
    "            Discrete(\"learning_rate\", [1e-4, 5e-4, 1e-3]), \n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    GridSearch(\n",
    "        model=md_ease, space=[\n",
    "            Discrete(\"posB\", [True, False]),\n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    ]\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "test_result = Experiment(\n",
    "    eval_method=eval_method,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=False,\n",
    "    save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "    verbose=True\n",
    ").run()\n",
    "\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
