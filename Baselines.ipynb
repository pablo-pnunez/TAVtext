{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"models/Baselines\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/nas/pperez/code/TAVtext/models/Baselines/restaurants/paris/results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# path+=[f for f in os.listdir(path) if \".log\" in f][0]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Leer el fichero con pandas, saltando las primeras dos líneas y usando el separador |\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#df = pd.read_csv(path, skiprows=2, sep=\"|\", comment=\"-\", header=1)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m path\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresults.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path)\n\u001b[1;32m     21\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(\u001b[39m\"\u001b[39m\u001b[39mF1@1\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m df\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPosition\u001b[39m\u001b[39m\"\u001b[39m, df\u001b[39m.\u001b[39mindex\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/media/nas/pperez/miniconda3/envs/TAV_text/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/nas/pperez/miniconda3/envs/TAV_text/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/media/nas/pperez/miniconda3/envs/TAV_text/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/media/nas/pperez/miniconda3/envs/TAV_text/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/media/nas/pperez/miniconda3/envs/TAV_text/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/media/nas/pperez/miniconda3/envs/TAV_text/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/nas/pperez/code/TAVtext/models/Baselines/restaurants/paris/results.csv'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Importar pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "column_names = None\n",
    "all_data = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        # Definir el nombre del fichero\n",
    "        path = f\"/media/nas/pperez/code/TAVtext/{base_path}/{dataset}/{subset}/\"\n",
    "        # path+=[f for f in os.listdir(path) if \".log\" in f][0]\n",
    "        # Leer el fichero con pandas, saltando las primeras dos líneas y usando el separador |\n",
    "        #df = pd.read_csv(path, skiprows=2, sep=\"|\", comment=\"-\", header=1)\n",
    "        path+=\"results.csv\"\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.sort_values(\"F1@1\", ascending=False).reset_index(drop=True)\n",
    "        df.insert(0, \"Position\", df.index+1)\n",
    "        df[\"Set\"] = dataset\n",
    "        df[\"Subset\"] = subset\n",
    "\n",
    "        if column_names is None: column_names = df.columns # [\"Model\"] + [c.strip() for c in df.columns[1:]]\n",
    "        \n",
    "        all_data.extend(df.to_records(index=False).tolist())\n",
    "\n",
    "all_data = pd.DataFrame(all_data, columns=column_names)\n",
    "all_data.to_excel(f\"/media/nas/pperez/code/TAVtext/{base_path}/all_results.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Common import print_b, print_e\n",
    "from src.datasets.text_datasets.RestaurantDataset import RestaurantDataset\n",
    "from src.datasets.text_datasets.AmazonDataset import AmazonDataset\n",
    "from src.datasets.text_datasets.POIDataset import POIDataset\n",
    "\n",
    "from cornac.eval_methods import BaseMethod\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import ReviewModality\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import json\n",
    "\n",
    "gpu = int(np.argmin(list(map(lambda x: x[\"mem_used_percent\"], nvgpu.gpu_info())))) \n",
    "\n",
    "def load_set(dataset, subset, model = \"ATT2ITM\"):\n",
    "    best_model = pd.read_csv(\"models/best_models.csv\")\n",
    "    best_model = best_model.loc[(best_model.dataset == dataset) & (best_model.subset == subset) & (best_model.model == model)][\"model_md5\"].values[0]\n",
    "    model_path = f\"models/{model}/{dataset}/{subset}/{best_model}\"\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    dts_cfg = model_config[\"dataset_config\"]\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    mdl_cfg = {\"model\": model_config[\"model\"], \"session\": {\"gpu\": gpu, \"mixed_precision\": False, \"in_md5\": False}}\n",
    "\n",
    "    print_b(f\"Loading best model: {best_model}\")\n",
    "\n",
    "    if dataset == \"restaurants\":\n",
    "        # text_dataset = RestaurantDataset(dts_cfg, load=[\"TRAIN_DEV\", \"TEXT_TOKENIZER\", \"TEXT_SEQUENCES\", \"WORD_INDEX\", \"VOCAB_SIZE\", \"MAX_LEN_PADDING\", \"N_ITEMS\", \"FEATURES_NAME\", \"BOW_SEQUENCES\"])\n",
    "        text_dataset = RestaurantDataset(dts_cfg)\n",
    "    elif dataset == \"pois\":\n",
    "        text_dataset = POIDataset(dts_cfg)\n",
    "    elif dataset == \"amazon\":\n",
    "        text_dataset = AmazonDataset(dts_cfg)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    all_data = pd.read_pickle(f\"{text_dataset.DATASET_PATH}ALL_DATA\")\n",
    "    all_data[\"rating\"]/=10\n",
    "    all_data=all_data[[\"userId\", \"id_item\", \"rating\", \"dev\", \"test\", \"text\"]]\n",
    "\n",
    "    # Eliminar usuarios desconocidos y dividir en 3 subconjuntos\n",
    "    train_data = all_data[(all_data[\"dev\"] == 0) & (all_data[\"test\"] == 0)]\n",
    "    train_users = train_data[\"userId\"].unique()\n",
    "    id_user, userId = pd.factorize(train_data[\"userId\"])\n",
    "    user_map = pd.DataFrame(zip(userId, id_user), columns=[\"userId\", \"id_user\"])\n",
    "    val_data = all_data[(all_data[\"dev\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "    test_data = all_data[(all_data[\"test\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "\n",
    "    train_data = train_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]]\n",
    "    val_data = val_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "    test_data = test_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "\n",
    "    # Instantiate a Base evaluation method using the provided train and test sets\n",
    "    eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=False, rating_threshold=3)\n",
    "    # Ojo, lo anterior elimina las repeticiones de USUARIO, ITEM\n",
    "\n",
    "    # max_vocab = 3000\n",
    "    # max_doc_freq = 0.5\n",
    "    # tokenizer = BaseTokenizer()\n",
    "    # reviews = all_data.drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False).merge(user_map)[[\"id_user\", \"id_item\", \"text\"]].to_records(index=False).tolist()\n",
    "    # eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), review_text=rm, val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=True, rating_threshold=3)\n",
    "\n",
    "    return text_dataset, eval_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "models = [\n",
    "    cornac.models.MostPop(),\n",
    "    cornac.models.BPR(seed=seed),\n",
    "    cornac.models.EASE(seed=seed)\n",
    "]\n",
    "\n",
    "model = \"ATT2ITM\"\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\"]}\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        text_dataset, eval_method = load_set(dataset, subset)\n",
    "        test_result = Experiment(\n",
    "            eval_method=eval_method,\n",
    "            show_validation=False,\n",
    "            models=models,\n",
    "            metrics=metrics,\n",
    "            save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "            verbose=True\n",
    "        ).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.hyperopt import GridSearch, Discrete\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "dataset = \"restaurants\"\n",
    "subset = \"barcelona\"\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "_, eval_method = load_set(dataset, subset)\n",
    "\n",
    "md_bpr = cornac.models.BPR(seed=seed, verbose=True) #  k=50, max_iter=200, learning_rate=0.001, lambda_reg=0.001, verbose=True\n",
    "md_ease = cornac.models.EASE(seed=seed, verbose=True) \n",
    "\n",
    "models = [\n",
    "    GridSearch(\n",
    "        model=md_bpr, space=[ \n",
    "            Discrete(\"k\", [25, 50, 75]), \n",
    "            Discrete(\"max_iter\", [50, 100, 200]), \n",
    "            Discrete(\"learning_rate\", [1e-4, 5e-4, 1e-3]), \n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    GridSearch(\n",
    "        model=md_ease, space=[\n",
    "            Discrete(\"posB\", [True, False]),\n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    ]\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "test_result = Experiment(\n",
    "    eval_method=eval_method,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=False,\n",
    "    save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "    verbose=True\n",
    ").run()\n",
    "\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
