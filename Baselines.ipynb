{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "base_path = \"models/Baselines\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "column_names = None\n",
    "all_data = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        # Definir el nombre del fichero\n",
    "        path = f\"/media/nas/pperez/code/TAVtext/{base_path}/{dataset}/{subset}/\"\n",
    "        # path+=[f for f in os.listdir(path) if \".log\" in f][0]\n",
    "        # Leer el fichero con pandas, saltando las primeras dos líneas y usando el separador |\n",
    "        #df = pd.read_csv(path, skiprows=2, sep=\"|\", comment=\"-\", header=1)\n",
    "        path+=\"results.csv\"\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.sort_values(\"F1@1\", ascending=False).reset_index(drop=True)\n",
    "        df.insert(0, \"Position\", df.index+1)\n",
    "        df[\"Set\"] = dataset\n",
    "        df[\"Subset\"] = subset\n",
    "\n",
    "        if column_names is None: column_names = df.columns # [\"Model\"] + [c.strip() for c in df.columns[1:]]\n",
    "        \n",
    "        all_data.extend(df.to_records(index=False).tolist())\n",
    "\n",
    "results = pd.DataFrame(all_data, columns=column_names)\n",
    "# all_data.to_excel(f\"/media/nas/pperez/code/TAVtext/baselines_evaluation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar tabla Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_excel(\"/media/nas/pperez/code/TAVtext/baselines_evaluation.xlsx\")\n",
    "# Poner nombres decentes para el artículo\n",
    "results[\"Set\"].replace({\"restaurants\": \"TAV-RSTS\", \"pois\":\"TAV-POIS\", \"amazon\":\"AM\"}, inplace=True)\n",
    "results[\"Subset\"].replace({\"digital_music\": \"Music\", \"fashion\":\"Fashion\", \"gijon\": \"Gijón\", \"barcelona\":\"Barcelona\", \"madrid\": \"Madrid\", \"paris\": \"Paris\", \"newyorkcity\": \"New York\", \"london\": \"London\"}, inplace=True)\n",
    "results[\"Model\"].replace({\"MostPop\": \"M.Pop\", \"USEM2ITM\":\"USEM\", \"BERT2ITM\":\"BERT\", \"BOW2ITM\":\"TRecX\", \"ATT2ITM\": \"AITRecX\", \"GridSearch_BPR\":\"BPR\", \"GridSearch_EASEᴿ\":\"EASEᴿ\", \"online_ibpr\":\"O.IBPR\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrrrrrr}\n",
      "\\toprule\n",
      "         &       & Model &     M.Pop &   BiVAECF &       BPR &     EASEᴿ &   BiVAECF &     TRecX &      USEM &      BERT \\\\\n",
      "Set & Subset & Metric &           &           &           &           &           &           &           &           \\\\\n",
      "\\midrule\n",
      "AM & Music & NDCG@10 &  0.017087 &  0.019177 &  0.017087 &  0.137109 &  0.019177 &  0.448943 &  0.020438 &  0.020696 \\\\\n",
      "         &       & Recall@10 &  0.036953 &  0.042766 &  0.036953 &  0.199114 &  0.042766 &  0.565356 &  0.049372 &  0.049728 \\\\\n",
      "         & Fashion & NDCG@10 &  0.201554 &  0.120556 &  0.201627 &  0.702576 &  0.120556 &  0.483608 &  0.190321 &  0.179363 \\\\\n",
      "         &       & Recall@10 &  0.263843 &  0.278205 &  0.264096 &  0.718710 &  0.278205 &  0.713580 &  0.332660 &  0.301235 \\\\\n",
      "TAV-POIS & Barcelona & NDCG@10 &  0.489284 &  0.471701 &  0.490673 &  0.495575 &  0.471701 &  0.830578 &  0.843983 &  0.849578 \\\\\n",
      "         &       & Recall@10 &  0.742966 &  0.730425 &  0.742787 &  0.749046 &  0.730425 &  0.948938 &  0.949729 &  0.959670 \\\\\n",
      "         & Madrid & NDCG@10 &  0.428150 &  0.387123 &  0.427785 &  0.453350 &  0.387123 &  0.832245 &  0.851635 &  0.858953 \\\\\n",
      "         &       & Recall@10 &  0.658553 &  0.617578 &  0.656883 &  0.682431 &  0.617578 &  0.949567 &  0.947196 &  0.958803 \\\\\n",
      "         & New York & NDCG@10 &  0.474940 &  0.463901 &  0.475041 &  0.491557 &  0.463901 &  0.847110 &  0.879589 &  0.878660 \\\\\n",
      "         &       & Recall@10 &  0.731538 &  0.726067 &  0.731788 &  0.742593 &  0.726067 &  0.963240 &  0.971541 &  0.974186 \\\\\n",
      "         & Paris & NDCG@10 &  0.605144 &  0.597789 &  0.608817 &  0.616885 &  0.597789 &  0.866698 &  0.893362 &  0.891450 \\\\\n",
      "         &       & Recall@10 &  0.841524 &  0.839556 &  0.841185 &  0.847211 &  0.839556 &  0.971347 &  0.975557 &  0.976236 \\\\\n",
      "         & London & NDCG@10 &  0.437917 &  0.407162 &  0.436963 &  0.451517 &  0.407162 &  0.849768 &  0.872671 &  0.879207 \\\\\n",
      "         &       & Recall@10 &  0.720916 &  0.716601 &  0.717141 &  0.727164 &  0.716601 &  0.964950 &  0.970598 &  0.974917 \\\\\n",
      "TAV-RSTS & Gijón & NDCG@10 &  0.165442 &  0.169375 &  0.165395 &  0.165435 &  0.169375 &  0.537447 &  0.561010 &  0.576972 \\\\\n",
      "         &       & Recall@10 &  0.268601 &  0.274902 &  0.268302 &  0.269516 &  0.274902 &  0.718737 &  0.710537 &  0.751538 \\\\\n",
      "         & Barcelona & NDCG@10 &  0.028318 &  0.028739 &  0.028667 &  0.064107 &  0.028739 &  0.412013 &  0.413274 &  0.447767 \\\\\n",
      "         &       & Recall@10 &  0.052224 &  0.052025 &  0.053199 &  0.101021 &  0.052025 &  0.558635 &  0.557574 &  0.608900 \\\\\n",
      "         & Madrid & NDCG@10 &  0.041201 &  0.031769 &  0.041257 &  0.070837 &  0.031769 &  0.480226 &  0.457656 &  0.462415 \\\\\n",
      "         &       & Recall@10 &  0.071617 &  0.052839 &  0.071720 &  0.111913 &  0.052839 &  0.620581 &  0.598635 &  0.648238 \\\\\n",
      "         & New York & NDCG@10 &  0.051455 &  0.044935 &  0.051539 &  0.078180 &  0.044935 &  0.513455 &  0.537598 &  0.519621 \\\\\n",
      "         &       & Recall@10 &  0.091244 &  0.079135 &  0.091575 &  0.131786 &  0.079135 &  0.651832 &  0.674930 &  0.701178 \\\\\n",
      "         & Paris & NDCG@10 &  0.020301 &  0.016176 &  0.020316 &  0.050598 &  0.016176 &  0.337207 &  0.341193 &  0.331982 \\\\\n",
      "         &       & Recall@10 &  0.032509 &  0.028747 &  0.032543 &  0.078042 &  0.028747 &  0.454366 &  0.457250 &  0.499795 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1063459/2010833423.py:21: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(result_df[models].to_latex())\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"NDCG@10\", \"Recall@10\"]\n",
    "models = ['M.Pop', 'BiVAECF', 'BPR', 'EASEᴿ','TRecX', 'USEM', 'BERT']\n",
    "# metrics = [\"Recall@5\", \"Recall@10\", \"NDCG@10\"]\n",
    "# models = ['M.Pop', 'BiVAECF', 'EASEᴿ', 'TRecX', 'AITRecX','BERT', 'USEM']\n",
    "\n",
    "result_df = []\n",
    "\n",
    "for metric in metrics:\n",
    "    mres = results.pivot_table(index=[ \"Set\", \"Subset\"], columns=[\"Model\"])[metric].reset_index()\n",
    "    mres[\"Metric\"] = metric\n",
    "    result_df.extend(mres.values)\n",
    "\n",
    "result_df = pd.DataFrame(result_df, columns=mres.columns)\n",
    "\n",
    "# Especificar el orden de los datos\n",
    "result_df['Set'] = pd.Categorical(result_df['Set'], [\"AM\", \"TAV-POIS\", \"TAV-RSTS\"])\n",
    "result_df['Subset'] = pd.Categorical(result_df['Subset'], [\"Music\", \"Fashion\", \"Gijón\", \"Barcelona\", \"Madrid\", \"New York\", \"Paris\", \"London\"])\n",
    "result_df['Metric'] = pd.Categorical(result_df['Metric'], metrics)\n",
    "\n",
    "result_df = result_df.pivot_table(index=[\"Set\", \"Subset\", \"Metric\"])\n",
    "print(result_df[models].to_latex())\n",
    "result_df[models].to_excel(\"baselines_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrrrr}\n",
      "\\toprule\n",
      "         &       & Model &     M.Pop &   BiVAECF &     EASEᴿ &     TRecX &   AITRecX &      BERT \\\\\n",
      "Set & Subset & Metric &           &           &           &           &           &           \\\\\n",
      "\\midrule\n",
      "AM & Music & Recall@5 &  0.021669 &  0.025197 &  0.154089 &  0.506275 &  0.563580 &  0.021075 \\\\\n",
      "         &       & Recall@10 &  0.036953 &  0.042766 &  0.199114 &  0.565356 &  0.623017 &  0.049728 \\\\\n",
      "         &       & NDCG@10 &  0.017087 &  0.019177 &  0.137109 &  0.448943 &  0.509774 &  0.020696 \\\\\n",
      "         & Fashion & Recall@5 &  0.193932 &  0.097472 &  0.709735 &  0.622896 &  0.707071 &  0.189450 \\\\\n",
      "         &       & Recall@10 &  0.263843 &  0.278205 &  0.718710 &  0.713580 &  0.790572 &  0.301235 \\\\\n",
      "         &       & NDCG@10 &  0.201554 &  0.120556 &  0.702576 &  0.483608 &  0.558292 &  0.179363 \\\\\n",
      "TAV-POIS & Barcelona & Recall@5 &  0.557025 &  0.546115 &  0.581460 &  0.911771 &  0.929056 &  0.928830 \\\\\n",
      "         &       & Recall@10 &  0.742966 &  0.730425 &  0.749046 &  0.948938 &  0.963850 &  0.959670 \\\\\n",
      "         &       & NDCG@10 &  0.489284 &  0.471701 &  0.495575 &  0.830578 &  0.846443 &  0.849578 \\\\\n",
      "         & Madrid & Recall@5 &  0.491151 &  0.438476 &  0.531793 &  0.908697 &  0.928233 &  0.925862 \\\\\n",
      "         &       & Recall@10 &  0.658553 &  0.617578 &  0.682431 &  0.949567 &  0.964689 &  0.958803 \\\\\n",
      "         &       & NDCG@10 &  0.428150 &  0.387123 &  0.453350 &  0.832245 &  0.855511 &  0.858953 \\\\\n",
      "         & New York & Recall@5 &  0.547541 &  0.535681 &  0.560917 &  0.927301 &  0.949831 &  0.950652 \\\\\n",
      "         &       & Recall@10 &  0.731538 &  0.726067 &  0.742593 &  0.963240 &  0.974460 &  0.974186 \\\\\n",
      "         &       & NDCG@10 &  0.474940 &  0.463901 &  0.491557 &  0.847110 &  0.876720 &  0.878660 \\\\\n",
      "         & Paris & Recall@5 &  0.684655 &  0.678899 &  0.707575 &  0.938349 &  0.950706 &  0.952336 \\\\\n",
      "         &       & Recall@10 &  0.841524 &  0.839556 &  0.847211 &  0.971347 &  0.976779 &  0.976236 \\\\\n",
      "         &       & NDCG@10 &  0.605144 &  0.597789 &  0.616885 &  0.866698 &  0.884536 &  0.891450 \\\\\n",
      "         & London & Recall@5 &  0.502698 &  0.479894 &  0.531646 &  0.930399 &  0.942857 &  0.950997 \\\\\n",
      "         &       & Recall@10 &  0.720916 &  0.716601 &  0.727164 &  0.964950 &  0.977907 &  0.974917 \\\\\n",
      "         &       & NDCG@10 &  0.437917 &  0.407162 &  0.451517 &  0.849768 &  0.875012 &  0.879207 \\\\\n",
      "TAV-RSTS & Gijón & Recall@5 &  0.189463 &  0.186580 &  0.181976 &  0.619516 &  0.677737 &  0.653957 \\\\\n",
      "         &       & Recall@10 &  0.268601 &  0.274902 &  0.269516 &  0.718737 &  0.771628 &  0.751538 \\\\\n",
      "         &       & NDCG@10 &  0.165442 &  0.169375 &  0.165435 &  0.537447 &  0.592078 &  0.576972 \\\\\n",
      "         & Barcelona & Recall@5 &  0.032697 &  0.032504 &  0.070135 &  0.477288 &  0.528384 &  0.524418 \\\\\n",
      "         &       & Recall@10 &  0.052224 &  0.052025 &  0.101021 &  0.558635 &  0.610514 &  0.608900 \\\\\n",
      "         &       & NDCG@10 &  0.028318 &  0.028739 &  0.064107 &  0.412013 &  0.459019 &  0.447767 \\\\\n",
      "         & Madrid & Recall@5 &  0.047674 &  0.037430 &  0.078962 &  0.547348 &  0.583243 &  0.562470 \\\\\n",
      "         &       & Recall@10 &  0.071617 &  0.052839 &  0.111913 &  0.620581 &  0.659061 &  0.648238 \\\\\n",
      "         &       & NDCG@10 &  0.041201 &  0.031769 &  0.070837 &  0.480226 &  0.512577 &  0.462415 \\\\\n",
      "         & New York & Recall@5 &  0.061976 &  0.053228 &  0.090856 &  0.578296 &  0.629899 &  0.620688 \\\\\n",
      "         &       & Recall@10 &  0.091244 &  0.079135 &  0.131786 &  0.651832 &  0.702379 &  0.701178 \\\\\n",
      "         &       & NDCG@10 &  0.051455 &  0.044935 &  0.078180 &  0.513455 &  0.562275 &  0.519621 \\\\\n",
      "         & Paris & Recall@5 &  0.021815 &  0.016916 &  0.056640 &  0.387320 &  0.449436 &  0.405998 \\\\\n",
      "         &       & Recall@10 &  0.032509 &  0.028747 &  0.078042 &  0.454366 &  0.522640 &  0.499795 \\\\\n",
      "         &       & NDCG@10 &  0.020301 &  0.016176 &  0.050598 &  0.337207 &  0.396079 &  0.331982 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3850442/1467941004.py:21: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(result_df[models].to_latex())\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"Recall@5\", \"Recall@10\", \"NDCG@10\"]\n",
    "models = ['USEM', 'AITRecX', 'BERT', 'TRecX', 'BiVAECF', 'BPR', 'M.Pop', 'EASEᴿ', 'MF', 'O.IBPR']\n",
    "models = ['M.Pop', 'BPR', 'EASEᴿ','BiVAECF',  'MF', 'O.IBPR', 'TRecX', 'AITRecX','USEM', 'BERT']\n",
    "models = ['M.Pop', 'BiVAECF', 'EASEᴿ', 'TRecX', 'AITRecX','BERT']\n",
    "\n",
    "result_df = []\n",
    "\n",
    "for metric in metrics:\n",
    "    mres = results.pivot_table(index=[ \"Set\", \"Subset\"], columns=[\"Model\"])[metric].reset_index()\n",
    "    mres[\"Metric\"] = metric\n",
    "    result_df.extend(mres.values)\n",
    "\n",
    "result_df = pd.DataFrame(result_df, columns=mres.columns)\n",
    "\n",
    "# Especificar el orden de los datos\n",
    "result_df['Set'] = pd.Categorical(result_df['Set'], [\"AM\", \"TAV-POIS\", \"TAV-RSTS\"])\n",
    "result_df['Subset'] = pd.Categorical(result_df['Subset'], [\"Music\", \"Fashion\", \"Gijón\", \"Barcelona\", \"Madrid\", \"New York\", \"Paris\", \"London\"])\n",
    "result_df['Metric'] = pd.Categorical(result_df['Metric'], metrics)\n",
    "\n",
    "result_df = result_df.pivot_table(index=[\"Set\", \"Subset\", \"Metric\"])\n",
    "print(result_df[models].to_latex())\n",
    "result_df[models].to_excel(\"baselines_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[models].to_excel(\"only_known.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Common import print_b, print_e\n",
    "from src.datasets.text_datasets.RestaurantDataset import RestaurantDataset\n",
    "from src.datasets.text_datasets.AmazonDataset import AmazonDataset\n",
    "from src.datasets.text_datasets.POIDataset import POIDataset\n",
    "\n",
    "from cornac.eval_methods import BaseMethod\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import ReviewModality\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import json\n",
    "\n",
    "gpu = int(np.argmin(list(map(lambda x: x[\"mem_used_percent\"], nvgpu.gpu_info())))) \n",
    "\n",
    "def load_set(dataset, subset, model = \"ATT2ITM\"):\n",
    "    best_model = pd.read_csv(\"models/best_models.csv\")\n",
    "    best_model = best_model.loc[(best_model.dataset == dataset) & (best_model.subset == subset) & (best_model.model == model)][\"model_md5\"].values[0]\n",
    "    model_path = f\"models/{model}/{dataset}/{subset}/{best_model}\"\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    dts_cfg = model_config[\"dataset_config\"]\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    mdl_cfg = {\"model\": model_config[\"model\"], \"session\": {\"gpu\": gpu, \"mixed_precision\": False, \"in_md5\": False}}\n",
    "\n",
    "    print_b(f\"Loading best model: {best_model}\")\n",
    "\n",
    "    if dataset == \"restaurants\":\n",
    "        # text_dataset = RestaurantDataset(dts_cfg, load=[\"TRAIN_DEV\", \"TEXT_TOKENIZER\", \"TEXT_SEQUENCES\", \"WORD_INDEX\", \"VOCAB_SIZE\", \"MAX_LEN_PADDING\", \"N_ITEMS\", \"FEATURES_NAME\", \"BOW_SEQUENCES\"])\n",
    "        text_dataset = RestaurantDataset(dts_cfg)\n",
    "    elif dataset == \"pois\":\n",
    "        text_dataset = POIDataset(dts_cfg)\n",
    "    elif dataset == \"amazon\":\n",
    "        text_dataset = AmazonDataset(dts_cfg)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    all_data = pd.read_pickle(f\"{text_dataset.DATASET_PATH}ALL_DATA\")\n",
    "    all_data[\"rating\"]/=10\n",
    "    all_data=all_data[[\"userId\", \"id_item\", \"rating\", \"dev\", \"test\", \"text\"]]\n",
    "\n",
    "    # Eliminar usuarios desconocidos y dividir en 3 subconjuntos\n",
    "    train_data = all_data[(all_data[\"dev\"] == 0) & (all_data[\"test\"] == 0)]\n",
    "    train_users = train_data[\"userId\"].unique()\n",
    "    id_user, userId = pd.factorize(train_data[\"userId\"])\n",
    "    user_map = pd.DataFrame(zip(userId, id_user), columns=[\"userId\", \"id_user\"])\n",
    "    val_data = all_data[(all_data[\"dev\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "    test_data = all_data[(all_data[\"test\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "\n",
    "    train_data = train_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]]\n",
    "    val_data = val_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "    test_data = test_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "\n",
    "    # Instantiate a Base evaluation method using the provided train and test sets\n",
    "    eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=False, rating_threshold=3)\n",
    "    # Ojo, lo anterior elimina las repeticiones de USUARIO, ITEM\n",
    "\n",
    "    # max_vocab = 3000\n",
    "    # max_doc_freq = 0.5\n",
    "    # tokenizer = BaseTokenizer()\n",
    "    # reviews = all_data.drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False).merge(user_map)[[\"id_user\", \"id_item\", \"text\"]].to_records(index=False).tolist()\n",
    "    # eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), review_text=rm, val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=True, rating_threshold=3)\n",
    "\n",
    "    return text_dataset, eval_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "models = [\n",
    "    cornac.models.MostPop(),\n",
    "    cornac.models.BPR(seed=seed),\n",
    "    cornac.models.EASE(seed=seed)\n",
    "]\n",
    "\n",
    "model = \"ATT2ITM\"\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\"]}\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        text_dataset, eval_method = load_set(dataset, subset)\n",
    "        test_result = Experiment(\n",
    "            eval_method=eval_method,\n",
    "            show_validation=False,\n",
    "            models=models,\n",
    "            metrics=metrics,\n",
    "            save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "            verbose=True\n",
    "        ).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.hyperopt import GridSearch, Discrete\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "dataset = \"restaurants\"\n",
    "subset = \"barcelona\"\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "_, eval_method = load_set(dataset, subset)\n",
    "\n",
    "md_bpr = cornac.models.BPR(seed=seed, verbose=True) #  k=50, max_iter=200, learning_rate=0.001, lambda_reg=0.001, verbose=True\n",
    "md_ease = cornac.models.EASE(seed=seed, verbose=True) \n",
    "\n",
    "models = [\n",
    "    GridSearch(\n",
    "        model=md_bpr, space=[ \n",
    "            Discrete(\"k\", [25, 50, 75]), \n",
    "            Discrete(\"max_iter\", [50, 100, 200]), \n",
    "            Discrete(\"learning_rate\", [1e-4, 5e-4, 1e-3]), \n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    GridSearch(\n",
    "        model=md_ease, space=[\n",
    "            Discrete(\"posB\", [True, False]),\n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    ]\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "test_result = Experiment(\n",
    "    eval_method=eval_method,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=False,\n",
    "    save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "    verbose=True\n",
    ").run()\n",
    "\n",
    "print(test_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con RatioSplit\n",
    "Para ver si aprende mejor que con nuestros datos ya divididos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locale import setlocale, LC_TIME\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornac\n",
    "import os\n",
    "\n",
    "city = \"madrid\"\n",
    "\n",
    "setlocale(LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "seed=2032\n",
    "data_path = f\"/media/nas/datasets/tripadvisor/restaurants/{city}/reviews.pkl\"\n",
    "data = pd.read_pickle(data_path)\n",
    "# Ordenar por fecha (- a +) y quedarse con la última (si hay repeticiones)\n",
    "data[\"date\"] =  pd.to_datetime(data[\"date\"] , format='%d de %B de %Y')\n",
    "data[\"timestamp\"] = data[\"date\"].values.astype(np.int64) // 10 ** 9\n",
    "data = data.sort_values(\"date\").reset_index(drop=True)\n",
    "data = data.drop_duplicates(subset=[\"userId\", \"restaurantId\"], keep='last', inplace=False)\n",
    "\n",
    "feedback = list(zip(data[\"userId\"], data[\"restaurantId\"], data[\"rating\"]/10))\n",
    "reviews = list(zip(data[\"userId\"], data[\"restaurantId\"], data[\"text\"].values.tolist()))\n",
    "\n",
    "cold_start = False\n",
    "eval_method = cornac.eval_methods.RatioSplit(data=feedback, test_size=0.1, val_size=0.1, exclude_unknowns=not cold_start, verbose=False, seed=123, rating_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "        |   F1@1 |  F1@10 |   F1@5 | NDCG@-1 | NDCG@1 | NDCG@10 | Precision@1 | Precision@10 | Precision@5 | Recall@1 | Recall@10 | Recall@5 | Train (s) | Test (s)\n",
      "------- + ------ + ------ + ------ + ------- + ------ + ------- + ----------- + ------------ + ----------- + -------- + --------- + -------- + --------- + --------\n",
      "MostPop | 0.0135 | 0.0128 | 0.0147 |  0.1552 | 0.0159 |  0.0334 |      0.0159 |       0.0074 |      0.0096 |   0.0127 |    0.0578 |   0.0378 |    0.0003 |  31.7415\n",
      "BPR     | 0.0135 | 0.0128 | 0.0147 |  0.1553 | 0.0159 |  0.0334 |      0.0159 |       0.0074 |      0.0096 |   0.0127 |    0.0578 |   0.0378 |    2.2495 |  39.6140\n",
      "EASEᴿ   | 0.0220 | 0.0168 | 0.0204 |  0.1658 | 0.0248 |  0.0472 |      0.0248 |       0.0097 |      0.0132 |   0.0209 |    0.0768 |   0.0531 |    4.1852 |  39.0607\n",
      "MF      | 0.0001 | 0.0002 | 0.0001 |  0.0987 | 0.0001 |  0.0003 |      0.0001 |       0.0001 |      0.0001 |   0.0001 |    0.0007 |   0.0003 |    0.1750 |  41.5808\n",
      "WBPR    | 0.0080 | 0.0022 | 0.0035 |  0.1105 | 0.0095 |  0.0091 |      0.0095 |       0.0013 |      0.0023 |   0.0075 |    0.0098 |   0.0088 |    4.1824 |  41.6347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cornac.metrics import Recall, Precision, FMeasure, NDCG, RMSE, MSE\n",
    "from cornac.hyperopt import GridSearch, Discrete\n",
    "from cornac.experiment import Experiment\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10),\n",
    "    NDCG(), NDCG(k=1), NDCG(k=10),\n",
    "    ]\n",
    "\n",
    "md_bpr = cornac.models.BPR(seed=seed, verbose=True)\n",
    "md_ease = cornac.models.EASE(seed=seed, verbose=True)\n",
    "\n",
    "models = [\n",
    "    cornac.models.MostPop(),\n",
    "    # GridSearch( model=md_bpr, space=[ Discrete(\"k\", [25, 50]), Discrete(\"max_iter\", [50, 100]), Discrete(\"learning_rate\", [1e-4, 5e-4, 1e-3]), ], metric=NDCG(), eval_method=eval_method),\n",
    "    # GridSearch( model=md_ease, space=[ Discrete(\"posB\", [True, False]), ], metric=NDCG(), eval_method=eval_method),\n",
    "    cornac.models.BPR(seed=seed, k=25, learning_rate=0.0005, max_iter=50),  # Best parameter settings: {'k': 25, 'learning_rate': 0.0005, 'max_iter': 50}\n",
    "    cornac.models.EASE(seed=seed, posB=True),\n",
    "    cornac.models.MF(seed=seed),  # Best parameter settings: {'k': 30, 'learning_rate': 5e-06, 'max_iter': 10}\n",
    "    cornac.models.WBPR(seed=seed),\n",
    "    #cornac.models.MMMF(seed=seed),  # Best parameter settings: {'k': 5, 'learning_rate': 0.001, 'max_iter': 50}\n",
    "    #cornac.models.NeuMF(seed=seed),\n",
    "    ## cornac.models.WBPR(seed=seed),\n",
    "    #cornac.models.FM(seed=seed),\n",
    "    #cornac.models.HPF(seed=seed),\n",
    "    #cornac.models.NMF(seed=seed),\n",
    "    #cornac.models.PMF(seed=seed),\n",
    "    #cornac.models.SKMeans(seed=seed),\n",
    "    #cornac.models.SVD(seed=seed),\n",
    "    #cornac.models.WMF(seed=seed),\n",
    "]\n",
    "\n",
    "experiment = Experiment(\n",
    "    eval_method=eval_method,\n",
    "    show_validation=False,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    verbose=True,\n",
    "    user_based=False,\n",
    ")\n",
    "\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, times = np.unique(eval_method.train_set.uir_tuple[0], return_counts=True)\n",
    "user_train_items = pd.DataFrame(zip(user, times), columns=[\"user\", \"train_rvws\"])\n",
    "\n",
    "for result in experiment.result:\n",
    "    result_model = result.model_name\n",
    "    result_data = result.metric_user_results\n",
    "    result_metrics = list(result_data.keys())\n",
    "    \n",
    "    model_user_results = pd.DataFrame(result_data).reset_index().rename(columns={\"index\":\"user\"}).merge(user_train_items, how=\"left\")\n",
    "    model_user_results = model_user_results.groupby(\"train_rvws\")[result_metrics].mean().reset_index()\n",
    "    model_user_results.to_excel(f\"{result_model}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_user_results[result_metrics].expanding().mean().to_excel(f\"{result_model}.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
