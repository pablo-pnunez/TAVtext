{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "base_path = \"models/Baselines\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "column_names = None\n",
    "all_data = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        # Definir el nombre del fichero\n",
    "        path = f\"/media/nas/pperez/code/TAVtext/{base_path}/{dataset}/{subset}/\"\n",
    "        # path+=[f for f in os.listdir(path) if \".log\" in f][0]\n",
    "        # Leer el fichero con pandas, saltando las primeras dos líneas y usando el separador |\n",
    "        #df = pd.read_csv(path, skiprows=2, sep=\"|\", comment=\"-\", header=1)\n",
    "        path+=\"results.csv\"\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.sort_values(\"F1@1\", ascending=False).reset_index(drop=True)\n",
    "        df.insert(0, \"Position\", df.index+1)\n",
    "        df[\"Set\"] = dataset\n",
    "        df[\"Subset\"] = subset\n",
    "\n",
    "        if column_names is None: column_names = df.columns # [\"Model\"] + [c.strip() for c in df.columns[1:]]\n",
    "        \n",
    "        all_data.extend(df.to_records(index=False).tolist())\n",
    "\n",
    "all_data = pd.DataFrame(all_data, columns=column_names)\n",
    "all_data.to_excel(f\"/media/nas/pperez/code/TAVtext/baselines_evaluation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar tabla Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(\"/media/nas/pperez/code/TAVtext/baselines_evaluation.xlsx\")\n",
    "# Poner nombres decentes para el artículo\n",
    "results[\"Set\"].replace({\"restaurants\": \"TAV-RSTS\", \"pois\":\"TAV-POIS\", \"amazon\":\"AM\"}, inplace=True)\n",
    "results[\"Subset\"].replace({\"digital_music\": \"Music\", \"fashion\":\"Fashion\", \"gijon\": \"Gijón\", \"barcelona\":\"Barcelona\", \"madrid\": \"Madrid\", \"paris\": \"Paris\", \"newyorkcity\": \"New York\", \"london\": \"London\"}, inplace=True)\n",
    "results[\"Model\"].replace({\"MostPop\": \"M.Pop\", \"USEM2ITM\":\"USEM\", \"BERT2ITM\":\"BERT\", \"BOW2ITM\":\"TRecX\", \"ATT2ITM\": \"AITRecX\", \"GridSearch_BPR\":\"BPR\", \"GridSearch_EASEᴿ\":\"EASEᴿ\", \"online_ibpr\":\"O.IBPR\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrrrrrrr}\n",
      "\\toprule\n",
      "         &       & Model &     M.Pop &       BPR &     EASEᴿ &   BiVAECF &        MF &    O.IBPR &     TRecX &      USEM &      BERT \\\\\n",
      "Set & Subset & Metric &           &           &           &           &           &           &           &           &           \\\\\n",
      "\\midrule\n",
      "AM & Music & NDCG@10 &  0.017087 &  0.017087 &  0.137109 &  0.017410 &  0.003500 &  0.224665 &  0.445238 &  0.019826 &  0.019939 \\\\\n",
      "         &       & Recall@10 &  0.036953 &  0.036953 &  0.199114 &  0.040996 &  0.006734 &  0.301342 &  0.562699 &  0.047868 &  0.048122 \\\\\n",
      "         & Fashion & NDCG@10 &  0.201554 &  0.201627 &  0.702576 &  0.319526 &  0.008964 &  0.119423 &  0.479716 &  0.188214 &  0.179036 \\\\\n",
      "         &       & Recall@10 &  0.263843 &  0.264096 &  0.718710 &  0.425158 &  0.024147 &  0.155247 &  0.710840 &  0.330329 &  0.302801 \\\\\n",
      "TAV-POIS & Barcelona & NDCG@10 &  0.489284 &  0.490673 &  0.495575 &  0.427139 &  0.248527 &  0.067308 &  0.829371 &  0.842953 &  0.848363 \\\\\n",
      "         &       & Recall@10 &  0.742966 &  0.742787 &  0.749046 &  0.721828 &  0.479490 &  0.135721 &  0.948840 &  0.949656 &  0.959212 \\\\\n",
      "         & Madrid & NDCG@10 &  0.428150 &  0.427785 &  0.453350 &  0.413206 &  0.238118 &  0.048772 &  0.831915 &  0.851004 &  0.858504 \\\\\n",
      "         &       & Recall@10 &  0.658553 &  0.656883 &  0.682431 &  0.653286 &  0.358795 &  0.083359 &  0.949815 &  0.946537 &  0.958810 \\\\\n",
      "         & New York & NDCG@10 &  0.474940 &  0.475041 &  0.491557 &  0.446613 &  0.263329 &  0.062716 &  0.846430 &  0.879409 &  0.878174 \\\\\n",
      "         &       & Recall@10 &  0.731538 &  0.731788 &  0.742593 &  0.728404 &  0.385608 &  0.128641 &  0.962977 &  0.971307 &  0.974084 \\\\\n",
      "         & Paris & NDCG@10 &  0.605144 &  0.608817 &  0.616885 &  0.580405 &  0.303897 &  0.145316 &  0.865789 &  0.892221 &  0.890476 \\\\\n",
      "         &       & Recall@10 &  0.841524 &  0.841185 &  0.847211 &  0.835847 &  0.670822 &  0.278106 &  0.970767 &  0.975062 &  0.975755 \\\\\n",
      "         & London & NDCG@10 &  0.437917 &  0.436963 &  0.451517 &  0.425466 &  0.116999 &  0.048253 &  0.850446 &  0.873329 &  0.879565 \\\\\n",
      "         &       & Recall@10 &  0.720916 &  0.717141 &  0.727164 &  0.712356 &  0.297219 &  0.106527 &  0.965260 &  0.970852 &  0.974750 \\\\\n",
      "TAV-RSTS & Gijón & NDCG@10 &  0.165442 &  0.165395 &  0.165435 &  0.165792 &  0.052852 &  0.058573 &  0.539632 &  0.564603 &  0.581294 \\\\\n",
      "         &       & Recall@10 &  0.268601 &  0.268302 &  0.269516 &  0.266403 &  0.088754 &  0.098146 &  0.721610 &  0.714831 &  0.755508 \\\\\n",
      "         & Barcelona & NDCG@10 &  0.028318 &  0.028667 &  0.064107 &  0.031025 &  0.002967 &  0.018441 &  0.413017 &  0.413664 &  0.448032 \\\\\n",
      "         &       & Recall@10 &  0.052224 &  0.053199 &  0.101021 &  0.058516 &  0.005967 &  0.028186 &  0.559342 &  0.557830 &  0.609633 \\\\\n",
      "         & Madrid & NDCG@10 &  0.041201 &  0.041257 &  0.070837 &  0.035498 &  0.001729 &  0.013531 &  0.480898 &  0.457385 &  0.460973 \\\\\n",
      "         &       & Recall@10 &  0.071617 &  0.071720 &  0.111913 &  0.064751 &  0.003637 &  0.021570 &  0.621414 &  0.598670 &  0.648313 \\\\\n",
      "         & New York & NDCG@10 &  0.051455 &  0.051539 &  0.078180 &  0.044935 &  0.002955 &  0.005555 &  0.513455 &  0.537598 &  0.519621 \\\\\n",
      "         &       & Recall@10 &  0.091244 &  0.091575 &  0.131786 &  0.079135 &  0.006871 &  0.009597 &  0.651832 &  0.674930 &  0.701178 \\\\\n",
      "         & Paris & NDCG@10 &  0.020301 &  0.020316 &  0.050598 &  0.018589 &  0.001564 &  0.010081 &  0.337236 &  0.341171 &  0.331413 \\\\\n",
      "         &       & Recall@10 &  0.032509 &  0.032543 &  0.078042 &  0.029003 &  0.003595 &  0.015828 &  0.454459 &  0.457514 &  0.499694 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1016011/1910423191.py:19: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(result_df[models].to_latex())\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"NDCG@10\", \"Recall@10\"]\n",
    "models = ['M.Pop', 'BPR', 'EASEᴿ','BiVAECF',  'MF', 'O.IBPR', 'TRecX', 'USEM', 'BERT']\n",
    "\n",
    "result_df = []\n",
    "\n",
    "for metric in metrics:\n",
    "    mres = results.pivot_table(index=[ \"Set\", \"Subset\"], columns=[\"Model\"])[metric].reset_index()\n",
    "    mres[\"Metric\"] = metric\n",
    "    result_df.extend(mres.values)\n",
    "\n",
    "result_df = pd.DataFrame(result_df, columns=mres.columns)\n",
    "\n",
    "# Especificar el orden de los datos\n",
    "result_df['Set'] = pd.Categorical(result_df['Set'], [\"AM\", \"TAV-POIS\", \"TAV-RSTS\"])\n",
    "result_df['Subset'] = pd.Categorical(result_df['Subset'], [\"Music\", \"Fashion\", \"Gijón\", \"Barcelona\", \"Madrid\", \"New York\", \"Paris\", \"London\"])\n",
    "result_df['Metric'] = pd.Categorical(result_df['Metric'], metrics)\n",
    "\n",
    "result_df = result_df.pivot_table(index=[\"Set\", \"Subset\", \"Metric\"])\n",
    "print(result_df[models].to_latex())\n",
    "result_df[models].to_excel(\"trecx_known.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrrrrrrrrr}\n",
      "\\toprule\n",
      "         &       & Model &     M.Pop &       BPR &     EASEᴿ &   BiVAECF &        MF &    O.IBPR &     TRecX &   AITRecX &      USEM &      BERT \\\\\n",
      "Set & Subset & Metric &           &           &           &           &           &           &           &           &           &           \\\\\n",
      "\\midrule\n",
      "AM & Music & Recall@5 &  0.021669 &  0.021669 &  0.154089 &  0.023926 &  0.004879 &  0.262992 &  0.502101 &  0.561171 &  0.020624 &  0.020751 \\\\\n",
      "         &       & Recall@10 &  0.036953 &  0.036953 &  0.199114 &  0.040996 &  0.006734 &  0.301342 &  0.562699 &  0.620369 &  0.047868 &  0.048122 \\\\\n",
      "         &       & NDCG@10 &  0.017087 &  0.017087 &  0.137109 &  0.017410 &  0.003500 &  0.224665 &  0.445238 &  0.507629 &  0.019826 &  0.019939 \\\\\n",
      "         & Fashion & Recall@5 &  0.193932 &  0.193932 &  0.709735 &  0.368091 &  0.009229 &  0.139064 &  0.618514 &  0.706212 &  0.187576 &  0.188063 \\\\\n",
      "         &       & Recall@10 &  0.263843 &  0.264096 &  0.718710 &  0.425158 &  0.024147 &  0.155247 &  0.710840 &  0.791474 &  0.330329 &  0.302801 \\\\\n",
      "         &       & NDCG@10 &  0.201554 &  0.201627 &  0.702576 &  0.319526 &  0.008964 &  0.119423 &  0.479716 &  0.557168 &  0.188214 &  0.179036 \\\\\n",
      "TAV-POIS & Barcelona & Recall@5 &  0.557025 &  0.555807 &  0.581460 &  0.531459 &  0.404211 &  0.066115 &  0.911199 &  0.928680 &  0.914695 &  0.928213 \\\\\n",
      "         &       & Recall@10 &  0.742966 &  0.742787 &  0.749046 &  0.721828 &  0.479490 &  0.135721 &  0.948840 &  0.963291 &  0.949656 &  0.959212 \\\\\n",
      "         &       & NDCG@10 &  0.489284 &  0.490673 &  0.495575 &  0.427139 &  0.248527 &  0.067308 &  0.829371 &  0.845394 &  0.842953 &  0.848363 \\\\\n",
      "         & Madrid & Recall@5 &  0.491151 &  0.492055 &  0.531793 &  0.488343 &  0.286585 &  0.055624 &  0.908877 &  0.928127 &  0.910138 &  0.925773 \\\\\n",
      "         &       & Recall@10 &  0.658553 &  0.656883 &  0.682431 &  0.653286 &  0.358795 &  0.083359 &  0.949815 &  0.964442 &  0.946537 &  0.958810 \\\\\n",
      "         &       & NDCG@10 &  0.428150 &  0.427785 &  0.453350 &  0.413206 &  0.238118 &  0.048772 &  0.831915 &  0.854852 &  0.851004 &  0.858504 \\\\\n",
      "         & New York & Recall@5 &  0.547541 &  0.547968 &  0.560917 &  0.531234 &  0.319318 &  0.060876 &  0.926879 &  0.949371 &  0.948908 &  0.950204 \\\\\n",
      "         &       & Recall@10 &  0.731538 &  0.731788 &  0.742593 &  0.728404 &  0.385608 &  0.128641 &  0.962977 &  0.974176 &  0.971307 &  0.974084 \\\\\n",
      "         &       & NDCG@10 &  0.474940 &  0.475041 &  0.491557 &  0.446613 &  0.263329 &  0.062716 &  0.846430 &  0.876242 &  0.879409 &  0.878174 \\\\\n",
      "         & Paris & Recall@5 &  0.684655 &  0.680518 &  0.707575 &  0.676572 &  0.463297 &  0.152098 &  0.937517 &  0.949848 &  0.951787 &  0.951510 \\\\\n",
      "         &       & Recall@10 &  0.841524 &  0.841185 &  0.847211 &  0.835847 &  0.670822 &  0.278106 &  0.970767 &  0.976309 &  0.975062 &  0.975755 \\\\\n",
      "         &       & NDCG@10 &  0.605144 &  0.608817 &  0.616885 &  0.580405 &  0.303897 &  0.145316 &  0.865789 &  0.883385 &  0.892221 &  0.890476 \\\\\n",
      "         & London & Recall@5 &  0.502698 &  0.505185 &  0.531646 &  0.509848 &  0.162153 &  0.056797 &  0.931029 &  0.942891 &  0.943061 &  0.950686 \\\\\n",
      "         &       & Recall@10 &  0.720916 &  0.717141 &  0.727164 &  0.712356 &  0.297219 &  0.106527 &  0.965260 &  0.977800 &  0.970852 &  0.974750 \\\\\n",
      "         &       & NDCG@10 &  0.437917 &  0.436963 &  0.451517 &  0.425466 &  0.116999 &  0.048253 &  0.850446 &  0.875457 &  0.873329 &  0.879565 \\\\\n",
      "TAV-RSTS & Gijón & Recall@5 &  0.189463 &  0.189463 &  0.181976 &  0.183027 &  0.043988 &  0.066461 &  0.622458 &  0.680508 &  0.631356 &  0.657203 \\\\\n",
      "         &       & Recall@10 &  0.268601 &  0.268302 &  0.269516 &  0.266403 &  0.088754 &  0.098146 &  0.721610 &  0.775847 &  0.714831 &  0.755508 \\\\\n",
      "         &       & NDCG@10 &  0.165442 &  0.165395 &  0.165435 &  0.165792 &  0.052852 &  0.058573 &  0.539632 &  0.594994 &  0.564603 &  0.581294 \\\\\n",
      "         & Barcelona & Recall@5 &  0.032697 &  0.032697 &  0.070135 &  0.034162 &  0.003382 &  0.020588 &  0.478234 &  0.528430 &  0.479369 &  0.525358 \\\\\n",
      "         &       & Recall@10 &  0.052224 &  0.053199 &  0.101021 &  0.058516 &  0.005967 &  0.028186 &  0.559342 &  0.610625 &  0.557830 &  0.609633 \\\\\n",
      "         &       & NDCG@10 &  0.028318 &  0.028667 &  0.064107 &  0.031025 &  0.002967 &  0.018441 &  0.413017 &  0.459231 &  0.413664 &  0.448032 \\\\\n",
      "         & Madrid & Recall@5 &  0.047674 &  0.047674 &  0.078962 &  0.040209 &  0.001614 &  0.015788 &  0.547981 &  0.583590 &  0.524098 &  0.562569 \\\\\n",
      "         &       & Recall@10 &  0.071617 &  0.071720 &  0.111913 &  0.064751 &  0.003637 &  0.021570 &  0.621414 &  0.659547 &  0.598670 &  0.648313 \\\\\n",
      "         &       & NDCG@10 &  0.041201 &  0.041257 &  0.070837 &  0.035498 &  0.001729 &  0.013531 &  0.480898 &  0.512624 &  0.457385 &  0.460973 \\\\\n",
      "         & New York & Recall@5 &  0.061976 &  0.061976 &  0.090856 &  0.053228 &  0.004362 &  0.006196 &  0.578296 &  0.629899 &  0.601394 &  0.620688 \\\\\n",
      "         &       & Recall@10 &  0.091244 &  0.091575 &  0.131786 &  0.079135 &  0.006871 &  0.009597 &  0.651832 &  0.702379 &  0.674930 &  0.701178 \\\\\n",
      "         &       & NDCG@10 &  0.051455 &  0.051539 &  0.078180 &  0.044935 &  0.002955 &  0.005555 &  0.513455 &  0.562275 &  0.537598 &  0.519621 \\\\\n",
      "         & Paris & Recall@5 &  0.021815 &  0.021778 &  0.056640 &  0.020956 &  0.001549 &  0.011600 &  0.387264 &  0.449589 &  0.389822 &  0.405996 \\\\\n",
      "         &       & Recall@10 &  0.032509 &  0.032543 &  0.078042 &  0.029003 &  0.003595 &  0.015828 &  0.454459 &  0.522723 &  0.457514 &  0.499694 \\\\\n",
      "         &       & NDCG@10 &  0.020301 &  0.020316 &  0.050598 &  0.018589 &  0.001564 &  0.010081 &  0.337236 &  0.396069 &  0.341171 &  0.331413 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1016011/184466216.py:20: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(result_df[models].to_latex())\n"
     ]
    }
   ],
   "source": [
    "metrics = [\"Recall@5\", \"Recall@10\", \"NDCG@10\"]\n",
    "models = ['USEM', 'AITRecX', 'BERT', 'TRecX', 'BiVAECF', 'BPR', 'M.Pop', 'EASEᴿ', 'MF', 'O.IBPR']\n",
    "models = ['M.Pop', 'BPR', 'EASEᴿ','BiVAECF',  'MF', 'O.IBPR', 'TRecX', 'AITRecX','USEM', 'BERT']\n",
    "\n",
    "result_df = []\n",
    "\n",
    "for metric in metrics:\n",
    "    mres = results.pivot_table(index=[ \"Set\", \"Subset\"], columns=[\"Model\"])[metric].reset_index()\n",
    "    mres[\"Metric\"] = metric\n",
    "    result_df.extend(mres.values)\n",
    "\n",
    "result_df = pd.DataFrame(result_df, columns=mres.columns)\n",
    "\n",
    "# Especificar el orden de los datos\n",
    "result_df['Set'] = pd.Categorical(result_df['Set'], [\"AM\", \"TAV-POIS\", \"TAV-RSTS\"])\n",
    "result_df['Subset'] = pd.Categorical(result_df['Subset'], [\"Music\", \"Fashion\", \"Gijón\", \"Barcelona\", \"Madrid\", \"New York\", \"Paris\", \"London\"])\n",
    "result_df['Metric'] = pd.Categorical(result_df['Metric'], metrics)\n",
    "\n",
    "result_df = result_df.pivot_table(index=[\"Set\", \"Subset\", \"Metric\"])\n",
    "print(result_df[models].to_latex())\n",
    "result_df[models].to_excel(\"aitrecx_known.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[models].to_excel(\"only_known.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Common import print_b, print_e\n",
    "from src.datasets.text_datasets.RestaurantDataset import RestaurantDataset\n",
    "from src.datasets.text_datasets.AmazonDataset import AmazonDataset\n",
    "from src.datasets.text_datasets.POIDataset import POIDataset\n",
    "\n",
    "from cornac.eval_methods import BaseMethod\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import ReviewModality\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import json\n",
    "\n",
    "gpu = int(np.argmin(list(map(lambda x: x[\"mem_used_percent\"], nvgpu.gpu_info())))) \n",
    "\n",
    "def load_set(dataset, subset, model = \"ATT2ITM\"):\n",
    "    best_model = pd.read_csv(\"models/best_models.csv\")\n",
    "    best_model = best_model.loc[(best_model.dataset == dataset) & (best_model.subset == subset) & (best_model.model == model)][\"model_md5\"].values[0]\n",
    "    model_path = f\"models/{model}/{dataset}/{subset}/{best_model}\"\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    dts_cfg = model_config[\"dataset_config\"]\n",
    "    with open(f'{model_path}/cfg.json') as f: model_config = json.load(f)\n",
    "    mdl_cfg = {\"model\": model_config[\"model\"], \"session\": {\"gpu\": gpu, \"mixed_precision\": False, \"in_md5\": False}}\n",
    "\n",
    "    print_b(f\"Loading best model: {best_model}\")\n",
    "\n",
    "    if dataset == \"restaurants\":\n",
    "        # text_dataset = RestaurantDataset(dts_cfg, load=[\"TRAIN_DEV\", \"TEXT_TOKENIZER\", \"TEXT_SEQUENCES\", \"WORD_INDEX\", \"VOCAB_SIZE\", \"MAX_LEN_PADDING\", \"N_ITEMS\", \"FEATURES_NAME\", \"BOW_SEQUENCES\"])\n",
    "        text_dataset = RestaurantDataset(dts_cfg)\n",
    "    elif dataset == \"pois\":\n",
    "        text_dataset = POIDataset(dts_cfg)\n",
    "    elif dataset == \"amazon\":\n",
    "        text_dataset = AmazonDataset(dts_cfg)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "    all_data = pd.read_pickle(f\"{text_dataset.DATASET_PATH}ALL_DATA\")\n",
    "    all_data[\"rating\"]/=10\n",
    "    all_data=all_data[[\"userId\", \"id_item\", \"rating\", \"dev\", \"test\", \"text\"]]\n",
    "\n",
    "    # Eliminar usuarios desconocidos y dividir en 3 subconjuntos\n",
    "    train_data = all_data[(all_data[\"dev\"] == 0) & (all_data[\"test\"] == 0)]\n",
    "    train_users = train_data[\"userId\"].unique()\n",
    "    id_user, userId = pd.factorize(train_data[\"userId\"])\n",
    "    user_map = pd.DataFrame(zip(userId, id_user), columns=[\"userId\", \"id_user\"])\n",
    "    val_data = all_data[(all_data[\"dev\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "    test_data = all_data[(all_data[\"test\"] == 1) & (all_data[\"userId\"].isin(train_users))]\n",
    "\n",
    "    train_data = train_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]]\n",
    "    val_data = val_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "    test_data = test_data.merge(user_map)[[\"id_user\", \"id_item\", \"rating\"]].drop_duplicates(subset=[\"id_user\", \"id_item\"], keep='last', inplace=False)\n",
    "\n",
    "    # Instantiate a Base evaluation method using the provided train and test sets\n",
    "    eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=False, rating_threshold=3)\n",
    "    # Ojo, lo anterior elimina las repeticiones de USUARIO, ITEM\n",
    "\n",
    "    # max_vocab = 3000\n",
    "    # max_doc_freq = 0.5\n",
    "    # tokenizer = BaseTokenizer()\n",
    "    # reviews = all_data.drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False).merge(user_map)[[\"id_user\", \"id_item\", \"text\"]].to_records(index=False).tolist()\n",
    "    # eval_method = BaseMethod.from_splits(train_data=train_data.to_records(index=False), review_text=rm, val_data=val_data.to_records(index=False), test_data=test_data.to_records(index=False),  verbose=True, rating_threshold=3)\n",
    "\n",
    "    return text_dataset, eval_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "models = [\n",
    "    cornac.models.MostPop(),\n",
    "    cornac.models.BPR(seed=seed),\n",
    "    cornac.models.EASE(seed=seed)\n",
    "]\n",
    "\n",
    "model = \"ATT2ITM\"\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\"]}\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        text_dataset, eval_method = load_set(dataset, subset)\n",
    "        test_result = Experiment(\n",
    "            eval_method=eval_method,\n",
    "            show_validation=False,\n",
    "            models=models,\n",
    "            metrics=metrics,\n",
    "            save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "            verbose=True\n",
    "        ).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cornac.metrics import Recall, Precision, FMeasure\n",
    "from cornac.hyperopt import GridSearch, Discrete\n",
    "from cornac.experiment import Experiment\n",
    "import cornac\n",
    "\n",
    "seed = 2048\n",
    "\n",
    "dataset = \"restaurants\"\n",
    "subset = \"barcelona\"\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10)\n",
    "    ]\n",
    "\n",
    "_, eval_method = load_set(dataset, subset)\n",
    "\n",
    "md_bpr = cornac.models.BPR(seed=seed, verbose=True) #  k=50, max_iter=200, learning_rate=0.001, lambda_reg=0.001, verbose=True\n",
    "md_ease = cornac.models.EASE(seed=seed, verbose=True) \n",
    "\n",
    "models = [\n",
    "    GridSearch(\n",
    "        model=md_bpr, space=[ \n",
    "            Discrete(\"k\", [25, 50, 75]), \n",
    "            Discrete(\"max_iter\", [50, 100, 200]), \n",
    "            Discrete(\"learning_rate\", [1e-4, 5e-4, 1e-3]), \n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    GridSearch(\n",
    "        model=md_ease, space=[\n",
    "            Discrete(\"posB\", [True, False]),\n",
    "        ], metric=FMeasure(k=1), eval_method=eval_method),\n",
    "    ]\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "test_result = Experiment(\n",
    "    eval_method=eval_method,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    user_based=False,\n",
    "    save_dir=f\"{base_path}/{dataset}/{subset}\", \n",
    "    verbose=True\n",
    ").run()\n",
    "\n",
    "print(test_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con RatioSplit\n",
    "Para ver si aprende mejor que con nuestros datos ya divididos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locale import setlocale, LC_TIME\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornac\n",
    "import os\n",
    "\n",
    "city = \"madrid\"\n",
    "\n",
    "setlocale(LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "seed=2032\n",
    "data_path = f\"/media/nas/datasets/tripadvisor/restaurants/{city}/reviews.pkl\"\n",
    "data = pd.read_pickle(data_path)\n",
    "# Ordenar por fecha (- a +) y quedarse con la última (si hay repeticiones)\n",
    "data[\"date\"] =  pd.to_datetime(data[\"date\"] , format='%d de %B de %Y')\n",
    "data[\"timestamp\"] = data[\"date\"].values.astype(np.int64) // 10 ** 9\n",
    "data = data.sort_values(\"date\").reset_index(drop=True)\n",
    "data = data.drop_duplicates(subset=[\"userId\", \"restaurantId\"], keep='last', inplace=False)\n",
    "\n",
    "feedback = list(zip(data[\"userId\"], data[\"restaurantId\"], data[\"rating\"]/10))\n",
    "reviews = list(zip(data[\"userId\"], data[\"restaurantId\"], data[\"text\"].values.tolist()))\n",
    "\n",
    "cold_start = False\n",
    "eval_method = cornac.eval_methods.RatioSplit(data=feedback, test_size=0.1, val_size=0.1, exclude_unknowns=not cold_start, verbose=False, seed=123, rating_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "        |   F1@1 |  F1@10 |   F1@5 | NDCG@-1 | NDCG@1 | NDCG@10 | Precision@1 | Precision@10 | Precision@5 | Recall@1 | Recall@10 | Recall@5 | Train (s) | Test (s)\n",
      "------- + ------ + ------ + ------ + ------- + ------ + ------- + ----------- + ------------ + ----------- + -------- + --------- + -------- + --------- + --------\n",
      "MostPop | 0.0135 | 0.0128 | 0.0147 |  0.1552 | 0.0159 |  0.0334 |      0.0159 |       0.0074 |      0.0096 |   0.0127 |    0.0578 |   0.0378 |    0.0003 |  31.7415\n",
      "BPR     | 0.0135 | 0.0128 | 0.0147 |  0.1553 | 0.0159 |  0.0334 |      0.0159 |       0.0074 |      0.0096 |   0.0127 |    0.0578 |   0.0378 |    2.2495 |  39.6140\n",
      "EASEᴿ   | 0.0220 | 0.0168 | 0.0204 |  0.1658 | 0.0248 |  0.0472 |      0.0248 |       0.0097 |      0.0132 |   0.0209 |    0.0768 |   0.0531 |    4.1852 |  39.0607\n",
      "MF      | 0.0001 | 0.0002 | 0.0001 |  0.0987 | 0.0001 |  0.0003 |      0.0001 |       0.0001 |      0.0001 |   0.0001 |    0.0007 |   0.0003 |    0.1750 |  41.5808\n",
      "WBPR    | 0.0080 | 0.0022 | 0.0035 |  0.1105 | 0.0095 |  0.0091 |      0.0095 |       0.0013 |      0.0023 |   0.0075 |    0.0098 |   0.0088 |    4.1824 |  41.6347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cornac.metrics import Recall, Precision, FMeasure, NDCG, RMSE, MSE\n",
    "from cornac.hyperopt import GridSearch, Discrete\n",
    "from cornac.experiment import Experiment\n",
    "\n",
    "metrics = [\n",
    "    FMeasure(k=1), FMeasure(k=5), FMeasure(k=10),\n",
    "    Recall(k=1), Recall(k=5), Recall(k=10),\n",
    "    Precision(k=1), Precision(k=5), Precision(k=10),\n",
    "    NDCG(), NDCG(k=1), NDCG(k=10),\n",
    "    ]\n",
    "\n",
    "md_bpr = cornac.models.BPR(seed=seed, verbose=True)\n",
    "md_ease = cornac.models.EASE(seed=seed, verbose=True)\n",
    "\n",
    "models = [\n",
    "    cornac.models.MostPop(),\n",
    "    # GridSearch( model=md_bpr, space=[ Discrete(\"k\", [25, 50]), Discrete(\"max_iter\", [50, 100]), Discrete(\"learning_rate\", [1e-4, 5e-4, 1e-3]), ], metric=NDCG(), eval_method=eval_method),\n",
    "    # GridSearch( model=md_ease, space=[ Discrete(\"posB\", [True, False]), ], metric=NDCG(), eval_method=eval_method),\n",
    "    cornac.models.BPR(seed=seed, k=25, learning_rate=0.0005, max_iter=50),  # Best parameter settings: {'k': 25, 'learning_rate': 0.0005, 'max_iter': 50}\n",
    "    cornac.models.EASE(seed=seed, posB=True),\n",
    "    cornac.models.MF(seed=seed),  # Best parameter settings: {'k': 30, 'learning_rate': 5e-06, 'max_iter': 10}\n",
    "    cornac.models.WBPR(seed=seed),\n",
    "    #cornac.models.MMMF(seed=seed),  # Best parameter settings: {'k': 5, 'learning_rate': 0.001, 'max_iter': 50}\n",
    "    #cornac.models.NeuMF(seed=seed),\n",
    "    ## cornac.models.WBPR(seed=seed),\n",
    "    #cornac.models.FM(seed=seed),\n",
    "    #cornac.models.HPF(seed=seed),\n",
    "    #cornac.models.NMF(seed=seed),\n",
    "    #cornac.models.PMF(seed=seed),\n",
    "    #cornac.models.SKMeans(seed=seed),\n",
    "    #cornac.models.SVD(seed=seed),\n",
    "    #cornac.models.WMF(seed=seed),\n",
    "]\n",
    "\n",
    "experiment = Experiment(\n",
    "    eval_method=eval_method,\n",
    "    show_validation=False,\n",
    "    models=models,\n",
    "    metrics=metrics,\n",
    "    verbose=True,\n",
    "    user_based=False,\n",
    ")\n",
    "\n",
    "experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, times = np.unique(eval_method.train_set.uir_tuple[0], return_counts=True)\n",
    "user_train_items = pd.DataFrame(zip(user, times), columns=[\"user\", \"train_rvws\"])\n",
    "\n",
    "for result in experiment.result:\n",
    "    result_model = result.model_name\n",
    "    result_data = result.metric_user_results\n",
    "    result_metrics = list(result_data.keys())\n",
    "    \n",
    "    model_user_results = pd.DataFrame(result_data).reset_index().rename(columns={\"index\":\"user\"}).merge(user_train_items, how=\"left\")\n",
    "    model_user_results = model_user_results.groupby(\"train_rvws\")[result_metrics].mean().reset_index()\n",
    "    model_user_results.to_excel(f\"{result_model}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_user_results[result_metrics].expanding().mean().to_excel(f\"{result_model}.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
