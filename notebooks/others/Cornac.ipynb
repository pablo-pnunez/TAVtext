{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import TextModality\n",
    "from cornac.data import Reader\n",
    "import cornac\n",
    "\n",
    "# HFT jointly models the user-item preferences and item texts (e.g., product reviews) with shared item factors\n",
    "# Below we fit HFT to the MovieLens 1M dataset. We need  both the ratings and movie plots information\n",
    "plots, movie_ids = cornac.datasets.movielens.load_plot()\n",
    "ml_1m = cornac.datasets.movielens.load_feedback(variant=\"1M\", reader=Reader(item_set=movie_ids))\n",
    "\n",
    "# Instantiate a TextModality, it makes it convenient to work with text auxiliary information\n",
    "# For more details, please refer to the tutorial on how to work with auxiliary data\n",
    "item_text_modality = TextModality(corpus=plots, ids=movie_ids, tokenizer=BaseTokenizer(sep=\"\\t\", stop_words=\"english\"), max_vocab=5000, max_doc_freq=0.5)\n",
    "\n",
    "# Define an evaluation method to split feedback into train and test sets\n",
    "ratio_split = RatioSplit( data=ml_1m, test_size=0.1, exclude_unknowns=True, item_text=item_text_modality, verbose=True, seed=123 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ATTREX] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8641cc3f794563955b58b01d74c16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437/437 [==============================] - 1s 2ms/step - loss: 3.7508\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 3.4215\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 2.3586\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 1.4920\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 1.1777\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 1.0559\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 0.9959\n",
      "437/437 [==============================] - 1s 2ms/step - loss: 0.9628\n",
      "437/437 [==============================] - 1s 1ms/step - loss: 0.9432\n",
      "437/437 [==============================] - 1s 1ms/step - loss: 0.9311\n",
      "\n",
      "[ATTREX] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efc3c5286ff4c8ab8f99cdc271da34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rating:   0%|          | 0/99343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fcbb2cff5d46a6b09f0c137c922da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/5960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MF] Training started!\n",
      "\n",
      "[MF] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e283918c6cac4744a2d514196ce98259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rating:   0%|          | 0/99343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fd79a807124c76a0209d8c83d757a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/5960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GMF] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8884325c55f64d099ac97146518692d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GMF] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35486a241daa46869b2b14d039752a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rating:   0%|          | 0/99343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b953f76d2f04c28af31ec94a60abc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/5960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MLP] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a60fb7bea14d249aae6ca0490fadb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from cornac.metrics import MAE, RMSE, Precision, Recall, NDCG, AUC, MAP, FMeasure\n",
    "from cornac.hyperopt import GridSearch, RandomSearch, Discrete, Continuous\n",
    "\n",
    "from cornac.models import Recommender\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm.auto import trange\n",
    "from cornac import models\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ATTREX(Recommender):\n",
    "\n",
    "    def __init__(self, name=\"ATTREX\", learning_rate=5e-4, n_epochs=10, batch_size=2048, vocab_size=None, seed=2032, embedding_dim=8, trainable=True, verbose=False):\n",
    "        Recommender.__init__(self, name=name, trainable=trainable, verbose=verbose)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seed = seed\n",
    "\n",
    "        self.dot_prods = None\n",
    "\n",
    "    def __get_model__(self, num_users, num_items):\n",
    "        \n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "        # Capa de entrada para el usuario\n",
    "        user_input = tf.keras.layers.Input(shape=(1), dtype='int32', name=\"user\")\n",
    "        user_embedding = tf.keras.layers.Embedding(num_users, self.embedding_dim, name=\"user_emb\")(user_input)\n",
    "        user_vec = tf.keras.layers.Flatten()(user_embedding)\n",
    "        # Capa de entrada para el item\n",
    "        item_input = tf.keras.layers.Input(shape=(1), dtype='int32',name=\"item\")\n",
    "        item_embedding = tf.keras.layers.Embedding(num_items, self.embedding_dim, name=\"item_emb\")(item_input)\n",
    "        item_vec = tf.keras.layers.Flatten()(item_embedding)\n",
    "        # Concatenar los embeddings del usuario y el item\n",
    "        output_layer = tf.keras.layers.Dot(axes=1)([user_vec, item_vec])\n",
    "        # output_layer = tf.keras.layers.Activation(\"relu\")(output_layer)\n",
    "        # Crear el modelo\n",
    "        model =tf.keras.Model(inputs=[user_input, item_input], outputs=output_layer)\n",
    "        # Compilar\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(self.learning_rate), loss=root_mean_squared_error)\n",
    "        return model\n",
    "    \n",
    "    def fit(self, train_set, val_set=None):\n",
    "\n",
    "        Recommender.fit(self, train_set, val_set)\n",
    "        # Create Model\n",
    "        self.model = self.__get_model__(train_set.total_users, train_set.total_items)\n",
    "        \n",
    "        # Obtain Data\n",
    "        all_train_data = train_set.uir_tuple\n",
    "        train_tfset = tf.data.Dataset.from_tensor_slices(({\"user\": all_train_data[0], \"item\": all_train_data[1]}, all_train_data[2]))\n",
    "        train_tfset = train_tfset.batch(self.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "        val_tfset = None\n",
    "        if val_set is not None:\n",
    "            all_val_data = val_set.uir_tuple\n",
    "            val_tfset = tf.data.Dataset.from_tensor_slices(({\"user\": all_val_data[0], \"item\": all_val_data[1]}, all_val_data[2]))\n",
    "            val_tfset = val_tfset.batch(self.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Training loop\n",
    "        # self.model.fit(train_tfset, epochs=self.n_epochs, validation_data=val_tfset)\n",
    "        loop = trange(self.n_epochs)\n",
    "        for n_epoch in loop:\n",
    "            self.model.fit(train_tfset, epochs=1, validation_data=val_tfset)\n",
    "        loop.close()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self, user_id, item_id=None):\n",
    "        if self.dot_prods is None:\n",
    "            user_embs = self.model.get_layer(\"user_emb\").weights[0]\n",
    "            item_embs = self.model.get_layer(\"item_emb\").weights[0]\n",
    "            self.dot_prods = tf.tensordot(user_embs, tf.transpose(item_embs), axes=1).numpy()\n",
    "\n",
    "        if item_id is None:\n",
    "            return self.dot_prods[user_id]\n",
    "        else:\n",
    "            return [self.dot_prods[user_id, item_id]]\n",
    "\n",
    "# Register your model in Cornac's model dictionary\n",
    "models.ATTREX = ATTREX\n",
    "from cornac.models import ATTREX\n",
    "\n",
    "# Define metrics to evaluate the models\n",
    "metrics = [MAE(), RMSE(), FMeasure(), Precision(k=10), Recall(k=1), Recall(k=5), Recall(k=10), NDCG(k=10), AUC(), MAP()]\n",
    "# Instantiate models\n",
    "m_attrex = cornac.models.ATTREX(vocab_size=5000, seed=123)\n",
    "\n",
    "m_mf = cornac.models.MF(max_iter=50, seed=123)\n",
    "gmf = cornac.models.GMF(num_factors=8, num_epochs=10, learner=\"adam\", batch_size=256, lr=0.001, num_neg=50, seed=123, )\n",
    "mlp = cornac.models.MLP(layers=[64, 32, 16, 8], act_fn=\"tanh\", learner=\"adam\", num_epochs=10, batch_size=256, lr=0.001, num_neg=50, seed=123, )\n",
    "neumf1 = cornac.models.NeuMF(num_factors=8, layers=[64, 32, 16, 8], act_fn=\"tanh\", learner=\"adam\", num_epochs=10, batch_size=256, lr=0.001, num_neg=50, seed=123, )\n",
    "neumf2 = cornac.models.NeuMF(name=\"NeuMF_pretrained\", learner=\"adam\", num_epochs=10, batch_size=256, lr=0.001, num_neg=50, seed=123, num_factors=gmf.num_factors, layers=mlp.layers, act_fn=mlp.act_fn, ).pretrain(gmf, mlp)\n",
    "\n",
    "# GridSearch\n",
    "# gs_m_attrex = GridSearch( model=m_attrex, space=[ Discrete(\"learning_rate\", np.linspace(0.0001, 0.5, 2)), ], metric=AUC(), eval_method=ratio_split, )\n",
    "# Experiment\n",
    "cornac.Experiment(eval_method=ratio_split, models=[m_attrex, m_mf, gmf, mlp, neumf1, neumf2], metrics=metrics, user_based=False).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Grid search: max_iter = {:.2f}'.format(gs_hft.best_params.get('max_iter')))\n",
    "print('Grid search: lambda_reg = {:.2f}'.format(gs_hft.best_params.get('batch_size')))\n",
    "print('Grid search: learning_rate = {:.2f}'.format(gs_cdl.best_params.get('learning_rate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rating and sentiment information\n",
    "data = cornac.datasets.amazon_toy.load_feedback()\n",
    "sentiment = cornac.datasets.amazon_toy.load_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models, here we are comparing: Biased MF, PMF, and BPR\n",
    "models = [\n",
    "    ATTEX(),\n",
    "    MF(k=10, max_iter=25, learning_rate=0.01, lambda_reg=0.02, use_bias=True, seed=123),\n",
    "    # PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001, seed=123),\n",
    "    # NeuMF(seed=123),\n",
    "    # BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),\n",
    "    # HFT(k=10, max_iter=200, seed=123),\n",
    "    # BiVAECF(k=10, n_epochs=100, learning_rate=0.001, seed=123)\n",
    "]\n",
    "\n",
    "# put it together in an experiment, voilà!\n",
    "cornac.Experiment(eval_method=ratio_split, models=models, metrics=metrics, user_based=True).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
