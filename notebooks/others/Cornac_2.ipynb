{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locale import setlocale, LC_TIME\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm.auto import trange\n",
    "from cornac import models\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cornac\n",
    "import os\n",
    "\n",
    "city = \"gijon\"\n",
    "setlocale(LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "data_path = f\"/media/nas/datasets/tripadvisor/restaurants/{city}/reviews.pkl\"\n",
    "data = pd.read_pickle(data_path)\n",
    "# Ordenar por fecha (- a +) y quedarse con la Ãºltima (si hay repeticiones)\n",
    "data[\"date\"] =  pd.to_datetime(data[\"date\"] , format='%d de %B de %Y')\n",
    "data[\"timestamp\"] = data[\"date\"].values.astype(np.int64) // 10 ** 9\n",
    "data = data.sort_values(\"date\").reset_index(drop=True)\n",
    "data = data.drop_duplicates(subset=[\"userId\", \"restaurantId\"], keep='last', inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.datasets import amazon_digital_music\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import ReviewModality\n",
    "feedback = list(zip(data[\"userId\"], data[\"restaurantId\"], data[\"rating\"]/10))\n",
    "# feedback = amazon_digital_music.load_feedback()\n",
    "# reviews = amazon_digital_music.load_review()\n",
    "reviews = list(zip(data[\"userId\"], data[\"restaurantId\"], data[\"text\"].values.tolist()))\n",
    "\n",
    "cold_start = False\n",
    "ratio_split = RatioSplit(data=feedback, test_size=0.2, val_size=0.2, exclude_unknowns=not cold_start, verbose=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cornac.data.text.ReviewModality at 0x7fb54a90f430>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BaseTokenizer()\n",
    "max_vocab = 3000\n",
    "max_doc_freq = 0.5\n",
    "\n",
    "train_text = ReviewModality(data=reviews, tokenizer=tokenizer, max_vocab=max_vocab, max_doc_freq=max_doc_freq)\n",
    "train_text.build(ratio_split.train_set.uid_map, ratio_split.train_set.iid_map, ratio_split.train_set.dok_matrix)\n",
    "ratio_split.add_modalities(item_text=train_text)\n",
    "\n",
    "val_text = ReviewModality(data=reviews, tokenizer=tokenizer, max_vocab=max_vocab, max_doc_freq=max_doc_freq)\n",
    "val_text.build(ratio_split.val_set.uid_map, ratio_split.val_set.iid_map, ratio_split.val_set.dok_matrix)\n",
    "\n",
    "test_text = ReviewModality(data=reviews, tokenizer=tokenizer, max_vocab=max_vocab, max_doc_freq=max_doc_freq)\n",
    "test_text.build(ratio_split.test_set.uid_map, ratio_split.test_set.iid_map, ratio_split.test_set.dok_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.metrics import MAE, RMSE, Precision, Recall, NDCG, AUC, MAP, FMeasure\n",
    "from cornac.hyperopt import GridSearch, RandomSearch, Discrete, Continuous\n",
    "\n",
    "from cornac.models import Recommender\n",
    "\n",
    "\n",
    "class ATTREX(Recommender):\n",
    "\n",
    "    def __init__(self, name=\"ATTREX\", learning_rate=5e-4, max_iter=50, batch_size=512, vocab_size=3000, max_text_size=200, seed=2032, embedding_dim=16, train_text=None, val_text=None, test_text=None, trainable=True, verbose=False):\n",
    "        Recommender.__init__(self, name=name, trainable=trainable, verbose=verbose)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_text_size = max_text_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_text, self.val_text, self.test_text = (train_text, val_text, test_text)\n",
    "        self.max_iter = max_iter\n",
    "        self.seed = seed\n",
    "        self.gpu=0\n",
    "\n",
    "        self.test_predictions  = None\n",
    "\n",
    "    def __config_session__(self, mixed_precision=True):\n",
    "        # Selecciona una de las gpu dispobiles\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self.gpu)\n",
    "\n",
    "        if mixed_precision:\n",
    "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        \n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "    def __get_model__(self):\n",
    "        \n",
    "        def root_mean_squared_error(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "        itm_no = self.train_set.total_items\n",
    "        pad_len = self.max_text_size\n",
    "        vocab_size = self.vocab_size\n",
    "        \n",
    "        text_in = tf.keras.Input(shape=(pad_len,), dtype='int32', name=\"input_text\")\n",
    "        rest_in = tf.keras.Input(shape=(1,), dtype='int32', name=\"input_item\")\n",
    "        \n",
    "        emb_size = 128\n",
    "\n",
    "        # init = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        use_bias = True\n",
    "        \n",
    "        # word_importance = tf.keras.layers.Embedding(vocab_size, 1, name=\"word_importance\", embeddings_initializer=\"ones\", mask_zero=True)(text_in)\n",
    "\n",
    "        query_emb = tf.keras.layers.Embedding(vocab_size, emb_size*3, mask_zero=True)\n",
    "        mask_query = query_emb.compute_mask(text_in)\n",
    "        mask_query = tf.expand_dims(tf.cast(mask_query, dtype=tf.float32), -1)\n",
    "        mask_query = tf.tile(mask_query, [1, 1, 1])\n",
    "        ht_emb = query_emb(text_in)\n",
    "        # ht_emb = tf.keras.layers.Activation(\"tanh\")(ht_emb)\n",
    "        ht_emb = tf.keras.layers.Dense(emb_size*2, use_bias=use_bias)(ht_emb)\n",
    "        # ht_emb = tf.keras.layers.Activation(\"tanh\")(ht_emb)\n",
    "        ht_emb = tf.keras.layers.Dense(emb_size, use_bias=use_bias)(ht_emb)\n",
    "\n",
    "        # ht_emb = tf.keras.layers.Activation(\"tanh\")(ht_emb)\n",
    "        ht_emb = tf.keras.layers.Lambda(lambda x: x, name=\"word_emb\")(ht_emb)\n",
    "\n",
    "        rests_emb = tf.keras.layers.Embedding(itm_no, emb_size*3, name=f\"in_rsts\")\n",
    "        hr_emb = rests_emb(rest_in)\n",
    "        # hr_emb = tf.keras.layers.Activation(\"tanh\")(hr_emb)\n",
    "        hr_emb = tf.keras.layers.Dense(emb_size*2, use_bias=use_bias)(hr_emb)\n",
    "        # hr_emb = tf.keras.layers.Activation(\"tanh\")(hr_emb)\n",
    "        hr_emb = tf.keras.layers.Dense(emb_size, use_bias=use_bias)(hr_emb)\n",
    "        # hr_emb = tf.keras.layers.Activation(\"tanh\")(hr_emb)\n",
    "        hr_emb = tf.keras.layers.Lambda(lambda x: x, name=\"itm_emb\")(hr_emb)\n",
    "\n",
    "        model = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_b=True))([ht_emb, hr_emb])\n",
    "        # model = tf.keras.layers.BatchNormalization()(model)\n",
    "        # model = tf.keras.layers.GaussianNoise(.5)(model)\n",
    "        \n",
    "        model = tf.keras.layers.Lambda(lambda x: x[0]*x[1])([model, mask_query])\n",
    "        model = tf.keras.layers.Activation(\"tanh\", name=\"dotprod\")(model)\n",
    "        # model = tf.keras.layers.Lambda(lambda x: x[0]*x[1], name=\"importance\")([model, word_importance])\n",
    "        # model = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x[0], 2), name=\"dotprod\")([model])\n",
    "\n",
    "        model = tf.keras.layers.Dropout(.4)(model)\n",
    "\n",
    "        model = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, 1),  name=\"sum\")(model)\n",
    "        model_out = tf.keras.layers.Activation(\"linear\", name=\"out\", dtype='float32')(model)\n",
    "    \n",
    "        model = tf.keras.models.Model(inputs=[text_in, rest_in], outputs=[model_out], name=self.name)\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
    "        model.compile(loss=root_mean_squared_error, optimizer=optimizer)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def fit(self, train_set, val_set=None):\n",
    "\n",
    "        Recommender.fit(self, train_set, val_set)\n",
    "        # Config session\n",
    "        self.__config_session__()\n",
    "\n",
    "        # Create Model\n",
    "        self.model = self.__get_model__()\n",
    "        \n",
    "        # Text data\n",
    "        all_train_data = [(k, k1, v1) for k, v in self.train_text.user_review.items() for k1, v1 in v.items()]\n",
    "        all_train_data = pd.DataFrame(all_train_data, columns=['user', 'item', 'review']).sort_values([\"user\", \"item\"]).reset_index(drop=True)\n",
    "        all_train_data[\"rating\"] = pd.DataFrame(zip(train_set.uir_tuple[0],train_set.uir_tuple[1],train_set.uir_tuple[2]), columns=[\"user\", \"item\", \"rating\"]).sort_values([\"user\", \"item\"])[\"rating\"].values\n",
    "        all_train_seqs = self.train_text.batch_seq(all_train_data[\"review\"], max_length=self.max_text_size)\n",
    "        all_train_seqs = tf.keras.utils.pad_sequences(all_train_seqs, maxlen=self.max_text_size, padding=\"post\")\n",
    "        \n",
    "        data_x = tf.data.Dataset.from_tensor_slices(all_train_seqs)\n",
    "        data_x_rsts =  tf.data.Dataset.from_tensor_slices(all_train_data[\"item\"])\n",
    "        data_x = tf.data.Dataset.zip((data_x, data_x_rsts))\n",
    "        data_y = tf.data.Dataset.from_tensor_slices(all_train_data[\"rating\"])\n",
    "        train_tfset = tf.data.Dataset.zip((data_x, data_y))\n",
    "        train_tfset = train_tfset.batch(self.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "        val_tfset = None\n",
    "        if val_set is not None:\n",
    "            # TODO\n",
    "            val_tfset = None\n",
    "\n",
    "        # Training loop\n",
    "        # self.model.fit(train_tfset, epochs=self.n_epochs, validation_data=val_tfset)\n",
    "        loop = trange(self.max_iter)\n",
    "        for n_epoch in loop:\n",
    "            epoch_hist = self.model.fit(train_tfset, epochs=1, validation_data=val_tfset, verbose=1)\n",
    "            log_line = f', train_loss: {epoch_hist.history[\"loss\"][-1]:0.4f}'\n",
    "            if val_tfset is not None: log_line+= f', val_loss: {epoch_hist.history[\"val_loss\"][-1]:0.4f} '\n",
    "            loop.set_postfix_str(log_line,refresh=True)\n",
    "\n",
    "        loop.close()\n",
    "\n",
    "        # Obtener todos los resultados para el conjunto de test para acelerar\n",
    "        all_test_data = [(k1, v1) for k, v in self.test_text.user_review.items() for k1, v1 in v.items()]\n",
    "        all_test_data = pd.DataFrame(all_test_data, columns=['item', 'review']).sort_values([\"review\"]).reset_index(drop=True)\n",
    "        all_test_seqs = self.test_text.batch_seq(all_test_data[\"review\"], max_length=self.max_text_size)\n",
    "        all_test_seqs = tf.keras.utils.pad_sequences(all_test_seqs, maxlen=self.max_text_size, padding=\"post\")\n",
    "        data_test_x_seq = tf.data.Dataset.from_tensor_slices(all_test_seqs)\n",
    "        data_test_x_rsts =  tf.data.Dataset.from_tensor_slices(all_test_data[\"item\"])\n",
    "        data_test_x = tf.data.Dataset.from_tensor_slices({\"input_text\":all_test_seqs, \"input_item\":all_test_data[\"item\"]})\n",
    "        data_test_x = data_test_x.batch(self.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        self.test_predictions = self.model.predict(data_test_x, verbose=0)\n",
    "        # user_embs = self.model.get_layer(\"user_emb\").weights[0]\n",
    "        # item_embs = self.model.get_layer(\"item_emb\").weights[0]\n",
    "        # self.dot_prods = tf.tensordot(user_embs, tf.transpose(item_embs), axes=1).numpy()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self, user_id, item_id=None):\n",
    "        \"\"\"\n",
    "        if item_id is None:\n",
    "            return self.dot_prods[user_id]\n",
    "        else:\n",
    "            return [self.dot_prods[user_id, item_id]]\n",
    "        \"\"\"\n",
    "        if item_id is not None:\n",
    "            review_id = self.test_text.user_review[user_id][item_id]\n",
    "            return self.test_predictions[review_id]\n",
    "        else:\n",
    "            print(user_id)\n",
    "            return [0]\n",
    "\n",
    "\n",
    "# Register your model in Cornac's model dictionary\n",
    "models.ATTREX = ATTREX\n",
    "from cornac.models import ATTREX\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Define metrics to evaluate the models\n",
    "metrics_k = 100\n",
    "metrics = [MAE(), RMSE()]#, MAE(), RMSE(), AUC(), MAP(), FMeasure(k=metrics_k), Precision(k=metrics_k), Recall(k=metrics_k), NDCG(k=metrics_k)]\n",
    "# Instantiate models\n",
    "m_attrex = cornac.models.ATTREX(max_iter=1, vocab_size=max_vocab, learning_rate=1e-4, batch_size=512, seed=10, train_text=train_text, val_text=val_text, test_text=test_text)\n",
    "m_cdl = cornac.models.CDL(max_iter=5, vocab_size=max_vocab)\n",
    "\n",
    "cornac.Experiment(eval_method=ratio_split, \n",
    "                    models=[\n",
    "                        cornac.models.ATTREX(max_iter=10, vocab_size=max_vocab, learning_rate=1e-4, batch_size=512, seed=10, train_text=train_text, val_text=val_text, test_text=test_text),\n",
    "                        cornac.models.MF(k=10, max_iter=25, learning_rate=0.01, lambda_reg=0.02, use_bias=True, seed=123),\n",
    "                        #cornac.models.PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001, seed=123),\n",
    "                        #cornac.models.BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),\n",
    "                    ],\n",
    "                    metrics=metrics,\n",
    "                    user_based=False, show_validation=False).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RandomSearch_ATTREX] Training started!\n"
     ]
    }
   ],
   "source": [
    "# RandomSearch\n",
    "rs_attrex = RandomSearch(\n",
    "    model= cornac.models.ATTREX(max_iter=1, vocab_size=max_vocab, learning_rate=1e-4, batch_size=512, seed=10, train_text=train_text, val_text=val_text, test_text=test_text),\n",
    "    space=[\n",
    "        Continuous(\"learning_rate\", low=1e-5, high=1e-3),\n",
    "        # Discrete(\"max_iter\", np.linspace(5,100,20, dtype=int)),\n",
    "        Discrete(\"batch_size\", [32,64,128,256,512])\n",
    "    ],\n",
    "    metric=RMSE(),\n",
    "    eval_method=ratio_split,\n",
    "    n_trails=20,\n",
    ")\n",
    "\n",
    "cornac.Experiment( eval_method=ratio_split, models=[rs_attrex], metrics=[MAE(), RMSE()], user_based=False, show_validation=False).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Grid search: max_iter = {:.2f}'.format(gs_hft.best_params.get('max_iter')))\n",
    "print('Grid search: lambda_reg = {:.2f}'.format(gs_hft.best_params.get('batch_size')))\n",
    "print('Grid search: learning_rate = {:.2f}'.format(gs_cdl.best_params.get('learning_rate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models, here we are comparing: Biased MF, PMF, and BPR\n",
    "models = [\n",
    "    ATTEX(),\n",
    "    MF(k=10, max_iter=25, learning_rate=0.01, lambda_reg=0.02, use_bias=True, seed=123),\n",
    "    # PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001, seed=123),\n",
    "    # NeuMF(seed=123),\n",
    "    # BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),\n",
    "    # HFT(k=10, max_iter=200, seed=123),\n",
    "    # BiVAECF(k=10, n_epochs=100, learning_rate=0.001, seed=123)\n",
    "]\n",
    "\n",
    "# put it together in an experiment, voilÃ !\n",
    "cornac.Experiment(eval_method=ratio_split, models=models, metrics=metrics, user_based=True).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAV_text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
