{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 21:04:03.502194: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 21:04:03.502218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 21:04:03.503243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 21:04:03.508364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 21:04:04.444970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-08 21:04:06.779140: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-08 21:04:06.780225: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-08 21:04:06.780408: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "/media/nas/pperez/conda/ns3/envs/TAVtext/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nas/pperez/conda/ns3/envs/TAVtext/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import os\n",
    "\n",
    "gpu = np.argmin([g[\"mem_used_percent\"] for g in nvgpu.gpu_info()]) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "from src.experiments.Common import load_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener los resultados en test del modelo (**para todo tipo de usuarios**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_results(datasets, models, users=0):\n",
    "    '''users=0 todos, 1 solo conocidos, 2 desconocidos durante el entrenamiento'''\n",
    "    results = []\n",
    "\n",
    "    for dataset, subsets in datasets.items():\n",
    "        for subset in subsets:\n",
    "            for model in models:\n",
    "                # Cargar configuración mejor modelo\n",
    "                model_class = load_best_model(model=model, dataset=dataset, subset=subset, gpu=gpu)\n",
    "                # Cargar el modelo entrenado\n",
    "                model_class.train(dev=False, save_model=True)\n",
    "                \n",
    "                if users==1:\n",
    "                    # Se buscan los usuarios de train y se dejan solo esos en test\n",
    "                    train_dev_users = model_class.DATASET.DATA[\"TRAIN_DEV\"].userId.unique()\n",
    "                    model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"][model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)]\n",
    "                    model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"].drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False)\n",
    "\n",
    "                elif users==2:\n",
    "                    # Se buscan los usuarios de train+dev y se eliminan de test\n",
    "                    train_dev_users = model_class.DATASET.DATA[\"TRAIN_DEV\"].userId.unique()\n",
    "                    model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"][~model_class.DATASET.DATA[\"TEST\"][\"userId\"].isin(train_dev_users)]\n",
    "                    model_class.DATASET.DATA[\"TEST\"] = model_class.DATASET.DATA[\"TEST\"].drop_duplicates(subset=[\"userId\", \"id_item\"], keep='last', inplace=False)\n",
    "\n",
    "                # Evaluar el modelo final\n",
    "                result = model_class.evaluate(test=True, user_info=True)\n",
    "                result[\"Model\"] = model\n",
    "                result[\"Set\"] = dataset\n",
    "                result[\"Subset\"] = subset\n",
    "                results.append(result.values[0])\n",
    "                \n",
    "    results = pd.DataFrame(results, columns=result.columns)\n",
    "    # Poner nombres decentes para el artículo\n",
    "    results[\"Set\"].replace({\"restaurants\": \"TAV-RSTS\", \"pois\":\"TAV-POIS\", \"amazon\":\"AM\"}, inplace=True)\n",
    "    results[\"Subset\"].replace({\"digital_music\": \"Music\", \"fashion\":\"Fashion\", \"gijon\": \"Gijón\", \"barcelona\":\"Barcelona\", \"madrid\": \"Madrid\", \"paris\": \"Paris\", \"newyorkcity\": \"New York\", \"london\": \"London\"}, inplace=True)\n",
    "    results[\"Model\"].replace({\"ATT2ITM\": \"AITRecX\", \"ATT2ITM_2\": \"AITRecX (NT)\",}, inplace=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "datasets = {\"restaurants\":[ \"newyorkcity\"]}\n",
    "models = [\"ATT2ITM\", \"ATT2ITM_2\"]\n",
    "\n",
    "results_all = obtain_results(datasets=datasets, models=models, users=0)\n",
    "results_known = obtain_results(datasets=datasets, models=models, users=1)\n",
    "results_unknown = obtain_results(datasets=datasets, models=models, users=2)\n",
    "\n",
    "results = []\n",
    "results_all[\"users\"]=\"all\"\n",
    "results.extend(results_all.values)\n",
    "results_known[\"users\"]=\"known\"\n",
    "results.extend(results_known.values)\n",
    "results_unknown[\"users\"]=\"unknown\"\n",
    "results.extend(results_unknown.values)\n",
    "results = pd.DataFrame(results, columns=results_all.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear tabla Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_table(results, metrics=[\"NDCG@10\"], models = ['AITRecX', 'AITRecX (NT)']):\n",
    "    result_df = []\n",
    "\n",
    "    for metric in metrics:\n",
    "        mres = results.pivot_table(index=[ \"Set\", \"Subset\"], columns=[\"users\", \"Model\"])[metric].reset_index()\n",
    "        mres[\"Metric\"] = metric\n",
    "        result_df.extend(mres.values)\n",
    "\n",
    "    result_df = pd.DataFrame(result_df, columns=mres.columns)\n",
    "\n",
    "    # Especificar el orden de los datos\n",
    "    result_df['Set'] = pd.Categorical(result_df['Set'], [\"AM\", \"TAV-POIS\", \"TAV-RSTS\"])\n",
    "    result_df['Subset'] = pd.Categorical(result_df['Subset'], [\"Music\", \"Fashion\", \"Gijón\", \"Barcelona\", \"Madrid\", \"New York\", \"Paris\", \"London\"])\n",
    "    result_df['Metric'] = pd.Categorical(result_df['Metric'], metrics)\n",
    "\n",
    "    result_df = result_df.pivot_table(index=[\"Set\", \"Subset\", \"Metric\"])\n",
    "    print(result_df.to_latex(float_format=\"%.3f\"))\n",
    "    # result_df[models].to_excel(\"trecx_known.xlsx\")\n",
    "    return result_df\n",
    "\n",
    "metrics = [\"NDCG@10\"]\n",
    "models = ['AITRecX', 'AITRecX (NT)']\n",
    "\n",
    "table = latex_table(results, metrics=metrics, models=models)\n",
    "table.to_excel(\"output/ablation_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar gráficas y valores de \"attention\" para ambos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mLoading best ATT2ITM model: afe7526023b1827d88ecad6479de8971\u001b[0m\n",
      "\u001b[93m[WARNING] Model folder already exists...\u001b[0m\n",
      "\u001b[93m[WARNING] Model weights are not loaded!\u001b[0m\n",
      "\u001b[92m[INFO] Best epoch number: 244\u001b[0m\n",
      "\u001b[92m[INFO] Model already trained. Loading weights...\u001b[0m\n",
      "\u001b[92m[QUERY] 'no'\u001b[0m\n",
      "[PREPR]         [no]\n",
      "[TXT2ID]        [11]\n",
      "[WORD FREQ]     [88862]\n",
      "El rango de valores de la matriz de atención para todas las palabras se mueve en el rango [-1.0,0.9999994039535522].\n",
      "\u001b[91m[ERROR] Reparar y unificar la parte de selección de palabras relevantes\u001b[0m\n",
      "\t[0.53] Parque de El Capricho                        {'osuna': 0.99927163, 'bunker': 0.9990261, 'capricho': 0.9983824, 'duquesa': 0.9964534, 'retirado': 0.9932595, 'comunion': 0.992307, 'casita': 0.9911413, 'cisn': 0.9891181, 'duque': 0.9873863, 'canal': 0.98469764}\n",
      "\t[0.52] Catedral de Sta María la Real de la Almudena {'catedral': 0.9971976, 'neogotico': 0.99526435, 'religioso': 0.99112713, 'cripta': 0.990997, 'misa': 0.98993, 'vidriera': 0.98922557, 'altar': 0.9882856, 'virgen': 0.9812937, 'catolico': 0.966136, 'techo': 0.9652268}\n",
      "\t[0.52] Faunia                                       {'pinguino': 0.9992354, 'faunia': 0.9991275, 'mariposa': 0.9953337, 'olivo': 0.99488556, 'ecosistema': 0.99199045, 'selva': 0.98955804, 'habitats': 0.9862815, 'jungla': 0.98550475, 'marino': 0.9799365, 'habitat': 0.9708955}\n",
      "\t[0.52] Museo del Prado                              {'ingr': 0.9997906, 'hilandera': 0.9986727, 'velasquez': 0.99855596, 'bosco': 0.9982289, 'velazquez': 0.9980678, 'menina': 0.99800783, 'gioconda': 0.9972516, 'fortuny': 0.99696493, 'pinacoteca': 0.9968705, 'pincel': 0.99681497}\n",
      "\u001b[94mLoading best ATT2ITM_2 model: ad8ea3dcf66a1113cfc74f8e48320aa3\u001b[0m\n",
      "\u001b[93m[WARNING] Model folder already exists...\u001b[0m\n",
      "\u001b[93m[WARNING] Model weights are not loaded!\u001b[0m\n",
      "\u001b[92m[INFO] Best epoch number: 51\u001b[0m\n",
      "\u001b[92m[INFO] Model already trained. Loading weights...\u001b[0m\n",
      "\u001b[92m[QUERY] 'no'\u001b[0m\n",
      "[PREPR]         [no]\n",
      "[TXT2ID]        [11]\n",
      "[WORD FREQ]     [88862]\n",
      "El rango de valores de la matriz de atención para todas las palabras se mueve en el rango [-11.868982315063477,5.12892484664917].\n",
      "\u001b[91m[ERROR] Reparar y unificar la parte de selección de palabras relevantes\u001b[0m\n",
      "\t[0.54] Museo del Prado                                           {'ingr': 3.6050158, 'gioconda': 2.8520575, 'velasquez': 2.6607327, 'hermitage': 2.557814, 'fortuny': 2.1879425, 'anunciacion': 2.1460257, 'hilandera': 1.9313931, 'menina': 1.7408292, 'jeronir': 1.7390983, 'louvre': 1.7196553}\n",
      "\t[0.53] Parque de El Capricho                                     {'osuna': 4.147054, 'bunker': 3.6929004, 'capricho': 3.0321848, 'duquesa': 2.9986427, 'duque': 2.65402, 'comunion': 2.2449417, 'casita': 2.0222268, 'reportaje': 2.0131435, 'balon': 1.9665502, 'pelota': 1.8034806}\n",
      "\t[0.52] Museo de la Real Academia de Bellas Artes de San Fernando {'zurbarar': 1.8435405, 'academia': 1.7749571, 'zurbaran': 1.6750859, 'casino': 1.4825267, 'alcalar': 1.4243917, 'fernando': 1.3926126, 'madrazo': 1.364155, 'goya': 1.2498721, 'grabado': 1.2173014, 'talla': 1.168811}\n",
      "\t[0.52] Museo Cerralbo                                            {'cerralbo': 2.4029644, 'marques': 2.1658034, 'debot': 2.160595, 'rodriguez': 2.0950086, 'propietario': 1.9704542, 'baile': 1.9595968, 'marqu': 1.8338655, 'alonso': 1.8145788, 'aristocracia': 1.5575829, 'trasladado': 1.4860599}\n",
      "\u001b[94mLoading best ATT2ITM model: 734cbcb93f6f6f5d7b5c5a5247f561f9\u001b[0m\n",
      "\u001b[93m[WARNING] Model folder already exists...\u001b[0m\n",
      "\u001b[93m[WARNING] Model weights are not loaded!\u001b[0m\n",
      "\u001b[92m[INFO] Best epoch number: 79\u001b[0m\n",
      "\u001b[92m[INFO] Model already trained. Loading weights...\u001b[0m\n",
      "\u001b[92m[QUERY] 'no'\u001b[0m\n",
      "[PREPR]         [no]\n",
      "[TXT2ID]        [9]\n",
      "[WORD FREQ]     [53554]\n",
      "El rango de valores de la matriz de atención para todas las palabras se mueve en el rango [-1.0,0.9997376799583435].\n",
      "\u001b[91m[ERROR] Reparar y unificar la parte de selección de palabras relevantes\u001b[0m\n",
      "\t[0.52] Museo de la Orangerie {'monet': 0.99834913, 'nenufar': 0.9982832, 'claude': 0.9786842, 'modigliani': 0.9462953, 'mural': 0.94113714, 'ninfa': 0.93957835, 'orsay': 0.9193844, 'orangerie': 0.916293, 'panel': 0.9152403, 'reposar': 0.91113895}\n",
      "\t[0.52] Santa Capilla         {'capilla': 0.9993754, 'vidriera': 0.99889296, 'vitral': 0.9982101, 'vitreaux': 0.99402297, 'vitraux': 0.99262756, 'justicia': 0.9891098, 'cristalera': 0.9865508, 'escondido': 0.98650277, 'biblia': 0.9828503, 'vitro': 0.9815082}\n",
      "\t[0.52] Museo del Louvre      {'gioconda': 0.9997377, 'monalisa': 0.9992875, 'lisar': 0.99926406, 'mona': 0.99921346, 'piramid': 0.9976062, 'piramidar': 0.99740404, 'piramidir': 0.99703884, 'egipcia': 0.996569, 'samotracia': 0.9963985, 'piramide': 0.9960797}\n",
      "\t[0.52] The Paris Catacombs   {'hueso': 0.99737006, 'catacumba': 0.9956867, 'pasillo': 0.9626597, 'experimentar': 0.95571744, 'tunel': 0.950287, 'subterraneo': 0.9457705, 'serie': 0.9456656, 'muerte': 0.93592334, 'profundidad': 0.9279236, 'humilde': 0.90839213}\n",
      "\u001b[94mLoading best ATT2ITM_2 model: 4aaa35673f6b1b05305a2f85c831bd4e\u001b[0m\n",
      "\u001b[93m[WARNING] Model folder already exists...\u001b[0m\n",
      "\u001b[93m[WARNING] Model weights are not loaded!\u001b[0m\n",
      "\u001b[92m[INFO] Best epoch number: 28\u001b[0m\n",
      "\u001b[92m[INFO] Model already trained. Loading weights...\u001b[0m\n",
      "\u001b[92m[QUERY] 'no'\u001b[0m\n",
      "[PREPR]         [no]\n",
      "[TXT2ID]        [9]\n",
      "[WORD FREQ]     [53554]\n",
      "El rango de valores de la matriz de atención para todas las palabras se mueve en el rango [-7.374381065368652,4.052820205688477].\n",
      "\u001b[91m[ERROR] Reparar y unificar la parte de selección de palabras relevantes\u001b[0m\n",
      "\t[0.54] Torre Eiffel           {'ingenieria': 2.1130147, 'pata': 1.9705944, 'acero': 1.9374175, 'hierro': 1.7914371, 'reservamos': 1.5550873, 'metalico': 1.4760873, 'gustave': 1.3651905, 'tercer': 1.323595, 'vea': 1.2984087, 'top': 1.2139891}\n",
      "\t[0.52] Santa Capilla          {'vidriera': 2.046611, 'capilla': 2.010457, 'vitreaux': 1.8750339, 'vitraux': 1.7602346, 'cristalera': 1.6995527, 'concierto': 1.6748971, 'chiquito': 1.6526333, 'pequeñito': 1.6107688, 'vitral': 1.586863, 'escondido': 1.5683386}\n",
      "\t[0.52] Catedral de Notre Dame {'guadalupe': 2.1456013, 'incendio': 2.0056746, 'catedral': 1.3220031, 'reconstruccion': 1.2709892, 'gratuitamente': 1.2649361, 'canto': 1.2426562, 'aplicacion': 1.227635, 'jorobado': 1.2106578, 'frontal': 1.1606656, 'roseton': 1.1442225}\n",
      "\t[0.52] Museo del Louvre       {'piramide': 3.3121886, 'monalisa': 3.167441, 'piramidir': 3.151668, 'piramidar': 2.9151871, 'piramid': 2.911674, 'gioconda': 2.8865194, 'egipcia': 2.4530594, 'samotracia': 2.3438606, 'vinci': 2.1641939, 'ds': 2.0537267}\n",
      "\u001b[94mLoading best ATT2ITM model: 37ed8371d07fb3ebc09c24d421039f8a\u001b[0m\n",
      "\u001b[93m[WARNING] Model folder already exists...\u001b[0m\n",
      "\u001b[93m[WARNING] Model weights are not loaded!\u001b[0m\n",
      "\u001b[92m[INFO] Best epoch number: 778\u001b[0m\n",
      "\u001b[92m[INFO] Model already trained. Loading weights...\u001b[0m\n",
      "\u001b[92m[QUERY] 'no'\u001b[0m\n",
      "[PREPR]         [no]\n",
      "[TXT2ID]        [11]\n",
      "[WORD FREQ]     [68647]\n",
      "El rango de valores de la matriz de atención para todas las palabras se mueve en el rango [-1.0,0.9999996423721313].\n",
      "\u001b[91m[ERROR] Reparar y unificar la parte de selección de palabras relevantes\u001b[0m\n",
      "\t[0.55] Chrysler Building            {'cupula': 0.9933641, 'lobby': 0.97562826, 'hall': 0.95826596, 'deco': 0.955578, 'chrysler': 0.9524493, 'long': 0.9505994, 'vestibulo': 0.9446032, 'shake': 0.92054725, 'elegancia': 0.9195855, 'lexington': 0.9103106}\n",
      "\t[0.54] Solomon R. Guggenheim Museum {'espiral': 0.99884653, 'wright': 0.99698067, 'kandinsky': 0.98926073, 'guggenheim': 0.9680583, 'caracol': 0.9558063, 'rampa': 0.9418539, 'frank': 0.86493987, 'muestra': 0.8604693, 'obrero': 0.8602954, 'exposicion': 0.8594639}\n",
      "\t[0.54] Chinatown                    {'chino': 0.99546796, 'imitacion': 0.98919046, 'chinatown': 0.98866725, 'china': 0.98123664, 'regatear': 0.98021424, 'pato': 0.9790745, 'asiatico': 0.96471614, 'sucio': 0.9645008, 'riquisimo': 0.95540655, 'oriental': 0.9521274}\n",
      "\t[0.52] Statue of Liberty            {'corona': 0.9997823, 'pedestal': 0.99871755, 'bote': 0.9829983, 'simbolo': 0.9820225, 'lancha': 0.97880787, 'ferri': 0.9780542, 'barco': 0.9778321, 'barquito': 0.96839213, 'eiffel': 0.965263, 'caracol': 0.9636622}\n",
      "\u001b[94mLoading best ATT2ITM_2 model: a8a1966f69d3449e5abd7baf83304295\u001b[0m\n",
      "\u001b[93m[WARNING] Model folder already exists...\u001b[0m\n",
      "\u001b[93m[WARNING] Model weights are not loaded!\u001b[0m\n",
      "\u001b[92m[INFO] Best epoch number: 57\u001b[0m\n",
      "\u001b[92m[INFO] Model already trained. Loading weights...\u001b[0m\n",
      "\u001b[92m[QUERY] 'no'\u001b[0m\n",
      "[PREPR]         [no]\n",
      "[TXT2ID]        [11]\n",
      "[WORD FREQ]     [68647]\n",
      "El rango de valores de la matriz de atención para todas las palabras se mueve en el rango [-14.34631061553955,5.706133842468262].\n",
      "\u001b[91m[ERROR] Reparar y unificar la parte de selección de palabras relevantes\u001b[0m\n",
      "\t[0.55] Chinatown                    {'pato': 2.437109, 'china': 2.1750357, 'regatear': 2.1027043, 'imitacion': 1.9456278, 'oriente': 1.8730319, 'asiatico': 1.8370187, 'oriental': 1.8075203, 'chino': 1.7380891, 'aroma': 1.6575415, 'chinatown': 1.5834844}\n",
      "\t[0.55] Chrysler Building            {'cupula': 2.2414951, 'lobby': 1.6677274, 'hall': 1.5487771, 'chrysler': 1.4827975, 'distinguir': 1.4376099, 'lexington': 1.305322, 'long': 1.2717854, 'shake': 1.2645761, 'vestibulo': 1.2382102, 'elegante': 1.1745989}\n",
      "\t[0.53] Solomon R. Guggenheim Museum {'espiral': 2.9734533, 'wright': 2.648756, 'rampa': 2.4360178, 'kandinsky': 2.4206367, 'caracol': 1.9961512, 'guggenheim': 1.9612069, 'sabado': 1.2697574, 'positivo': 1.1881751, 'pesimo': 1.1404349, 'esti': 1.1309402}\n",
      "\t[0.53] Memorial del 11S             {'morbo': 2.0066576, 'martes': 1.936325, 'retorcido': 1.5696194, 'century': 1.5241098, 'sensible': 1.5022799, 'acontecimiento': 1.499646, 'nostalgia': 1.4899936, 'solemne': 1.4709563, 'testimonio': 1.464165, 'emotivo': 1.3830049}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_samples(datasets, models):\n",
    "\n",
    "    for dataset, subsets in datasets.items():\n",
    "        for subset in subsets:\n",
    "            for model in models:\n",
    "                # Cargar configuración mejor modelo\n",
    "                model_class = load_best_model(model=model, dataset=dataset, subset=subset, gpu=gpu)\n",
    "                # Cargar el modelo entrenado\n",
    "                model_class.train(dev=False, save_model=True)\n",
    "                # Evaluar un texto\n",
    "                model_class.evaluate_text(\"no\")\n",
    "\n",
    "datasets = {\"pois\":[\"madrid\", \"paris\", \"newyorkcity\"]}\n",
    "models = [\"ATT2ITM\", \"ATT2ITM_2\"]\n",
    "\n",
    "evaluate_samples(datasets=datasets, models=models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
