{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener de cada modelo el tiempo de ejecución y los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas\n",
    "from src.experiments.Common import load_best_model\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"models/Baselines\"\n",
    "\n",
    "datasets = {\"restaurants\":[\"gijon\", \"barcelona\", \"madrid\", \"paris\", \"newyorkcity\"],\n",
    "            \"pois\":[\"barcelona\", \"madrid\", \"paris\", \"newyorkcity\", \"london\"],\n",
    "            \"amazon\":[\"fashion\", \"digital_music\"]}\n",
    "\n",
    "models_cold = [\"MOSTPOP2ITM\", \"BOW2ITM\", \"USEM2ITM\", \"BERT2ITM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_baseline_file(filepath):\n",
    "    \n",
    "    def process_section(lines):\n",
    "        # Obtener las columnas\n",
    "        columns = lines[1].split(\"|\")\n",
    "        columns = [col.strip() for col in columns]\n",
    "        \n",
    "        # Obtener los datos\n",
    "        data = []\n",
    "        for line in lines[3:]:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            row = line.split(\"|\")\n",
    "            row = [item.strip() for item in row]\n",
    "            data.append(row)\n",
    "        \n",
    "        # Crear el DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        return df\n",
    "\n",
    "    # Leer el archivo\n",
    "    with open(filepath, 'r') as file: lines = file.readlines()\n",
    "\n",
    "    # Separar las secciones\n",
    "    validation_start = lines.index('VALIDATION:\\n')\n",
    "    test_start = lines.index('TEST:\\n')\n",
    "\n",
    "    validation_lines = lines[validation_start + 1:test_start]\n",
    "    test_lines = lines[test_start + 1:]\n",
    "\n",
    "    # Procesar cada sección\n",
    "    validation_df = process_section(validation_lines)\n",
    "    test_df = process_section(test_lines)\n",
    "    \n",
    "    test_df.rename(columns={\"\":\"Model\", \"Train (s)\":\"Train_time\"}, inplace=True)\n",
    "    \n",
    "    model_paper_names = {\"GridSearch_EASEᴿ\":\"EASEᴿ\", \"GridSearch_BPR\":\"BPR\", \"online_ibpr\": \"IBPR\"}\n",
    "    test_df[\"Model\"] = test_df[\"Model\"].apply(lambda x: model_paper_names[x] if x in model_paper_names.keys() else x)\n",
    "\n",
    "    return test_df[[\"Model\",\"Train_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = []\n",
    "param_data = []\n",
    "\n",
    "for dataset, subsets in datasets.items():\n",
    "    for subset in subsets:\n",
    "        # Definir el nombre del fichero\n",
    "        path = f\"/media/nas/pperez/code/TAVtext/{base_path}/{dataset}/{subset}/\"\n",
    "        # Obtener el fichero con los tiempos\n",
    "        paths = sorted(Path(path).iterdir(), key=os.path.getmtime)\n",
    "        file = [str(f) for f in paths if \"CornacExp\" in f.name][-1]\n",
    "        # Cargar el fichero y leer los tiempos\n",
    "        model_times = process_baseline_file(file)\n",
    "        # Para los modelos no baseline, hacer lo mismo\n",
    "        for model in models_cold:\n",
    "            # Cargar mejor modelo\n",
    "            model_class = load_best_model(model=model, dataset=dataset, subset=subset)\n",
    "            # Obtener el número de parámetros\n",
    "            param_data.append({\"Set\":dataset, \"Subset\":subset, \"Model\": model, \"Params\":model_class.MODEL.count_params()})\n",
    "            # Obtener el tiempo de train\n",
    "            time = pd.read_csv(model_class.MODEL_PATH+\"log.csv\")[\"e_time\"].sum()\n",
    "            model_times = model_times.append({\"Model\":model, \"Train_time\":time}, ignore_index=True)\n",
    "        # Crear una linea para el dataframe final de resultados\n",
    "        time_data_columns = [\"Set\", \"Subset\"]+model_times[\"Model\"].values.tolist()\n",
    "        time_data_values = [dataset, subset]+model_times.set_index(\"Model\").transpose().values[0].tolist()\n",
    "        time_data.append(dict(zip(time_data_columns, time_data_values)))\n",
    "\n",
    "time_results = pd.DataFrame(time_data)\n",
    "param_results = pd.DataFrame(param_data).pivot_table(index=[\"Set\", \"Subset\"], columns=[\"Model\"], values=[\"Params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_results\n",
    "time_results.to_excel(\"time_results.xlsx\")\n",
    "param_results.to_excel(\"param_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
