2023-06-07 23:30:17.111537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "WATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, None)]       0           []                               
                                                                                                  
 wr_att_embs (Embedding)        (None, None, 2)      21884       ['input_1[0][0]']                
                                                                                                  
 dropout (Dropout)              (None, None, 2)      0           ['wr_att_embs[0][0]']            
                                                                                                  
 tf.linalg.matmul (TFOpLambda)  (None, None, None)   0           ['dropout[0][0]',                
                                                                  'dropout[0][0]']                
                                                                                                  
 activation (Activation)        (None, None, None)   0           ['tf.linalg.matmul[0][0]']       
                                                                                                  
 all_words (Embedding)          (None, None, 128)    1400576     ['input_1[0][0]']                
                                                                                                  
 input_2 (InputLayer)           [(None, 1634)]       0           []                               
                                                                                                  
 tf.einsum (TFOpLambda)         (None, None, 128)    0           ['activation[0][0]',             
                                                                  'all_words[0][0]']              
                                                                                                  
 all_items (Embedding)          (None, 1634, 128)    209152      ['input_2[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, None, 128)    0           ['tf.einsum[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1634, 128)    0           ['all_items[0][0]']              
                                                                                                  
 tf.math.not_equal_1 (TFOpLambd  (None, None)        0           ['input_1[0][0]']                
 a)                                                                                               
                                                                                                  
 dropout_1 (Dropout)            (None, None, 128)    0           ['word_emb[0][0]']               
                                                                                                  
 dropout_2 (Dropout)            (None, 1634, 128)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.cast_1 (TFOpLambda)         (None, None)         0           ['tf.math.not_equal_1[0][0]']    
                                                                                                  
 dot_mul (Lambda)               (None, None, 1634)   0           ['dropout_1[0][0]',              
                                                                  'dropout_2[0][0]']              
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, None, 1)      0           ['tf.cast_1[0][0]']              
                                                                                                  
 dotprod (Activation)           (None, None, 1634)   0           ['dot_mul[0][0]']                
                                                                                                  
 tf.tile_1 (TFOpLambda)         (None, None, 1634)   0           ['tf.expand_dims_1[0][0]']       
                                                                                                  
 dot_mask (Lambda)              (None, None, 1634)   0           ['dotprod[0][0]',                
                                                                  'tf.tile_1[0][0]']              
                                                                                                  
 sum (Lambda)                   (None, 1634)         0           ['dot_mask[0][0]',               
                                                                  'tf.tile_1[0][0]']              
                                                                                                  
 activation_1 (Activation)      (None, 1634)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1634)         0           ['activation_1[0][0]']           
                                                                                                  
==================================================================================================
Total params: 1,631,612
Trainable params: 1,631,612
Non-trainable params: 0
__________________________________________________________________________________________________
None
Epoch 1/1000
2977/2977 - 28s - loss: nan - NDCG@10: 0.0267 - MAE: nan - RC@5: 0.0334 - RC@10: 0.0521 - val_loss: 0.8799 - val_NDCG@10: 0.0328 - val_MAE: 0.1093 - val_RC@5: 0.0411 - val_RC@10: 0.0653 - lr: 9.9901e-05 - e_time: 27.4284 - 28s/epoch - 9ms/step
Epoch 2/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.0414 - MAE: nan - RC@5: 0.0524 - RC@10: 0.0769 - val_loss: 0.8620 - val_NDCG@10: 0.0574 - val_MAE: 0.1142 - val_RC@5: 0.0725 - val_RC@10: 0.0986 - lr: 9.9802e-05 - e_time: 25.4927 - 26s/epoch - 9ms/step
Epoch 3/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.0700 - MAE: nan - RC@5: 0.0849 - RC@10: 0.1173 - val_loss: 0.8325 - val_NDCG@10: 0.1046 - val_MAE: 0.1115 - val_RC@5: 0.1253 - val_RC@10: 0.1634 - lr: 9.9703e-05 - e_time: 25.6061 - 26s/epoch - 9ms/step
Epoch 4/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.1276 - MAE: nan - RC@5: 0.1530 - RC@10: 0.2019 - val_loss: 0.7772 - val_NDCG@10: 0.1859 - val_MAE: 0.1018 - val_RC@5: 0.2248 - val_RC@10: 0.2895 - lr: 9.9604e-05 - e_time: 25.6432 - 26s/epoch - 9ms/step
Epoch 5/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.1937 - MAE: nan - RC@5: 0.2326 - RC@10: 0.3025 - val_loss: 0.7230 - val_NDCG@10: 0.2580 - val_MAE: 0.0927 - val_RC@5: 0.3074 - val_RC@10: 0.3905 - lr: 9.9505e-05 - e_time: 25.8417 - 26s/epoch - 9ms/step
Epoch 6/1000
2977/2977 - 38s - loss: nan - NDCG@10: 0.2436 - MAE: nan - RC@5: 0.2925 - RC@10: 0.3756 - val_loss: 0.6858 - val_NDCG@10: 0.3061 - val_MAE: 0.0866 - val_RC@5: 0.3642 - val_RC@10: 0.4559 - lr: 9.9406e-05 - e_time: 37.2700 - 38s/epoch - 13ms/step
Epoch 7/1000
2977/2977 - 59s - loss: nan - NDCG@10: 0.2810 - MAE: nan - RC@5: 0.3366 - RC@10: 0.4272 - val_loss: 0.6589 - val_NDCG@10: 0.3421 - val_MAE: 0.0792 - val_RC@5: 0.4066 - val_RC@10: 0.5020 - lr: 9.9307e-05 - e_time: 58.0632 - 59s/epoch - 20ms/step
Epoch 8/1000
2977/2977 - 55s - loss: nan - NDCG@10: 0.3106 - MAE: nan - RC@5: 0.3722 - RC@10: 0.4668 - val_loss: 0.6379 - val_NDCG@10: 0.3705 - val_MAE: 0.0778 - val_RC@5: 0.4406 - val_RC@10: 0.5377 - lr: 9.9208e-05 - e_time: 54.3262 - 55s/epoch - 18ms/step
Epoch 9/1000
2977/2977 - 54s - loss: nan - NDCG@10: 0.3351 - MAE: nan - RC@5: 0.4008 - RC@10: 0.4989 - val_loss: 0.6223 - val_NDCG@10: 0.3954 - val_MAE: 0.0726 - val_RC@5: 0.4679 - val_RC@10: 0.5654 - lr: 9.9109e-05 - e_time: 53.9431 - 54s/epoch - 18ms/step
Epoch 10/1000
2977/2977 - 53s - loss: nan - NDCG@10: 0.3549 - MAE: nan - RC@5: 0.4248 - RC@10: 0.5228 - val_loss: 0.6101 - val_NDCG@10: 0.4128 - val_MAE: 0.0709 - val_RC@5: 0.4880 - val_RC@10: 0.5838 - lr: 9.9010e-05 - e_time: 52.8809 - 53s/epoch - 18ms/step
Epoch 11/1000
2977/2977 - 56s - loss: nan - NDCG@10: 0.3719 - MAE: nan - RC@5: 0.4452 - RC@10: 0.5440 - val_loss: 0.6008 - val_NDCG@10: 0.4280 - val_MAE: 0.0676 - val_RC@5: 0.5042 - val_RC@10: 0.6003 - lr: 9.8911e-05 - e_time: 55.4531 - 56s/epoch - 19ms/step
Epoch 12/1000
2977/2977 - 55s - loss: nan - NDCG@10: 0.3871 - MAE: nan - RC@5: 0.4624 - RC@10: 0.5606 - val_loss: 0.5931 - val_NDCG@10: 0.4405 - val_MAE: 0.0661 - val_RC@5: 0.5185 - val_RC@10: 0.6118 - lr: 9.8812e-05 - e_time: 54.2560 - 55s/epoch - 18ms/step
Epoch 13/1000
2977/2977 - 53s - loss: nan - NDCG@10: 0.3994 - MAE: nan - RC@5: 0.4767 - RC@10: 0.5747 - val_loss: 0.5872 - val_NDCG@10: 0.4508 - val_MAE: 0.0650 - val_RC@5: 0.5308 - val_RC@10: 0.6224 - lr: 9.8713e-05 - e_time: 52.3792 - 53s/epoch - 18ms/step
Epoch 14/1000
2977/2977 - 53s - loss: nan - NDCG@10: 0.4112 - MAE: nan - RC@5: 0.4898 - RC@10: 0.5878 - val_loss: 0.5824 - val_NDCG@10: 0.4603 - val_MAE: 0.0622 - val_RC@5: 0.5411 - val_RC@10: 0.6306 - lr: 9.8614e-05 - e_time: 52.5924 - 53s/epoch - 18ms/step
Epoch 15/1000
2977/2977 - 50s - loss: nan - NDCG@10: 0.4210 - MAE: nan - RC@5: 0.5011 - RC@10: 0.5977 - val_loss: 0.5786 - val_NDCG@10: 0.4662 - val_MAE: 0.0609 - val_RC@5: 0.5479 - val_RC@10: 0.6357 - lr: 9.8515e-05 - e_time: 49.5904 - 50s/epoch - 17ms/step
Epoch 16/1000
2977/2977 - 57s - loss: nan - NDCG@10: 0.4306 - MAE: nan - RC@5: 0.5121 - RC@10: 0.6073 - val_loss: 0.5753 - val_NDCG@10: 0.4735 - val_MAE: 0.0597 - val_RC@5: 0.5547 - val_RC@10: 0.6425 - lr: 9.8416e-05 - e_time: 56.4107 - 57s/epoch - 19ms/step
Epoch 17/1000
2977/2977 - 56s - loss: nan - NDCG@10: 0.4382 - MAE: nan - RC@5: 0.5208 - RC@10: 0.6159 - val_loss: 0.5734 - val_NDCG@10: 0.4775 - val_MAE: 0.0615 - val_RC@5: 0.5599 - val_RC@10: 0.6458 - lr: 9.8317e-05 - e_time: 55.6364 - 56s/epoch - 19ms/step
Epoch 18/1000
2977/2977 - 54s - loss: nan - NDCG@10: 0.4453 - MAE: nan - RC@5: 0.5280 - RC@10: 0.6232 - val_loss: 0.5708 - val_NDCG@10: 0.4826 - val_MAE: 0.0597 - val_RC@5: 0.5649 - val_RC@10: 0.6489 - lr: 9.8218e-05 - e_time: 53.1407 - 54s/epoch - 18ms/step
Epoch 19/1000
2977/2977 - 54s - loss: nan - NDCG@10: 0.4519 - MAE: nan - RC@5: 0.5356 - RC@10: 0.6295 - val_loss: 0.5684 - val_NDCG@10: 0.4876 - val_MAE: 0.0577 - val_RC@5: 0.5696 - val_RC@10: 0.6529 - lr: 9.8119e-05 - e_time: 53.9418 - 54s/epoch - 18ms/step
Epoch 20/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4578 - MAE: nan - RC@5: 0.5421 - RC@10: 0.6357 - val_loss: 0.5671 - val_NDCG@10: 0.4905 - val_MAE: 0.0555 - val_RC@5: 0.5731 - val_RC@10: 0.6548 - lr: 9.8020e-05 - e_time: 26.0729 - 26s/epoch - 9ms/step
Epoch 21/1000
2977/2977 - 27s - loss: nan - NDCG@10: 0.4635 - MAE: nan - RC@5: 0.5490 - RC@10: 0.6424 - val_loss: 0.5653 - val_NDCG@10: 0.4940 - val_MAE: 0.0547 - val_RC@5: 0.5760 - val_RC@10: 0.6573 - lr: 9.7921e-05 - e_time: 26.2927 - 27s/epoch - 9ms/step
Epoch 22/1000
2977/2977 - 27s - loss: nan - NDCG@10: 0.4685 - MAE: nan - RC@5: 0.5544 - RC@10: 0.6475 - val_loss: 0.5650 - val_NDCG@10: 0.4959 - val_MAE: 0.0558 - val_RC@5: 0.5773 - val_RC@10: 0.6588 - lr: 9.7822e-05 - e_time: 26.3707 - 27s/epoch - 9ms/step
Epoch 23/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4728 - MAE: nan - RC@5: 0.5586 - RC@10: 0.6520 - val_loss: 0.5635 - val_NDCG@10: 0.4981 - val_MAE: 0.0546 - val_RC@5: 0.5799 - val_RC@10: 0.6613 - lr: 9.7723e-05 - e_time: 26.0923 - 26s/epoch - 9ms/step
Epoch 24/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4774 - MAE: nan - RC@5: 0.5636 - RC@10: 0.6564 - val_loss: 0.5634 - val_NDCG@10: 0.5011 - val_MAE: 0.0547 - val_RC@5: 0.5824 - val_RC@10: 0.6624 - lr: 9.7624e-05 - e_time: 26.0562 - 26s/epoch - 9ms/step
Epoch 25/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4814 - MAE: nan - RC@5: 0.5680 - RC@10: 0.6611 - val_loss: 0.5616 - val_NDCG@10: 0.5027 - val_MAE: 0.0518 - val_RC@5: 0.5844 - val_RC@10: 0.6625 - lr: 9.7525e-05 - e_time: 26.0471 - 26s/epoch - 9ms/step
Epoch 26/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4855 - MAE: nan - RC@5: 0.5727 - RC@10: 0.6652 - val_loss: 0.5619 - val_NDCG@10: 0.5031 - val_MAE: 0.0511 - val_RC@5: 0.5849 - val_RC@10: 0.6626 - lr: 9.7426e-05 - e_time: 26.1835 - 26s/epoch - 9ms/step
Epoch 27/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4890 - MAE: nan - RC@5: 0.5772 - RC@10: 0.6684 - val_loss: 0.5620 - val_NDCG@10: 0.5052 - val_MAE: 0.0537 - val_RC@5: 0.5859 - val_RC@10: 0.6647 - lr: 9.7327e-05 - e_time: 26.2309 - 26s/epoch - 9ms/step
Epoch 28/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4918 - MAE: nan - RC@5: 0.5800 - RC@10: 0.6717 - val_loss: 0.5611 - val_NDCG@10: 0.5053 - val_MAE: 0.0518 - val_RC@5: 0.5875 - val_RC@10: 0.6634 - lr: 9.7228e-05 - e_time: 26.1484 - 26s/epoch - 9ms/step
Epoch 29/1000
2977/2977 - 27s - loss: nan - NDCG@10: 0.4938 - MAE: nan - RC@5: 0.5827 - RC@10: 0.6736 - val_loss: 0.5607 - val_NDCG@10: 0.5069 - val_MAE: 0.0498 - val_RC@5: 0.5871 - val_RC@10: 0.6646 - lr: 9.7129e-05 - e_time: 26.5396 - 27s/epoch - 9ms/step
Epoch 30/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.4976 - MAE: nan - RC@5: 0.5858 - RC@10: 0.6772 - val_loss: 0.5611 - val_NDCG@10: 0.5078 - val_MAE: 0.0507 - val_RC@5: 0.5892 - val_RC@10: 0.6656 - lr: 9.7030e-05 - e_time: 26.1890 - 26s/epoch - 9ms/step
Epoch 31/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5005 - MAE: nan - RC@5: 0.5896 - RC@10: 0.6812 - val_loss: 0.5604 - val_NDCG@10: 0.5086 - val_MAE: 0.0490 - val_RC@5: 0.5901 - val_RC@10: 0.6655 - lr: 9.6931e-05 - e_time: 26.1847 - 26s/epoch - 9ms/step
Epoch 32/1000
2977/2977 - 27s - loss: nan - NDCG@10: 0.5024 - MAE: nan - RC@5: 0.5922 - RC@10: 0.6825 - val_loss: 0.5602 - val_NDCG@10: 0.5090 - val_MAE: 0.0474 - val_RC@5: 0.5905 - val_RC@10: 0.6657 - lr: 9.6832e-05 - e_time: 26.3866 - 27s/epoch - 9ms/step
Epoch 33/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5053 - MAE: nan - RC@5: 0.5949 - RC@10: 0.6850 - val_loss: 0.5606 - val_NDCG@10: 0.5099 - val_MAE: 0.0469 - val_RC@5: 0.5902 - val_RC@10: 0.6659 - lr: 9.6733e-05 - e_time: 26.1626 - 26s/epoch - 9ms/step
Epoch 34/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5077 - MAE: nan - RC@5: 0.5975 - RC@10: 0.6877 - val_loss: 0.5607 - val_NDCG@10: 0.5102 - val_MAE: 0.0492 - val_RC@5: 0.5917 - val_RC@10: 0.6663 - lr: 9.6634e-05 - e_time: 26.0228 - 26s/epoch - 9ms/step
Epoch 35/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5097 - MAE: nan - RC@5: 0.6006 - RC@10: 0.6903 - val_loss: 0.5604 - val_NDCG@10: 0.5105 - val_MAE: 0.0483 - val_RC@5: 0.5907 - val_RC@10: 0.6672 - lr: 9.6535e-05 - e_time: 26.0309 - 26s/epoch - 9ms/step
Epoch 36/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5114 - MAE: nan - RC@5: 0.6025 - RC@10: 0.6922 - val_loss: 0.5599 - val_NDCG@10: 0.5101 - val_MAE: 0.0454 - val_RC@5: 0.5901 - val_RC@10: 0.6652 - lr: 9.6436e-05 - e_time: 26.0106 - 26s/epoch - 9ms/step
Epoch 37/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5138 - MAE: nan - RC@5: 0.6048 - RC@10: 0.6949 - val_loss: 0.5609 - val_NDCG@10: 0.5103 - val_MAE: 0.0473 - val_RC@5: 0.5910 - val_RC@10: 0.6660 - lr: 9.6337e-05 - e_time: 26.1094 - 26s/epoch - 9ms/step
Epoch 38/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5153 - MAE: nan - RC@5: 0.6067 - RC@10: 0.6969 - val_loss: 0.5600 - val_NDCG@10: 0.5113 - val_MAE: 0.0448 - val_RC@5: 0.5922 - val_RC@10: 0.6663 - lr: 9.6238e-05 - e_time: 26.1917 - 26s/epoch - 9ms/step
Epoch 39/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5179 - MAE: nan - RC@5: 0.6089 - RC@10: 0.6987 - val_loss: 0.5605 - val_NDCG@10: 0.5114 - val_MAE: 0.0442 - val_RC@5: 0.5915 - val_RC@10: 0.6662 - lr: 9.6139e-05 - e_time: 26.0946 - 26s/epoch - 9ms/step
Epoch 40/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5197 - MAE: nan - RC@5: 0.6117 - RC@10: 0.7006 - val_loss: 0.5606 - val_NDCG@10: 0.5113 - val_MAE: 0.0436 - val_RC@5: 0.5908 - val_RC@10: 0.6654 - lr: 9.6040e-05 - e_time: 26.0075 - 26s/epoch - 9ms/step
Epoch 41/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5207 - MAE: nan - RC@5: 0.6129 - RC@10: 0.7023 - val_loss: 0.5610 - val_NDCG@10: 0.5111 - val_MAE: 0.0444 - val_RC@5: 0.5913 - val_RC@10: 0.6651 - lr: 9.5941e-05 - e_time: 26.0233 - 26s/epoch - 9ms/step
Epoch 42/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5222 - MAE: nan - RC@5: 0.6144 - RC@10: 0.7037 - val_loss: 0.5613 - val_NDCG@10: 0.5113 - val_MAE: 0.0448 - val_RC@5: 0.5909 - val_RC@10: 0.6650 - lr: 9.5842e-05 - e_time: 26.3156 - 26s/epoch - 9ms/step
Epoch 43/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5238 - MAE: nan - RC@5: 0.6158 - RC@10: 0.7056 - val_loss: 0.5609 - val_NDCG@10: 0.5107 - val_MAE: 0.0433 - val_RC@5: 0.5915 - val_RC@10: 0.6648 - lr: 9.5743e-05 - e_time: 26.3429 - 26s/epoch - 9ms/step
Epoch 44/1000
2977/2977 - 27s - loss: nan - NDCG@10: 0.5252 - MAE: nan - RC@5: 0.6181 - RC@10: 0.7070 - val_loss: 0.5617 - val_NDCG@10: 0.5107 - val_MAE: 0.0446 - val_RC@5: 0.5911 - val_RC@10: 0.6642 - lr: 9.5644e-05 - e_time: 26.7029 - 27s/epoch - 9ms/step
Epoch 45/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5265 - MAE: nan - RC@5: 0.6187 - RC@10: 0.7082 - val_loss: 0.5621 - val_NDCG@10: 0.5105 - val_MAE: 0.0451 - val_RC@5: 0.5901 - val_RC@10: 0.6648 - lr: 9.5545e-05 - e_time: 26.3107 - 26s/epoch - 9ms/step
Epoch 46/1000
2977/2977 - 26s - loss: nan - NDCG@10: 0.5273 - MAE: nan - RC@5: 0.6204 - RC@10: 0.7094 - val_loss: 0.5624 - val_NDCG@10: 0.5108 - val_MAE: 0.0430 - val_RC@5: 0.5896 - val_RC@10: 0.6636 - lr: 9.5446e-05 - e_time: 25.6019 - 26s/epoch - 9ms/step
Epoch 46: early stopping
Traceback (most recent call last):
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3621, in get_loc
    return self._engine.get_loc(casted_key)
  File "pandas/_libs/index.pyx", line 136, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 163, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'y'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 124, in <module>
    mdl.train(dev=True, save_model=True, callbacks=[])
  File "/media/nas/pperez/code/TAVtext/src/models/KerasModelClass.py", line 50, in train
    self.__train_model__(train_sequence=train_seq, dev_sequence=dev_seq, save_model=save_model, train_cfg=train_cfg, callbacks=callbacks)
  File "/media/nas/pperez/code/TAVtext/src/models/KerasModelClass.py", line 176, in __train_model__
    hplt = sns.lineplot(x=range(done_epochs), y=hist.history[self.CONFIG["model"]["early_st_monitor"].replace("val_", "")], label=self.CONFIG["model"]["early_st_monitor"].replace("val_", ""))
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/seaborn/relational.py", line 645, in lineplot
    p.plot(ax, kwargs)
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/seaborn/relational.py", line 459, in plot
    lines = ax.plot(sub_data["x"], sub_data["y"], **kws)
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/pandas/core/frame.py", line 3505, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3623, in get_loc
    raise KeyError(key) from err
KeyError: 'y'
