2023-06-07 22:42:12.350397: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "WATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, None)]       0           []                               
                                                                                                  
 wr_att_embs (Embedding)        (None, None, 2)      28844       ['input_1[0][0]']                
                                                                                                  
 dropout (Dropout)              (None, None, 2)      0           ['wr_att_embs[0][0]']            
                                                                                                  
 tf.linalg.matmul (TFOpLambda)  (None, None, None)   0           ['dropout[0][0]',                
                                                                  'dropout[0][0]']                
                                                                                                  
 activation (Activation)        (None, None, None)   0           ['tf.linalg.matmul[0][0]']       
                                                                                                  
 all_words (Embedding)          (None, None, 128)    1846016     ['input_1[0][0]']                
                                                                                                  
 input_2 (InputLayer)           [(None, 3415)]       0           []                               
                                                                                                  
 tf.einsum (TFOpLambda)         (None, None, 128)    0           ['activation[0][0]',             
                                                                  'all_words[0][0]']              
                                                                                                  
 all_items (Embedding)          (None, 3415, 128)    437120      ['input_2[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, None, 128)    0           ['tf.einsum[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 3415, 128)    0           ['all_items[0][0]']              
                                                                                                  
 tf.math.not_equal_1 (TFOpLambd  (None, None)        0           ['input_1[0][0]']                
 a)                                                                                               
                                                                                                  
 dropout_1 (Dropout)            (None, None, 128)    0           ['word_emb[0][0]']               
                                                                                                  
 dropout_2 (Dropout)            (None, 3415, 128)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.cast_1 (TFOpLambda)         (None, None)         0           ['tf.math.not_equal_1[0][0]']    
                                                                                                  
 dot_mul (Lambda)               (None, None, 3415)   0           ['dropout_1[0][0]',              
                                                                  'dropout_2[0][0]']              
                                                                                                  
 tf.expand_dims_1 (TFOpLambda)  (None, None, 1)      0           ['tf.cast_1[0][0]']              
                                                                                                  
 dotprod (Activation)           (None, None, 3415)   0           ['dot_mul[0][0]']                
                                                                                                  
 tf.tile_1 (TFOpLambda)         (None, None, 3415)   0           ['tf.expand_dims_1[0][0]']       
                                                                                                  
 dot_mask (Lambda)              (None, None, 3415)   0           ['dotprod[0][0]',                
                                                                  'tf.tile_1[0][0]']              
                                                                                                  
 sum (Lambda)                   (None, 3415)         0           ['dot_mask[0][0]',               
                                                                  'tf.tile_1[0][0]']              
                                                                                                  
 activation_1 (Activation)      (None, 3415)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 3415)         0           ['activation_1[0][0]']           
                                                                                                  
==================================================================================================
Total params: 2,311,980
Trainable params: 2,311,980
Non-trainable params: 0
__________________________________________________________________________________________________
None
Epoch 1/1000
5062/5062 - 74s - loss: nan - NDCG@10: 0.0176 - MAE: nan - RC@5: 0.0204 - RC@10: 0.0310 - val_loss: 0.8805 - val_NDCG@10: 0.0218 - val_MAE: 0.0928 - val_RC@5: 0.0255 - val_RC@10: 0.0390 - lr: 9.9901e-05 - e_time: 73.3415 - 74s/epoch - 15ms/step
Epoch 2/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.0245 - MAE: nan - RC@5: 0.0288 - RC@10: 0.0437 - val_loss: 0.8677 - val_NDCG@10: 0.0323 - val_MAE: 0.1012 - val_RC@5: 0.0382 - val_RC@10: 0.0581 - lr: 9.9802e-05 - e_time: 70.9147 - 71s/epoch - 14ms/step
Epoch 3/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.0538 - MAE: nan - RC@5: 0.0654 - RC@10: 0.0920 - val_loss: 0.8150 - val_NDCG@10: 0.1004 - val_MAE: 0.1003 - val_RC@5: 0.1192 - val_RC@10: 0.1602 - lr: 9.9703e-05 - e_time: 70.9033 - 71s/epoch - 14ms/step
Epoch 4/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.1142 - MAE: nan - RC@5: 0.1356 - RC@10: 0.1813 - val_loss: 0.7517 - val_NDCG@10: 0.1648 - val_MAE: 0.0984 - val_RC@5: 0.1952 - val_RC@10: 0.2527 - lr: 9.9604e-05 - e_time: 71.0405 - 71s/epoch - 14ms/step
Epoch 5/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.1600 - MAE: nan - RC@5: 0.1893 - RC@10: 0.2494 - val_loss: 0.7084 - val_NDCG@10: 0.2118 - val_MAE: 0.0895 - val_RC@5: 0.2495 - val_RC@10: 0.3198 - lr: 9.9505e-05 - e_time: 71.0616 - 71s/epoch - 14ms/step
Epoch 6/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.1943 - MAE: nan - RC@5: 0.2306 - RC@10: 0.2999 - val_loss: 0.6798 - val_NDCG@10: 0.2449 - val_MAE: 0.0842 - val_RC@5: 0.2888 - val_RC@10: 0.3656 - lr: 9.9406e-05 - e_time: 70.8965 - 71s/epoch - 14ms/step
Epoch 7/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.2220 - MAE: nan - RC@5: 0.2630 - RC@10: 0.3383 - val_loss: 0.6603 - val_NDCG@10: 0.2709 - val_MAE: 0.0799 - val_RC@5: 0.3192 - val_RC@10: 0.3984 - lr: 9.9307e-05 - e_time: 70.8914 - 71s/epoch - 14ms/step
Epoch 8/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.2435 - MAE: nan - RC@5: 0.2884 - RC@10: 0.3680 - val_loss: 0.6463 - val_NDCG@10: 0.2909 - val_MAE: 0.0783 - val_RC@5: 0.3419 - val_RC@10: 0.4229 - lr: 9.9208e-05 - e_time: 70.9410 - 71s/epoch - 14ms/step
Epoch 9/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.2616 - MAE: nan - RC@5: 0.3100 - RC@10: 0.3916 - val_loss: 0.6363 - val_NDCG@10: 0.3065 - val_MAE: 0.0795 - val_RC@5: 0.3589 - val_RC@10: 0.4424 - lr: 9.9109e-05 - e_time: 70.9894 - 71s/epoch - 14ms/step
Epoch 10/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.2770 - MAE: nan - RC@5: 0.3277 - RC@10: 0.4108 - val_loss: 0.6284 - val_NDCG@10: 0.3204 - val_MAE: 0.0752 - val_RC@5: 0.3751 - val_RC@10: 0.4579 - lr: 9.9010e-05 - e_time: 71.0172 - 71s/epoch - 14ms/step
Epoch 11/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.2897 - MAE: nan - RC@5: 0.3424 - RC@10: 0.4266 - val_loss: 0.6227 - val_NDCG@10: 0.3304 - val_MAE: 0.0723 - val_RC@5: 0.3864 - val_RC@10: 0.4696 - lr: 9.8911e-05 - e_time: 71.0042 - 71s/epoch - 14ms/step
Epoch 12/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3008 - MAE: nan - RC@5: 0.3555 - RC@10: 0.4404 - val_loss: 0.6180 - val_NDCG@10: 0.3397 - val_MAE: 0.0704 - val_RC@5: 0.3981 - val_RC@10: 0.4801 - lr: 9.8812e-05 - e_time: 70.9864 - 71s/epoch - 14ms/step
Epoch 13/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3112 - MAE: nan - RC@5: 0.3671 - RC@10: 0.4531 - val_loss: 0.6143 - val_NDCG@10: 0.3482 - val_MAE: 0.0678 - val_RC@5: 0.4063 - val_RC@10: 0.4895 - lr: 9.8713e-05 - e_time: 70.9869 - 71s/epoch - 14ms/step
Epoch 14/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3189 - MAE: nan - RC@5: 0.3759 - RC@10: 0.4621 - val_loss: 0.6115 - val_NDCG@10: 0.3544 - val_MAE: 0.0659 - val_RC@5: 0.4129 - val_RC@10: 0.4950 - lr: 9.8614e-05 - e_time: 70.9981 - 71s/epoch - 14ms/step
Epoch 15/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3270 - MAE: nan - RC@5: 0.3848 - RC@10: 0.4716 - val_loss: 0.6091 - val_NDCG@10: 0.3593 - val_MAE: 0.0644 - val_RC@5: 0.4191 - val_RC@10: 0.5004 - lr: 9.8515e-05 - e_time: 71.0297 - 71s/epoch - 14ms/step
Epoch 16/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3330 - MAE: nan - RC@5: 0.3924 - RC@10: 0.4790 - val_loss: 0.6073 - val_NDCG@10: 0.3642 - val_MAE: 0.0627 - val_RC@5: 0.4237 - val_RC@10: 0.5050 - lr: 9.8416e-05 - e_time: 71.0502 - 71s/epoch - 14ms/step
Epoch 17/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3391 - MAE: nan - RC@5: 0.3993 - RC@10: 0.4864 - val_loss: 0.6055 - val_NDCG@10: 0.3678 - val_MAE: 0.0632 - val_RC@5: 0.4273 - val_RC@10: 0.5077 - lr: 9.8317e-05 - e_time: 70.9930 - 71s/epoch - 14ms/step
Epoch 18/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3447 - MAE: nan - RC@5: 0.4055 - RC@10: 0.4930 - val_loss: 0.6045 - val_NDCG@10: 0.3712 - val_MAE: 0.0639 - val_RC@5: 0.4310 - val_RC@10: 0.5118 - lr: 9.8218e-05 - e_time: 71.0214 - 71s/epoch - 14ms/step
Epoch 19/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3494 - MAE: nan - RC@5: 0.4104 - RC@10: 0.4987 - val_loss: 0.6038 - val_NDCG@10: 0.3749 - val_MAE: 0.0629 - val_RC@5: 0.4351 - val_RC@10: 0.5148 - lr: 9.8119e-05 - e_time: 71.0429 - 71s/epoch - 14ms/step
Epoch 20/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3539 - MAE: nan - RC@5: 0.4159 - RC@10: 0.5042 - val_loss: 0.6025 - val_NDCG@10: 0.3766 - val_MAE: 0.0599 - val_RC@5: 0.4372 - val_RC@10: 0.5163 - lr: 9.8020e-05 - e_time: 70.9933 - 71s/epoch - 14ms/step
Epoch 21/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3579 - MAE: nan - RC@5: 0.4208 - RC@10: 0.5093 - val_loss: 0.6016 - val_NDCG@10: 0.3786 - val_MAE: 0.0587 - val_RC@5: 0.4385 - val_RC@10: 0.5182 - lr: 9.7921e-05 - e_time: 70.9935 - 71s/epoch - 14ms/step
Epoch 22/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3611 - MAE: nan - RC@5: 0.4245 - RC@10: 0.5126 - val_loss: 0.6010 - val_NDCG@10: 0.3798 - val_MAE: 0.0581 - val_RC@5: 0.4404 - val_RC@10: 0.5193 - lr: 9.7822e-05 - e_time: 71.0080 - 71s/epoch - 14ms/step
Epoch 23/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3639 - MAE: nan - RC@5: 0.4276 - RC@10: 0.5158 - val_loss: 0.6016 - val_NDCG@10: 0.3809 - val_MAE: 0.0614 - val_RC@5: 0.4414 - val_RC@10: 0.5203 - lr: 9.7723e-05 - e_time: 71.0164 - 71s/epoch - 14ms/step
Epoch 24/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3671 - MAE: nan - RC@5: 0.4315 - RC@10: 0.5203 - val_loss: 0.6002 - val_NDCG@10: 0.3823 - val_MAE: 0.0552 - val_RC@5: 0.4431 - val_RC@10: 0.5220 - lr: 9.7624e-05 - e_time: 70.9727 - 71s/epoch - 14ms/step
Epoch 25/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3704 - MAE: nan - RC@5: 0.4352 - RC@10: 0.5244 - val_loss: 0.6008 - val_NDCG@10: 0.3819 - val_MAE: 0.0578 - val_RC@5: 0.4433 - val_RC@10: 0.5214 - lr: 9.7525e-05 - e_time: 71.0260 - 71s/epoch - 14ms/step
Epoch 26/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3725 - MAE: nan - RC@5: 0.4378 - RC@10: 0.5274 - val_loss: 0.6007 - val_NDCG@10: 0.3832 - val_MAE: 0.0583 - val_RC@5: 0.4441 - val_RC@10: 0.5217 - lr: 9.7426e-05 - e_time: 70.9907 - 71s/epoch - 14ms/step
Epoch 27/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3746 - MAE: nan - RC@5: 0.4401 - RC@10: 0.5295 - val_loss: 0.6006 - val_NDCG@10: 0.3835 - val_MAE: 0.0567 - val_RC@5: 0.4442 - val_RC@10: 0.5222 - lr: 9.7327e-05 - e_time: 70.9896 - 71s/epoch - 14ms/step
Epoch 28/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3770 - MAE: nan - RC@5: 0.4426 - RC@10: 0.5324 - val_loss: 0.6002 - val_NDCG@10: 0.3839 - val_MAE: 0.0541 - val_RC@5: 0.4446 - val_RC@10: 0.5226 - lr: 9.7228e-05 - e_time: 71.0101 - 71s/epoch - 14ms/step
Epoch 29/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3788 - MAE: nan - RC@5: 0.4449 - RC@10: 0.5347 - val_loss: 0.6001 - val_NDCG@10: 0.3846 - val_MAE: 0.0525 - val_RC@5: 0.4449 - val_RC@10: 0.5227 - lr: 9.7129e-05 - e_time: 71.0134 - 71s/epoch - 14ms/step
Epoch 30/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3810 - MAE: nan - RC@5: 0.4474 - RC@10: 0.5381 - val_loss: 0.6007 - val_NDCG@10: 0.3846 - val_MAE: 0.0561 - val_RC@5: 0.4442 - val_RC@10: 0.5228 - lr: 9.7030e-05 - e_time: 71.0165 - 71s/epoch - 14ms/step
Epoch 31/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3825 - MAE: nan - RC@5: 0.4495 - RC@10: 0.5398 - val_loss: 0.6009 - val_NDCG@10: 0.3850 - val_MAE: 0.0539 - val_RC@5: 0.4452 - val_RC@10: 0.5229 - lr: 9.6931e-05 - e_time: 71.0174 - 71s/epoch - 14ms/step
Epoch 32/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3847 - MAE: nan - RC@5: 0.4519 - RC@10: 0.5426 - val_loss: 0.6010 - val_NDCG@10: 0.3850 - val_MAE: 0.0547 - val_RC@5: 0.4451 - val_RC@10: 0.5229 - lr: 9.6832e-05 - e_time: 71.0022 - 71s/epoch - 14ms/step
Epoch 33/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3864 - MAE: nan - RC@5: 0.4541 - RC@10: 0.5443 - val_loss: 0.6012 - val_NDCG@10: 0.3849 - val_MAE: 0.0535 - val_RC@5: 0.4450 - val_RC@10: 0.5230 - lr: 9.6733e-05 - e_time: 70.9993 - 71s/epoch - 14ms/step
Epoch 34/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3882 - MAE: nan - RC@5: 0.4554 - RC@10: 0.5466 - val_loss: 0.6013 - val_NDCG@10: 0.3848 - val_MAE: 0.0528 - val_RC@5: 0.4453 - val_RC@10: 0.5220 - lr: 9.6634e-05 - e_time: 71.0196 - 71s/epoch - 14ms/step
Epoch 35/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3889 - MAE: nan - RC@5: 0.4571 - RC@10: 0.5478 - val_loss: 0.6014 - val_NDCG@10: 0.3853 - val_MAE: 0.0526 - val_RC@5: 0.4460 - val_RC@10: 0.5218 - lr: 9.6535e-05 - e_time: 71.0134 - 71s/epoch - 14ms/step
Epoch 36/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3906 - MAE: nan - RC@5: 0.4588 - RC@10: 0.5500 - val_loss: 0.6015 - val_NDCG@10: 0.3853 - val_MAE: 0.0511 - val_RC@5: 0.4457 - val_RC@10: 0.5222 - lr: 9.6436e-05 - e_time: 71.0068 - 71s/epoch - 14ms/step
Epoch 37/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3918 - MAE: nan - RC@5: 0.4602 - RC@10: 0.5511 - val_loss: 0.6013 - val_NDCG@10: 0.3849 - val_MAE: 0.0485 - val_RC@5: 0.4447 - val_RC@10: 0.5211 - lr: 9.6337e-05 - e_time: 71.0306 - 71s/epoch - 14ms/step
Epoch 38/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3927 - MAE: nan - RC@5: 0.4609 - RC@10: 0.5527 - val_loss: 0.6023 - val_NDCG@10: 0.3842 - val_MAE: 0.0522 - val_RC@5: 0.4449 - val_RC@10: 0.5213 - lr: 9.6238e-05 - e_time: 70.9782 - 71s/epoch - 14ms/step
Epoch 39/1000
5062/5062 - 71s - loss: nan - NDCG@10: 0.3946 - MAE: nan - RC@5: 0.4634 - RC@10: 0.5547 - val_loss: 0.6028 - val_NDCG@10: 0.3841 - val_MAE: 0.0530 - val_RC@5: 0.4443 - val_RC@10: 0.5208 - lr: 9.6139e-05 - e_time: 71.0209 - 71s/epoch - 14ms/step
Epoch 39: early stopping
Traceback (most recent call last):
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3621, in get_loc
    return self._engine.get_loc(casted_key)
  File "pandas/_libs/index.pyx", line 136, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 163, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'y'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 124, in <module>
    mdl.train(dev=True, save_model=True, callbacks=[])
  File "/media/nas/pperez/code/TAVtext/src/models/KerasModelClass.py", line 50, in train
    self.__train_model__(train_sequence=train_seq, dev_sequence=dev_seq, save_model=save_model, train_cfg=train_cfg, callbacks=callbacks)
  File "/media/nas/pperez/code/TAVtext/src/models/KerasModelClass.py", line 176, in __train_model__
    hplt = sns.lineplot(x=range(done_epochs), y=hist.history[self.CONFIG["model"]["early_st_monitor"].replace("val_", "")], label=self.CONFIG["model"]["early_st_monitor"].replace("val_", ""))
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/seaborn/relational.py", line 645, in lineplot
    p.plot(ax, kwargs)
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/seaborn/relational.py", line 459, in plot
    lines = ax.plot(sub_data["x"], sub_data["y"], **kws)
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/pandas/core/frame.py", line 3505, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3623, in get_loc
    raise KeyError(key) from err
KeyError: 'y'
2023-06-08 23:18:47.070801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "WATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, None)]       0           []                               
                                                                                                  
 wr_att_embs (Embedding)        (None, None, 32)     461504      ['input_1[0][0]']                
                                                                                                  
 dropout (Dropout)              (None, None, 32)     0           ['wr_att_embs[0][0]']            
                                                                                                  
 tf.linalg.matmul (TFOpLambda)  (None, None, None)   0           ['dropout[0][0]',                
                                                                  'dropout[0][0]']                
                                                                                                  
 all_words (Embedding)          (None, None, 128)    1846016     ['input_1[0][0]']                
                                                                                                  
 input_2 (InputLayer)           [(None, 3415)]       0           []                               
                                                                                                  
 tf.einsum (TFOpLambda)         (None, None, 128)    0           ['tf.linalg.matmul[0][0]',       
                                                                  'all_words[0][0]']              
                                                                                                  
 all_items (Embedding)          (None, 3415, 128)    437120      ['input_2[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, None, 128)    0           ['tf.einsum[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 3415, 128)    0           ['all_items[0][0]']              
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, None)        0           ['input_1[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, None, 128)    0           ['word_emb[0][0]']               
                                                                                                  
 dropout_2 (Dropout)            (None, 3415, 128)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.cast (TFOpLambda)           (None, None)         0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dot_mul (Lambda)               (None, None, 3415)   0           ['dropout_1[0][0]',              
                                                                  'dropout_2[0][0]']              
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, None, 1)      0           ['tf.cast[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, None, 3415)   0           ['dot_mul[0][0]']                
                                                                                                  
 tf.tile (TFOpLambda)           (None, None, 3415)   0           ['tf.expand_dims[0][0]']         
                                                                                                  
 dot_mask (Lambda)              (None, None, 3415)   0           ['dotprod[0][0]',                
                                                                  'tf.tile[0][0]']                
                                                                                                  
 sum (Lambda)                   (None, 3415)         0           ['dot_mask[0][0]',               
                                                                  'tf.tile[0][0]']                
                                                                                                  
 activation (Activation)        (None, 3415)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 3415)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,744,640
Trainable params: 2,744,640
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-06-08 23:19:11.391827: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 461254248 exceeds 10% of free system memory.
[92m[INFO] Not saving the model...[0m
2023-06-08 23:19:13.534413: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 461254248 exceeds 10% of free system memory.
Epoch 1/1000
2531/2531 - 69s - loss: nan - NDCG@10: 0.0107 - MAE: nan - RC@5: 0.0127 - RC@10: 0.0209 - val_loss: 0.8896 - val_NDCG@10: 0.0157 - val_MAE: 0.0635 - val_RC@5: 0.0174 - val_RC@10: 0.0308 - lr: 4.9951e-05 - 69s/epoch - 27ms/step
Epoch 2/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.0202 - MAE: nan - RC@5: 0.0240 - RC@10: 0.0363 - val_loss: 0.8773 - val_NDCG@10: 0.0258 - val_MAE: 0.0789 - val_RC@5: 0.0301 - val_RC@10: 0.0464 - lr: 4.9901e-05 - 66s/epoch - 26ms/step
Epoch 3/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.0287 - MAE: nan - RC@5: 0.0345 - RC@10: 0.0502 - val_loss: 0.8668 - val_NDCG@10: 0.0399 - val_MAE: 0.0860 - val_RC@5: 0.0494 - val_RC@10: 0.0683 - lr: 4.9852e-05 - 66s/epoch - 26ms/step
Epoch 4/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.0525 - MAE: nan - RC@5: 0.0639 - RC@10: 0.0872 - val_loss: 0.8381 - val_NDCG@10: 0.0868 - val_MAE: 0.0836 - val_RC@5: 0.1058 - val_RC@10: 0.1394 - lr: 4.9802e-05 - 66s/epoch - 26ms/step
Epoch 5/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.0953 - MAE: nan - RC@5: 0.1137 - RC@10: 0.1489 - val_loss: 0.7967 - val_NDCG@10: 0.1347 - val_MAE: 0.0812 - val_RC@5: 0.1586 - val_RC@10: 0.2026 - lr: 4.9753e-05 - 66s/epoch - 26ms/step
Epoch 6/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.1292 - MAE: nan - RC@5: 0.1523 - RC@10: 0.1977 - val_loss: 0.7603 - val_NDCG@10: 0.1717 - val_MAE: 0.0853 - val_RC@5: 0.2018 - val_RC@10: 0.2550 - lr: 4.9703e-05 - 66s/epoch - 26ms/step
Epoch 7/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.1585 - MAE: nan - RC@5: 0.1864 - RC@10: 0.2407 - val_loss: 0.7301 - val_NDCG@10: 0.2031 - val_MAE: 0.0855 - val_RC@5: 0.2382 - val_RC@10: 0.2996 - lr: 4.9654e-05 - 66s/epoch - 26ms/step
Epoch 8/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.1832 - MAE: nan - RC@5: 0.2157 - RC@10: 0.2770 - val_loss: 0.7066 - val_NDCG@10: 0.2284 - val_MAE: 0.0822 - val_RC@5: 0.2669 - val_RC@10: 0.3349 - lr: 4.9604e-05 - 66s/epoch - 26ms/step
Epoch 9/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2043 - MAE: nan - RC@5: 0.2408 - RC@10: 0.3074 - val_loss: 0.6880 - val_NDCG@10: 0.2484 - val_MAE: 0.0790 - val_RC@5: 0.2917 - val_RC@10: 0.3629 - lr: 4.9555e-05 - 66s/epoch - 26ms/step
Epoch 10/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2216 - MAE: nan - RC@5: 0.2611 - RC@10: 0.3324 - val_loss: 0.6739 - val_NDCG@10: 0.2660 - val_MAE: 0.0725 - val_RC@5: 0.3109 - val_RC@10: 0.3861 - lr: 4.9505e-05 - 66s/epoch - 26ms/step
Epoch 11/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2367 - MAE: nan - RC@5: 0.2790 - RC@10: 0.3534 - val_loss: 0.6610 - val_NDCG@10: 0.2799 - val_MAE: 0.0728 - val_RC@5: 0.3270 - val_RC@10: 0.4045 - lr: 4.9456e-05 - 66s/epoch - 26ms/step
Epoch 12/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2497 - MAE: nan - RC@5: 0.2941 - RC@10: 0.3705 - val_loss: 0.6508 - val_NDCG@10: 0.2916 - val_MAE: 0.0714 - val_RC@5: 0.3408 - val_RC@10: 0.4192 - lr: 4.9406e-05 - 66s/epoch - 26ms/step
Epoch 13/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2611 - MAE: nan - RC@5: 0.3072 - RC@10: 0.3859 - val_loss: 0.6427 - val_NDCG@10: 0.3021 - val_MAE: 0.0693 - val_RC@5: 0.3524 - val_RC@10: 0.4326 - lr: 4.9357e-05 - 66s/epoch - 26ms/step
Epoch 14/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2720 - MAE: nan - RC@5: 0.3199 - RC@10: 0.4002 - val_loss: 0.6356 - val_NDCG@10: 0.3113 - val_MAE: 0.0690 - val_RC@5: 0.3629 - val_RC@10: 0.4438 - lr: 4.9307e-05 - 66s/epoch - 26ms/step
Epoch 15/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2810 - MAE: nan - RC@5: 0.3314 - RC@10: 0.4118 - val_loss: 0.6300 - val_NDCG@10: 0.3196 - val_MAE: 0.0665 - val_RC@5: 0.3717 - val_RC@10: 0.4545 - lr: 4.9258e-05 - 66s/epoch - 26ms/step
Epoch 16/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2891 - MAE: nan - RC@5: 0.3403 - RC@10: 0.4230 - val_loss: 0.6247 - val_NDCG@10: 0.3261 - val_MAE: 0.0655 - val_RC@5: 0.3797 - val_RC@10: 0.4620 - lr: 4.9208e-05 - 66s/epoch - 26ms/step
Epoch 17/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.2966 - MAE: nan - RC@5: 0.3489 - RC@10: 0.4322 - val_loss: 0.6206 - val_NDCG@10: 0.3323 - val_MAE: 0.0621 - val_RC@5: 0.3861 - val_RC@10: 0.4693 - lr: 4.9159e-05 - 66s/epoch - 26ms/step
Epoch 18/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3034 - MAE: nan - RC@5: 0.3567 - RC@10: 0.4403 - val_loss: 0.6172 - val_NDCG@10: 0.3385 - val_MAE: 0.0590 - val_RC@5: 0.3931 - val_RC@10: 0.4764 - lr: 4.9109e-05 - 66s/epoch - 26ms/step
Epoch 19/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3093 - MAE: nan - RC@5: 0.3641 - RC@10: 0.4480 - val_loss: 0.6131 - val_NDCG@10: 0.3429 - val_MAE: 0.0599 - val_RC@5: 0.3981 - val_RC@10: 0.4819 - lr: 4.9060e-05 - 66s/epoch - 26ms/step
Epoch 20/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3149 - MAE: nan - RC@5: 0.3706 - RC@10: 0.4553 - val_loss: 0.6110 - val_NDCG@10: 0.3479 - val_MAE: 0.0557 - val_RC@5: 0.4041 - val_RC@10: 0.4874 - lr: 4.9010e-05 - 66s/epoch - 26ms/step
Epoch 21/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3196 - MAE: nan - RC@5: 0.3756 - RC@10: 0.4614 - val_loss: 0.6080 - val_NDCG@10: 0.3520 - val_MAE: 0.0556 - val_RC@5: 0.4090 - val_RC@10: 0.4920 - lr: 4.8961e-05 - 66s/epoch - 26ms/step
Epoch 22/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3249 - MAE: nan - RC@5: 0.3821 - RC@10: 0.4675 - val_loss: 0.6066 - val_NDCG@10: 0.3555 - val_MAE: 0.0518 - val_RC@5: 0.4123 - val_RC@10: 0.4952 - lr: 4.8911e-05 - 66s/epoch - 26ms/step
Epoch 23/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3290 - MAE: nan - RC@5: 0.3863 - RC@10: 0.4728 - val_loss: 0.6039 - val_NDCG@10: 0.3586 - val_MAE: 0.0528 - val_RC@5: 0.4162 - val_RC@10: 0.4992 - lr: 4.8862e-05 - 66s/epoch - 26ms/step
Epoch 24/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3335 - MAE: nan - RC@5: 0.3923 - RC@10: 0.4789 - val_loss: 0.6024 - val_NDCG@10: 0.3604 - val_MAE: 0.0520 - val_RC@5: 0.4184 - val_RC@10: 0.5017 - lr: 4.8812e-05 - 66s/epoch - 26ms/step
Epoch 25/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3366 - MAE: nan - RC@5: 0.3961 - RC@10: 0.4826 - val_loss: 0.6000 - val_NDCG@10: 0.3627 - val_MAE: 0.0535 - val_RC@5: 0.4206 - val_RC@10: 0.5037 - lr: 4.8763e-05 - 66s/epoch - 26ms/step
Epoch 26/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3402 - MAE: nan - RC@5: 0.3996 - RC@10: 0.4873 - val_loss: 0.5992 - val_NDCG@10: 0.3657 - val_MAE: 0.0513 - val_RC@5: 0.4242 - val_RC@10: 0.5071 - lr: 4.8713e-05 - 66s/epoch - 26ms/step
Epoch 27/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3434 - MAE: nan - RC@5: 0.4032 - RC@10: 0.4907 - val_loss: 0.5979 - val_NDCG@10: 0.3668 - val_MAE: 0.0515 - val_RC@5: 0.4253 - val_RC@10: 0.5088 - lr: 4.8664e-05 - 66s/epoch - 26ms/step
Epoch 28/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3465 - MAE: nan - RC@5: 0.4075 - RC@10: 0.4949 - val_loss: 0.5981 - val_NDCG@10: 0.3688 - val_MAE: 0.0473 - val_RC@5: 0.4279 - val_RC@10: 0.5108 - lr: 4.8614e-05 - 66s/epoch - 26ms/step
Epoch 29/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3490 - MAE: nan - RC@5: 0.4102 - RC@10: 0.4983 - val_loss: 0.5962 - val_NDCG@10: 0.3701 - val_MAE: 0.0503 - val_RC@5: 0.4296 - val_RC@10: 0.5111 - lr: 4.8565e-05 - 66s/epoch - 26ms/step
Epoch 30/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3513 - MAE: nan - RC@5: 0.4129 - RC@10: 0.5016 - val_loss: 0.5960 - val_NDCG@10: 0.3711 - val_MAE: 0.0486 - val_RC@5: 0.4308 - val_RC@10: 0.5123 - lr: 4.8515e-05 - 66s/epoch - 26ms/step
Epoch 31/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3542 - MAE: nan - RC@5: 0.4162 - RC@10: 0.5046 - val_loss: 0.5961 - val_NDCG@10: 0.3720 - val_MAE: 0.0462 - val_RC@5: 0.4318 - val_RC@10: 0.5137 - lr: 4.8466e-05 - 66s/epoch - 26ms/step
Epoch 32/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3566 - MAE: nan - RC@5: 0.4197 - RC@10: 0.5079 - val_loss: 0.5946 - val_NDCG@10: 0.3731 - val_MAE: 0.0493 - val_RC@5: 0.4337 - val_RC@10: 0.5149 - lr: 4.8416e-05 - 66s/epoch - 26ms/step
Epoch 33/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3584 - MAE: nan - RC@5: 0.4217 - RC@10: 0.5101 - val_loss: 0.5954 - val_NDCG@10: 0.3741 - val_MAE: 0.0450 - val_RC@5: 0.4350 - val_RC@10: 0.5153 - lr: 4.8367e-05 - 66s/epoch - 26ms/step
Epoch 34/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3608 - MAE: nan - RC@5: 0.4245 - RC@10: 0.5131 - val_loss: 0.5948 - val_NDCG@10: 0.3753 - val_MAE: 0.0447 - val_RC@5: 0.4357 - val_RC@10: 0.5166 - lr: 4.8317e-05 - 66s/epoch - 26ms/step
Epoch 35/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3622 - MAE: nan - RC@5: 0.4261 - RC@10: 0.5155 - val_loss: 0.5956 - val_NDCG@10: 0.3766 - val_MAE: 0.0411 - val_RC@5: 0.4372 - val_RC@10: 0.5167 - lr: 4.8268e-05 - 66s/epoch - 26ms/step
Epoch 36/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3636 - MAE: nan - RC@5: 0.4281 - RC@10: 0.5177 - val_loss: 0.5941 - val_NDCG@10: 0.3763 - val_MAE: 0.0450 - val_RC@5: 0.4373 - val_RC@10: 0.5172 - lr: 4.8218e-05 - 66s/epoch - 26ms/step
Epoch 37/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3654 - MAE: nan - RC@5: 0.4298 - RC@10: 0.5192 - val_loss: 0.5951 - val_NDCG@10: 0.3771 - val_MAE: 0.0415 - val_RC@5: 0.4377 - val_RC@10: 0.5180 - lr: 4.8169e-05 - 66s/epoch - 26ms/step
Epoch 38/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3674 - MAE: nan - RC@5: 0.4327 - RC@10: 0.5218 - val_loss: 0.5946 - val_NDCG@10: 0.3766 - val_MAE: 0.0441 - val_RC@5: 0.4375 - val_RC@10: 0.5177 - lr: 4.8119e-05 - 66s/epoch - 26ms/step
Epoch 39/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3692 - MAE: nan - RC@5: 0.4341 - RC@10: 0.5244 - val_loss: 0.5944 - val_NDCG@10: 0.3769 - val_MAE: 0.0435 - val_RC@5: 0.4377 - val_RC@10: 0.5174 - lr: 4.8070e-05 - 66s/epoch - 26ms/step
Epoch 40/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3710 - MAE: nan - RC@5: 0.4366 - RC@10: 0.5268 - val_loss: 0.5948 - val_NDCG@10: 0.3785 - val_MAE: 0.0419 - val_RC@5: 0.4392 - val_RC@10: 0.5180 - lr: 4.8020e-05 - 66s/epoch - 26ms/step
Epoch 41/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3715 - MAE: nan - RC@5: 0.4365 - RC@10: 0.5280 - val_loss: 0.5958 - val_NDCG@10: 0.3784 - val_MAE: 0.0393 - val_RC@5: 0.4384 - val_RC@10: 0.5185 - lr: 4.7971e-05 - 66s/epoch - 26ms/step
Epoch 42/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3732 - MAE: nan - RC@5: 0.4390 - RC@10: 0.5299 - val_loss: 0.5945 - val_NDCG@10: 0.3779 - val_MAE: 0.0442 - val_RC@5: 0.4386 - val_RC@10: 0.5188 - lr: 4.7921e-05 - 66s/epoch - 26ms/step
Epoch 43/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3749 - MAE: nan - RC@5: 0.4410 - RC@10: 0.5325 - val_loss: 0.5956 - val_NDCG@10: 0.3780 - val_MAE: 0.0412 - val_RC@5: 0.4396 - val_RC@10: 0.5185 - lr: 4.7872e-05 - 66s/epoch - 26ms/step
Epoch 44/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3757 - MAE: nan - RC@5: 0.4424 - RC@10: 0.5331 - val_loss: 0.5964 - val_NDCG@10: 0.3789 - val_MAE: 0.0386 - val_RC@5: 0.4394 - val_RC@10: 0.5190 - lr: 4.7822e-05 - 66s/epoch - 26ms/step
Epoch 45/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3767 - MAE: nan - RC@5: 0.4433 - RC@10: 0.5349 - val_loss: 0.5955 - val_NDCG@10: 0.3779 - val_MAE: 0.0427 - val_RC@5: 0.4390 - val_RC@10: 0.5182 - lr: 4.7773e-05 - 66s/epoch - 26ms/step
Epoch 46/1000
2531/2531 - 66s - loss: nan - NDCG@10: 0.3782 - MAE: nan - RC@5: 0.4452 - RC@10: 0.5366 - val_loss: 0.5966 - val_NDCG@10: 0.3796 - val_MAE: 0.0381 - val_RC@5: 0.4397 - val_RC@10: 0.5192 - lr: 4.7723e-05 - 66s/epoch - 26ms/step
Epoch 46: early stopping
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
  warnings.warn(
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.
  warnings.warn(
(3415, 2)
