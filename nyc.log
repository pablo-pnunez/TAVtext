2023-05-29 22:41:13.498829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "ATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 200, 128)     1544704     ['input_1[0][0]']                
                                                                                                  
 embedding_1 (Embedding)        (None, 1985, 128)    254080      ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 200, 128)     0           ['embedding[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 128)    0           ['embedding_1[0][0]']            
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 200, 128)     0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 1985, 128)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 200, 1985)    0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 200, 1985)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['lambda_1[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,798,784
Trainable params: 1,798,784
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-05-29 22:41:19.629125: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
2023-05-29 22:41:22.091700: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
Epoch 1/1000
1292/1292 - 44s - loss: 1.3666 - NDCG@10: 0.0543 - MAE: 0.4735 - RC@5: 0.0653 - RC@10: 0.0932 - val_loss: 1.2462 - val_NDCG@10: 0.0948 - val_MAE: 0.2191 - val_RC@5: 0.1004 - val_RC@10: 0.1327 - lr: 9.9901e-05 - e_time: 44.1116 - 44s/epoch - 34ms/step
Epoch 2/1000
1292/1292 - 42s - loss: 1.2350 - NDCG@10: 0.1053 - MAE: 0.2231 - RC@5: 0.1222 - RC@10: 0.1549 - val_loss: 1.2036 - val_NDCG@10: 0.1625 - val_MAE: 0.2237 - val_RC@5: 0.1824 - val_RC@10: 0.2161 - lr: 9.9802e-05 - e_time: 41.6597 - 42s/epoch - 32ms/step
Epoch 3/1000
1292/1292 - 42s - loss: 1.1819 - NDCG@10: 0.1727 - MAE: 0.2419 - RC@5: 0.1999 - RC@10: 0.2428 - val_loss: 1.1405 - val_NDCG@10: 0.2248 - val_MAE: 0.2440 - val_RC@5: 0.2562 - val_RC@10: 0.3038 - lr: 9.9703e-05 - e_time: 41.6597 - 42s/epoch - 32ms/step
Epoch 4/1000
1292/1292 - 42s - loss: 1.1201 - NDCG@10: 0.2263 - MAE: 0.2528 - RC@5: 0.2624 - RC@10: 0.3173 - val_loss: 1.0815 - val_NDCG@10: 0.2731 - val_MAE: 0.2454 - val_RC@5: 0.3134 - val_RC@10: 0.3754 - lr: 9.9604e-05 - e_time: 41.7405 - 42s/epoch - 32ms/step
Epoch 5/1000
1292/1292 - 42s - loss: 1.0685 - NDCG@10: 0.2645 - MAE: 0.2528 - RC@5: 0.3083 - RC@10: 0.3732 - val_loss: 1.0347 - val_NDCG@10: 0.3090 - val_MAE: 0.2445 - val_RC@5: 0.3552 - val_RC@10: 0.4240 - lr: 9.9505e-05 - e_time: 41.7278 - 42s/epoch - 32ms/step
Epoch 6/1000
1292/1292 - 42s - loss: 1.0282 - NDCG@10: 0.2919 - MAE: 0.2515 - RC@5: 0.3406 - RC@10: 0.4122 - val_loss: 0.9989 - val_NDCG@10: 0.3342 - val_MAE: 0.2400 - val_RC@5: 0.3840 - val_RC@10: 0.4584 - lr: 9.9406e-05 - e_time: 41.7224 - 42s/epoch - 32ms/step
Epoch 7/1000
1292/1292 - 42s - loss: 0.9975 - NDCG@10: 0.3132 - MAE: 0.2479 - RC@5: 0.3654 - RC@10: 0.4422 - val_loss: 0.9716 - val_NDCG@10: 0.3546 - val_MAE: 0.2366 - val_RC@5: 0.4079 - val_RC@10: 0.4856 - lr: 9.9307e-05 - e_time: 41.7487 - 42s/epoch - 33ms/step
Epoch 8/1000
1292/1292 - 42s - loss: 0.9735 - NDCG@10: 0.3296 - MAE: 0.2440 - RC@5: 0.3850 - RC@10: 0.4651 - val_loss: 0.9501 - val_NDCG@10: 0.3711 - val_MAE: 0.2327 - val_RC@5: 0.4271 - val_RC@10: 0.5065 - lr: 9.9208e-05 - e_time: 41.7387 - 42s/epoch - 33ms/step
Epoch 9/1000
1292/1292 - 42s - loss: 0.9541 - NDCG@10: 0.3440 - MAE: 0.2402 - RC@5: 0.4022 - RC@10: 0.4842 - val_loss: 0.9329 - val_NDCG@10: 0.3848 - val_MAE: 0.2278 - val_RC@5: 0.4424 - val_RC@10: 0.5242 - lr: 9.9109e-05 - e_time: 41.7299 - 42s/epoch - 32ms/step
Epoch 10/1000
1292/1292 - 42s - loss: 0.9385 - NDCG@10: 0.3560 - MAE: 0.2364 - RC@5: 0.4161 - RC@10: 0.5001 - val_loss: 0.9186 - val_NDCG@10: 0.3969 - val_MAE: 0.2235 - val_RC@5: 0.4566 - val_RC@10: 0.5390 - lr: 9.9010e-05 - e_time: 41.7411 - 42s/epoch - 32ms/step
Epoch 11/1000
1292/1292 - 42s - loss: 0.9246 - NDCG@10: 0.3664 - MAE: 0.2330 - RC@5: 0.4283 - RC@10: 0.5136 - val_loss: 0.9061 - val_NDCG@10: 0.4072 - val_MAE: 0.2225 - val_RC@5: 0.4681 - val_RC@10: 0.5519 - lr: 9.8911e-05 - e_time: 41.7062 - 42s/epoch - 32ms/step
Epoch 12/1000
1292/1292 - 42s - loss: 0.9131 - NDCG@10: 0.3755 - MAE: 0.2299 - RC@5: 0.4388 - RC@10: 0.5250 - val_loss: 0.8957 - val_NDCG@10: 0.4160 - val_MAE: 0.2178 - val_RC@5: 0.4779 - val_RC@10: 0.5627 - lr: 9.8812e-05 - e_time: 41.7381 - 42s/epoch - 32ms/step
Epoch 13/1000
1292/1292 - 42s - loss: 0.9028 - NDCG@10: 0.3833 - MAE: 0.2274 - RC@5: 0.4482 - RC@10: 0.5346 - val_loss: 0.8860 - val_NDCG@10: 0.4254 - val_MAE: 0.2183 - val_RC@5: 0.4890 - val_RC@10: 0.5736 - lr: 9.8713e-05 - e_time: 41.7440 - 42s/epoch - 32ms/step
Epoch 14/1000
1292/1292 - 42s - loss: 0.8934 - NDCG@10: 0.3913 - MAE: 0.2249 - RC@5: 0.4571 - RC@10: 0.5452 - val_loss: 0.8780 - val_NDCG@10: 0.4323 - val_MAE: 0.2122 - val_RC@5: 0.4978 - val_RC@10: 0.5818 - lr: 9.8614e-05 - e_time: 41.7680 - 42s/epoch - 33ms/step
Epoch 15/1000
1292/1292 - 42s - loss: 0.8850 - NDCG@10: 0.3973 - MAE: 0.2227 - RC@5: 0.4647 - RC@10: 0.5528 - val_loss: 0.8707 - val_NDCG@10: 0.4391 - val_MAE: 0.2108 - val_RC@5: 0.5046 - val_RC@10: 0.5899 - lr: 9.8515e-05 - e_time: 41.7338 - 42s/epoch - 32ms/step
Epoch 16/1000
1292/1292 - 42s - loss: 0.8777 - NDCG@10: 0.4036 - MAE: 0.2202 - RC@5: 0.4719 - RC@10: 0.5605 - val_loss: 0.8642 - val_NDCG@10: 0.4448 - val_MAE: 0.2098 - val_RC@5: 0.5116 - val_RC@10: 0.5969 - lr: 9.8416e-05 - e_time: 41.7312 - 42s/epoch - 32ms/step
Epoch 17/1000
1292/1292 - 42s - loss: 0.8709 - NDCG@10: 0.4085 - MAE: 0.2181 - RC@5: 0.4775 - RC@10: 0.5670 - val_loss: 0.8586 - val_NDCG@10: 0.4509 - val_MAE: 0.2071 - val_RC@5: 0.5175 - val_RC@10: 0.6041 - lr: 9.8317e-05 - e_time: 41.6861 - 42s/epoch - 32ms/step
Epoch 18/1000
1292/1292 - 42s - loss: 0.8650 - NDCG@10: 0.4146 - MAE: 0.2160 - RC@5: 0.4842 - RC@10: 0.5738 - val_loss: 0.8534 - val_NDCG@10: 0.4563 - val_MAE: 0.2065 - val_RC@5: 0.5238 - val_RC@10: 0.6094 - lr: 9.8218e-05 - e_time: 41.6343 - 42s/epoch - 32ms/step
Epoch 19/1000
1292/1292 - 42s - loss: 0.8595 - NDCG@10: 0.4186 - MAE: 0.2139 - RC@5: 0.4891 - RC@10: 0.5790 - val_loss: 0.8489 - val_NDCG@10: 0.4603 - val_MAE: 0.2052 - val_RC@5: 0.5287 - val_RC@10: 0.6138 - lr: 9.8119e-05 - e_time: 41.6168 - 42s/epoch - 32ms/step
Epoch 20/1000
1292/1292 - 42s - loss: 0.8541 - NDCG@10: 0.4232 - MAE: 0.2122 - RC@5: 0.4940 - RC@10: 0.5839 - val_loss: 0.8450 - val_NDCG@10: 0.4645 - val_MAE: 0.2010 - val_RC@5: 0.5326 - val_RC@10: 0.6188 - lr: 9.8020e-05 - e_time: 41.6581 - 42s/epoch - 32ms/step
Epoch 21/1000
1292/1292 - 42s - loss: 0.8496 - NDCG@10: 0.4270 - MAE: 0.2104 - RC@5: 0.4989 - RC@10: 0.5884 - val_loss: 0.8414 - val_NDCG@10: 0.4684 - val_MAE: 0.1985 - val_RC@5: 0.5374 - val_RC@10: 0.6229 - lr: 9.7921e-05 - e_time: 41.6698 - 42s/epoch - 32ms/step
Epoch 22/1000
1292/1292 - 42s - loss: 0.8451 - NDCG@10: 0.4310 - MAE: 0.2090 - RC@5: 0.5034 - RC@10: 0.5934 - val_loss: 0.8378 - val_NDCG@10: 0.4719 - val_MAE: 0.1986 - val_RC@5: 0.5416 - val_RC@10: 0.6270 - lr: 9.7822e-05 - e_time: 41.6141 - 42s/epoch - 32ms/step
Epoch 23/1000
1292/1292 - 42s - loss: 0.8411 - NDCG@10: 0.4340 - MAE: 0.2072 - RC@5: 0.5071 - RC@10: 0.5972 - val_loss: 0.8348 - val_NDCG@10: 0.4750 - val_MAE: 0.1954 - val_RC@5: 0.5450 - val_RC@10: 0.6301 - lr: 9.7723e-05 - e_time: 41.6438 - 42s/epoch - 32ms/step
Epoch 24/1000
1292/1292 - 42s - loss: 0.8375 - NDCG@10: 0.4371 - MAE: 0.2059 - RC@5: 0.5100 - RC@10: 0.6003 - val_loss: 0.8318 - val_NDCG@10: 0.4785 - val_MAE: 0.1947 - val_RC@5: 0.5487 - val_RC@10: 0.6340 - lr: 9.7624e-05 - e_time: 41.6720 - 42s/epoch - 32ms/step
Epoch 25/1000
1292/1292 - 42s - loss: 0.8338 - NDCG@10: 0.4404 - MAE: 0.2045 - RC@5: 0.5137 - RC@10: 0.6043 - val_loss: 0.8292 - val_NDCG@10: 0.4804 - val_MAE: 0.1930 - val_RC@5: 0.5506 - val_RC@10: 0.6351 - lr: 9.7525e-05 - e_time: 41.6296 - 42s/epoch - 32ms/step
Epoch 26/1000
1292/1292 - 42s - loss: 0.8306 - NDCG@10: 0.4432 - MAE: 0.2030 - RC@5: 0.5178 - RC@10: 0.6074 - val_loss: 0.8267 - val_NDCG@10: 0.4840 - val_MAE: 0.1923 - val_RC@5: 0.5539 - val_RC@10: 0.6385 - lr: 9.7426e-05 - e_time: 41.6812 - 42s/epoch - 32ms/step
Epoch 27/1000
1292/1292 - 42s - loss: 0.8276 - NDCG@10: 0.4457 - MAE: 0.2019 - RC@5: 0.5199 - RC@10: 0.6103 - val_loss: 0.8247 - val_NDCG@10: 0.4858 - val_MAE: 0.1885 - val_RC@5: 0.5564 - val_RC@10: 0.6406 - lr: 9.7327e-05 - e_time: 41.6750 - 42s/epoch - 32ms/step
Epoch 28/1000
1292/1292 - 42s - loss: 0.8242 - NDCG@10: 0.4489 - MAE: 0.2007 - RC@5: 0.5235 - RC@10: 0.6144 - val_loss: 0.8221 - val_NDCG@10: 0.4895 - val_MAE: 0.1902 - val_RC@5: 0.5607 - val_RC@10: 0.6441 - lr: 9.7228e-05 - e_time: 41.6546 - 42s/epoch - 32ms/step
Epoch 29/1000
1292/1292 - 42s - loss: 0.8215 - NDCG@10: 0.4507 - MAE: 0.1997 - RC@5: 0.5258 - RC@10: 0.6160 - val_loss: 0.8202 - val_NDCG@10: 0.4910 - val_MAE: 0.1876 - val_RC@5: 0.5622 - val_RC@10: 0.6454 - lr: 9.7129e-05 - e_time: 41.6845 - 42s/epoch - 32ms/step
Epoch 30/1000
1292/1292 - 42s - loss: 0.8188 - NDCG@10: 0.4531 - MAE: 0.1988 - RC@5: 0.5285 - RC@10: 0.6187 - val_loss: 0.8184 - val_NDCG@10: 0.4928 - val_MAE: 0.1866 - val_RC@5: 0.5641 - val_RC@10: 0.6473 - lr: 9.7030e-05 - e_time: 41.7280 - 42s/epoch - 32ms/step
Epoch 31/1000
1292/1292 - 42s - loss: 0.8165 - NDCG@10: 0.4549 - MAE: 0.1981 - RC@5: 0.5302 - RC@10: 0.6207 - val_loss: 0.8166 - val_NDCG@10: 0.4938 - val_MAE: 0.1854 - val_RC@5: 0.5651 - val_RC@10: 0.6486 - lr: 9.6931e-05 - e_time: 41.6867 - 42s/epoch - 32ms/step
Epoch 32/1000
1292/1292 - 42s - loss: 0.8139 - NDCG@10: 0.4572 - MAE: 0.1970 - RC@5: 0.5328 - RC@10: 0.6230 - val_loss: 0.8149 - val_NDCG@10: 0.4965 - val_MAE: 0.1846 - val_RC@5: 0.5682 - val_RC@10: 0.6514 - lr: 9.6832e-05 - e_time: 41.6777 - 42s/epoch - 32ms/step
Epoch 33/1000
1292/1292 - 42s - loss: 0.8114 - NDCG@10: 0.4592 - MAE: 0.1963 - RC@5: 0.5357 - RC@10: 0.6255 - val_loss: 0.8132 - val_NDCG@10: 0.4987 - val_MAE: 0.1836 - val_RC@5: 0.5708 - val_RC@10: 0.6533 - lr: 9.6733e-05 - e_time: 41.6517 - 42s/epoch - 32ms/step
Epoch 34/1000
1292/1292 - 42s - loss: 0.8091 - NDCG@10: 0.4615 - MAE: 0.1954 - RC@5: 0.5376 - RC@10: 0.6280 - val_loss: 0.8117 - val_NDCG@10: 0.5002 - val_MAE: 0.1827 - val_RC@5: 0.5725 - val_RC@10: 0.6549 - lr: 9.6634e-05 - e_time: 41.6672 - 42s/epoch - 32ms/step
Epoch 35/1000
1292/1292 - 42s - loss: 0.8070 - NDCG@10: 0.4631 - MAE: 0.1947 - RC@5: 0.5395 - RC@10: 0.6298 - val_loss: 0.8106 - val_NDCG@10: 0.5014 - val_MAE: 0.1805 - val_RC@5: 0.5733 - val_RC@10: 0.6568 - lr: 9.6535e-05 - e_time: 41.6771 - 42s/epoch - 32ms/step
Epoch 36/1000
1292/1292 - 42s - loss: 0.8049 - NDCG@10: 0.4651 - MAE: 0.1937 - RC@5: 0.5424 - RC@10: 0.6321 - val_loss: 0.8093 - val_NDCG@10: 0.5022 - val_MAE: 0.1791 - val_RC@5: 0.5746 - val_RC@10: 0.6571 - lr: 9.6436e-05 - e_time: 41.6884 - 42s/epoch - 32ms/step
Epoch 37/1000
1292/1292 - 42s - loss: 0.8029 - NDCG@10: 0.4665 - MAE: 0.1930 - RC@5: 0.5433 - RC@10: 0.6336 - val_loss: 0.8083 - val_NDCG@10: 0.5036 - val_MAE: 0.1779 - val_RC@5: 0.5758 - val_RC@10: 0.6588 - lr: 9.6337e-05 - e_time: 41.6608 - 42s/epoch - 32ms/step
Epoch 38/1000
1292/1292 - 42s - loss: 0.8011 - NDCG@10: 0.4678 - MAE: 0.1922 - RC@5: 0.5451 - RC@10: 0.6351 - val_loss: 0.8070 - val_NDCG@10: 0.5056 - val_MAE: 0.1773 - val_RC@5: 0.5770 - val_RC@10: 0.6598 - lr: 9.6238e-05 - e_time: 41.6450 - 42s/epoch - 32ms/step
Epoch 39/1000
1292/1292 - 42s - loss: 0.7992 - NDCG@10: 0.4697 - MAE: 0.1915 - RC@5: 0.5473 - RC@10: 0.6372 - val_loss: 0.8060 - val_NDCG@10: 0.5067 - val_MAE: 0.1765 - val_RC@5: 0.5783 - val_RC@10: 0.6609 - lr: 9.6139e-05 - e_time: 41.6270 - 42s/epoch - 32ms/step
Epoch 40/1000
1292/1292 - 42s - loss: 0.7973 - NDCG@10: 0.4706 - MAE: 0.1907 - RC@5: 0.5483 - RC@10: 0.6386 - val_loss: 0.8049 - val_NDCG@10: 0.5093 - val_MAE: 0.1763 - val_RC@5: 0.5805 - val_RC@10: 0.6634 - lr: 9.6040e-05 - e_time: 41.6834 - 42s/epoch - 32ms/step
Epoch 41/1000
1292/1292 - 42s - loss: 0.7959 - NDCG@10: 0.4721 - MAE: 0.1903 - RC@5: 0.5499 - RC@10: 0.6400 - val_loss: 0.8039 - val_NDCG@10: 0.5093 - val_MAE: 0.1760 - val_RC@5: 0.5816 - val_RC@10: 0.6638 - lr: 9.5941e-05 - e_time: 41.7058 - 42s/epoch - 32ms/step
Epoch 42/1000
1292/1292 - 42s - loss: 0.7942 - NDCG@10: 0.4736 - MAE: 0.1894 - RC@5: 0.5517 - RC@10: 0.6415 - val_loss: 0.8028 - val_NDCG@10: 0.5105 - val_MAE: 0.1757 - val_RC@5: 0.5819 - val_RC@10: 0.6644 - lr: 9.5842e-05 - e_time: 41.7996 - 42s/epoch - 33ms/step
Epoch 43/1000
1292/1292 - 42s - loss: 0.7926 - NDCG@10: 0.4748 - MAE: 0.1890 - RC@5: 0.5531 - RC@10: 0.6430 - val_loss: 0.8022 - val_NDCG@10: 0.5100 - val_MAE: 0.1739 - val_RC@5: 0.5823 - val_RC@10: 0.6643 - lr: 9.5743e-05 - e_time: 41.8026 - 42s/epoch - 33ms/step
Epoch 44/1000
1292/1292 - 42s - loss: 0.7911 - NDCG@10: 0.4764 - MAE: 0.1884 - RC@5: 0.5547 - RC@10: 0.6441 - val_loss: 0.8014 - val_NDCG@10: 0.5128 - val_MAE: 0.1736 - val_RC@5: 0.5845 - val_RC@10: 0.6664 - lr: 9.5644e-05 - e_time: 41.7029 - 42s/epoch - 32ms/step
Epoch 45/1000
1292/1292 - 42s - loss: 0.7896 - NDCG@10: 0.4774 - MAE: 0.1877 - RC@5: 0.5554 - RC@10: 0.6451 - val_loss: 0.8007 - val_NDCG@10: 0.5134 - val_MAE: 0.1723 - val_RC@5: 0.5852 - val_RC@10: 0.6670 - lr: 9.5545e-05 - e_time: 41.7357 - 42s/epoch - 32ms/step
Epoch 46/1000
1292/1292 - 42s - loss: 0.7882 - NDCG@10: 0.4785 - MAE: 0.1873 - RC@5: 0.5573 - RC@10: 0.6469 - val_loss: 0.7999 - val_NDCG@10: 0.5140 - val_MAE: 0.1720 - val_RC@5: 0.5863 - val_RC@10: 0.6675 - lr: 9.5446e-05 - e_time: 41.7347 - 42s/epoch - 32ms/step
Epoch 47/1000
1292/1292 - 42s - loss: 0.7869 - NDCG@10: 0.4796 - MAE: 0.1868 - RC@5: 0.5585 - RC@10: 0.6481 - val_loss: 0.7995 - val_NDCG@10: 0.5146 - val_MAE: 0.1703 - val_RC@5: 0.5871 - val_RC@10: 0.6678 - lr: 9.5347e-05 - e_time: 41.6960 - 42s/epoch - 32ms/step
Epoch 48/1000
1292/1292 - 42s - loss: 0.7855 - NDCG@10: 0.4804 - MAE: 0.1863 - RC@5: 0.5600 - RC@10: 0.6494 - val_loss: 0.7988 - val_NDCG@10: 0.5155 - val_MAE: 0.1695 - val_RC@5: 0.5876 - val_RC@10: 0.6685 - lr: 9.5248e-05 - e_time: 41.6746 - 42s/epoch - 32ms/step
Epoch 49/1000
1292/1292 - 42s - loss: 0.7842 - NDCG@10: 0.4813 - MAE: 0.1856 - RC@5: 0.5602 - RC@10: 0.6501 - val_loss: 0.7980 - val_NDCG@10: 0.5163 - val_MAE: 0.1698 - val_RC@5: 0.5883 - val_RC@10: 0.6690 - lr: 9.5149e-05 - e_time: 41.6666 - 42s/epoch - 32ms/step
Epoch 50/1000
1292/1292 - 42s - loss: 0.7831 - NDCG@10: 0.4827 - MAE: 0.1852 - RC@5: 0.5625 - RC@10: 0.6513 - val_loss: 0.7975 - val_NDCG@10: 0.5167 - val_MAE: 0.1690 - val_RC@5: 0.5891 - val_RC@10: 0.6695 - lr: 9.5050e-05 - e_time: 41.6996 - 42s/epoch - 32ms/step
Epoch 51/1000
1292/1292 - 42s - loss: 0.7818 - NDCG@10: 0.4838 - MAE: 0.1847 - RC@5: 0.5629 - RC@10: 0.6519 - val_loss: 0.7970 - val_NDCG@10: 0.5182 - val_MAE: 0.1682 - val_RC@5: 0.5900 - val_RC@10: 0.6707 - lr: 9.4951e-05 - e_time: 41.6906 - 42s/epoch - 32ms/step
Epoch 52/1000
1292/1292 - 42s - loss: 0.7811 - NDCG@10: 0.4843 - MAE: 0.1843 - RC@5: 0.5635 - RC@10: 0.6528 - val_loss: 0.7962 - val_NDCG@10: 0.5187 - val_MAE: 0.1682 - val_RC@5: 0.5907 - val_RC@10: 0.6715 - lr: 9.4852e-05 - e_time: 41.6679 - 42s/epoch - 32ms/step
Epoch 53/1000
1292/1292 - 42s - loss: 0.7791 - NDCG@10: 0.4853 - MAE: 0.1839 - RC@5: 0.5653 - RC@10: 0.6549 - val_loss: 0.7955 - val_NDCG@10: 0.5193 - val_MAE: 0.1675 - val_RC@5: 0.5913 - val_RC@10: 0.6719 - lr: 9.4753e-05 - e_time: 41.7577 - 42s/epoch - 32ms/step
Epoch 54/1000
1292/1292 - 42s - loss: 0.7780 - NDCG@10: 0.4866 - MAE: 0.1835 - RC@5: 0.5661 - RC@10: 0.6558 - val_loss: 0.7957 - val_NDCG@10: 0.5195 - val_MAE: 0.1650 - val_RC@5: 0.5916 - val_RC@10: 0.6722 - lr: 9.4654e-05 - e_time: 41.7637 - 42s/epoch - 32ms/step
Epoch 55/1000
1292/1292 - 42s - loss: 0.7768 - NDCG@10: 0.4875 - MAE: 0.1829 - RC@5: 0.5676 - RC@10: 0.6569 - val_loss: 0.7952 - val_NDCG@10: 0.5196 - val_MAE: 0.1646 - val_RC@5: 0.5916 - val_RC@10: 0.6720 - lr: 9.4555e-05 - e_time: 41.7682 - 42s/epoch - 33ms/step
Epoch 56/1000
1292/1292 - 42s - loss: 0.7762 - NDCG@10: 0.4876 - MAE: 0.1828 - RC@5: 0.5673 - RC@10: 0.6572 - val_loss: 0.7946 - val_NDCG@10: 0.5206 - val_MAE: 0.1646 - val_RC@5: 0.5927 - val_RC@10: 0.6729 - lr: 9.4456e-05 - e_time: 41.8404 - 42s/epoch - 33ms/step
Epoch 57/1000
1292/1292 - 42s - loss: 0.7752 - NDCG@10: 0.4885 - MAE: 0.1822 - RC@5: 0.5688 - RC@10: 0.6577 - val_loss: 0.7939 - val_NDCG@10: 0.5209 - val_MAE: 0.1653 - val_RC@5: 0.5936 - val_RC@10: 0.6735 - lr: 9.4357e-05 - e_time: 41.8446 - 42s/epoch - 33ms/step
Epoch 58/1000
1292/1292 - 42s - loss: 0.7740 - NDCG@10: 0.4895 - MAE: 0.1819 - RC@5: 0.5694 - RC@10: 0.6587 - val_loss: 0.7937 - val_NDCG@10: 0.5210 - val_MAE: 0.1639 - val_RC@5: 0.5933 - val_RC@10: 0.6729 - lr: 9.4258e-05 - e_time: 41.8359 - 42s/epoch - 33ms/step
Epoch 59/1000
1292/1292 - 42s - loss: 0.7731 - NDCG@10: 0.4900 - MAE: 0.1816 - RC@5: 0.5702 - RC@10: 0.6592 - val_loss: 0.7932 - val_NDCG@10: 0.5223 - val_MAE: 0.1637 - val_RC@5: 0.5945 - val_RC@10: 0.6744 - lr: 9.4159e-05 - e_time: 41.8354 - 42s/epoch - 33ms/step
Epoch 60/1000
1292/1292 - 42s - loss: 0.7720 - NDCG@10: 0.4908 - MAE: 0.1811 - RC@5: 0.5712 - RC@10: 0.6603 - val_loss: 0.7928 - val_NDCG@10: 0.5230 - val_MAE: 0.1636 - val_RC@5: 0.5948 - val_RC@10: 0.6750 - lr: 9.4060e-05 - e_time: 41.8163 - 42s/epoch - 33ms/step
Epoch 61/1000
1292/1292 - 42s - loss: 0.7712 - NDCG@10: 0.4915 - MAE: 0.1808 - RC@5: 0.5719 - RC@10: 0.6610 - val_loss: 0.7925 - val_NDCG@10: 0.5232 - val_MAE: 0.1630 - val_RC@5: 0.5948 - val_RC@10: 0.6750 - lr: 9.3961e-05 - e_time: 41.8802 - 42s/epoch - 33ms/step
Epoch 62/1000
1292/1292 - 42s - loss: 0.7703 - NDCG@10: 0.4919 - MAE: 0.1806 - RC@5: 0.5722 - RC@10: 0.6617 - val_loss: 0.7927 - val_NDCG@10: 0.5237 - val_MAE: 0.1616 - val_RC@5: 0.5957 - val_RC@10: 0.6757 - lr: 9.3862e-05 - e_time: 41.7895 - 42s/epoch - 32ms/step
Epoch 63/1000
1292/1292 - 42s - loss: 0.7690 - NDCG@10: 0.4926 - MAE: 0.1800 - RC@5: 0.5729 - RC@10: 0.6621 - val_loss: 0.7923 - val_NDCG@10: 0.5239 - val_MAE: 0.1608 - val_RC@5: 0.5949 - val_RC@10: 0.6758 - lr: 9.3763e-05 - e_time: 41.8027 - 42s/epoch - 33ms/step
Epoch 64/1000
1292/1292 - 42s - loss: 0.7688 - NDCG@10: 0.4930 - MAE: 0.1798 - RC@5: 0.5735 - RC@10: 0.6630 - val_loss: 0.7917 - val_NDCG@10: 0.5242 - val_MAE: 0.1614 - val_RC@5: 0.5960 - val_RC@10: 0.6769 - lr: 9.3664e-05 - e_time: 41.7496 - 42s/epoch - 32ms/step
Epoch 65/1000
1292/1292 - 42s - loss: 0.7675 - NDCG@10: 0.4943 - MAE: 0.1794 - RC@5: 0.5748 - RC@10: 0.6636 - val_loss: 0.7913 - val_NDCG@10: 0.5248 - val_MAE: 0.1616 - val_RC@5: 0.5965 - val_RC@10: 0.6767 - lr: 9.3565e-05 - e_time: 41.7893 - 42s/epoch - 33ms/step
Epoch 66/1000
1292/1292 - 42s - loss: 0.7671 - NDCG@10: 0.4942 - MAE: 0.1793 - RC@5: 0.5754 - RC@10: 0.6644 - val_loss: 0.7910 - val_NDCG@10: 0.5258 - val_MAE: 0.1616 - val_RC@5: 0.5978 - val_RC@10: 0.6776 - lr: 9.3466e-05 - e_time: 41.7671 - 42s/epoch - 33ms/step
Epoch 67/1000
1292/1292 - 42s - loss: 0.7660 - NDCG@10: 0.4954 - MAE: 0.1789 - RC@5: 0.5764 - RC@10: 0.6658 - val_loss: 0.7905 - val_NDCG@10: 0.5256 - val_MAE: 0.1616 - val_RC@5: 0.5975 - val_RC@10: 0.6776 - lr: 9.3367e-05 - e_time: 41.7797 - 42s/epoch - 33ms/step
Epoch 68/1000
1292/1292 - 42s - loss: 0.7652 - NDCG@10: 0.4957 - MAE: 0.1786 - RC@5: 0.5767 - RC@10: 0.6655 - val_loss: 0.7909 - val_NDCG@10: 0.5256 - val_MAE: 0.1591 - val_RC@5: 0.5972 - val_RC@10: 0.6778 - lr: 9.3268e-05 - e_time: 41.7506 - 42s/epoch - 32ms/step
Epoch 69/1000
1292/1292 - 42s - loss: 0.7646 - NDCG@10: 0.4960 - MAE: 0.1783 - RC@5: 0.5770 - RC@10: 0.6664 - val_loss: 0.7904 - val_NDCG@10: 0.5260 - val_MAE: 0.1597 - val_RC@5: 0.5975 - val_RC@10: 0.6780 - lr: 9.3169e-05 - e_time: 41.7789 - 42s/epoch - 33ms/step
Epoch 70/1000
1292/1292 - 42s - loss: 0.7639 - NDCG@10: 0.4968 - MAE: 0.1779 - RC@5: 0.5775 - RC@10: 0.6672 - val_loss: 0.7899 - val_NDCG@10: 0.5260 - val_MAE: 0.1605 - val_RC@5: 0.5976 - val_RC@10: 0.6783 - lr: 9.3070e-05 - e_time: 41.7926 - 42s/epoch - 33ms/step
Epoch 71/1000
1292/1292 - 42s - loss: 0.7632 - NDCG@10: 0.4971 - MAE: 0.1777 - RC@5: 0.5780 - RC@10: 0.6676 - val_loss: 0.7898 - val_NDCG@10: 0.5270 - val_MAE: 0.1597 - val_RC@5: 0.5983 - val_RC@10: 0.6786 - lr: 9.2971e-05 - e_time: 41.6833 - 42s/epoch - 32ms/step
Epoch 72/1000
1292/1292 - 42s - loss: 0.7622 - NDCG@10: 0.4976 - MAE: 0.1773 - RC@5: 0.5788 - RC@10: 0.6682 - val_loss: 0.7899 - val_NDCG@10: 0.5270 - val_MAE: 0.1584 - val_RC@5: 0.5983 - val_RC@10: 0.6786 - lr: 9.2872e-05 - e_time: 41.7212 - 42s/epoch - 32ms/step
Epoch 73/1000
1292/1292 - 42s - loss: 0.7616 - NDCG@10: 0.4985 - MAE: 0.1769 - RC@5: 0.5800 - RC@10: 0.6684 - val_loss: 0.7893 - val_NDCG@10: 0.5278 - val_MAE: 0.1597 - val_RC@5: 0.5992 - val_RC@10: 0.6791 - lr: 9.2773e-05 - e_time: 41.7344 - 42s/epoch - 32ms/step
Epoch 74/1000
1292/1292 - 42s - loss: 0.7609 - NDCG@10: 0.4987 - MAE: 0.1768 - RC@5: 0.5807 - RC@10: 0.6688 - val_loss: 0.7894 - val_NDCG@10: 0.5277 - val_MAE: 0.1586 - val_RC@5: 0.5990 - val_RC@10: 0.6795 - lr: 9.2674e-05 - e_time: 41.8589 - 42s/epoch - 32ms/step
Epoch 75/1000
1292/1292 - 42s - loss: 0.7605 - NDCG@10: 0.4990 - MAE: 0.1766 - RC@5: 0.5807 - RC@10: 0.6697 - val_loss: 0.7891 - val_NDCG@10: 0.5279 - val_MAE: 0.1587 - val_RC@5: 0.5993 - val_RC@10: 0.6796 - lr: 9.2575e-05 - e_time: 41.8247 - 42s/epoch - 33ms/step
Epoch 76/1000
1292/1292 - 42s - loss: 0.7598 - NDCG@10: 0.4997 - MAE: 0.1763 - RC@5: 0.5811 - RC@10: 0.6699 - val_loss: 0.7898 - val_NDCG@10: 0.5275 - val_MAE: 0.1559 - val_RC@5: 0.5996 - val_RC@10: 0.6790 - lr: 9.2476e-05 - e_time: 41.8069 - 42s/epoch - 32ms/step
Epoch 77/1000
1292/1292 - 42s - loss: 0.7591 - NDCG@10: 0.5001 - MAE: 0.1759 - RC@5: 0.5816 - RC@10: 0.6706 - val_loss: 0.7892 - val_NDCG@10: 0.5285 - val_MAE: 0.1568 - val_RC@5: 0.6009 - val_RC@10: 0.6796 - lr: 9.2377e-05 - e_time: 41.7448 - 42s/epoch - 32ms/step
Epoch 78/1000
1292/1292 - 42s - loss: 0.7586 - NDCG@10: 0.5005 - MAE: 0.1758 - RC@5: 0.5826 - RC@10: 0.6713 - val_loss: 0.7888 - val_NDCG@10: 0.5282 - val_MAE: 0.1567 - val_RC@5: 0.6003 - val_RC@10: 0.6791 - lr: 9.2278e-05 - e_time: 41.6910 - 42s/epoch - 32ms/step
Epoch 79/1000
1292/1292 - 42s - loss: 0.7578 - NDCG@10: 0.5009 - MAE: 0.1755 - RC@5: 0.5827 - RC@10: 0.6714 - val_loss: 0.7891 - val_NDCG@10: 0.5283 - val_MAE: 0.1553 - val_RC@5: 0.5999 - val_RC@10: 0.6791 - lr: 9.2179e-05 - e_time: 41.6762 - 42s/epoch - 32ms/step
Epoch 80/1000
1292/1292 - 42s - loss: 0.7575 - NDCG@10: 0.5009 - MAE: 0.1755 - RC@5: 0.5825 - RC@10: 0.6714 - val_loss: 0.7881 - val_NDCG@10: 0.5288 - val_MAE: 0.1570 - val_RC@5: 0.6015 - val_RC@10: 0.6795 - lr: 9.2080e-05 - e_time: 41.5899 - 42s/epoch - 32ms/step
Epoch 81/1000
1292/1292 - 42s - loss: 0.7566 - NDCG@10: 0.5021 - MAE: 0.1751 - RC@5: 0.5836 - RC@10: 0.6728 - val_loss: 0.7880 - val_NDCG@10: 0.5293 - val_MAE: 0.1565 - val_RC@5: 0.6013 - val_RC@10: 0.6799 - lr: 9.1981e-05 - e_time: 41.7771 - 42s/epoch - 33ms/step
Epoch 82/1000
1292/1292 - 42s - loss: 0.7560 - NDCG@10: 0.5022 - MAE: 0.1749 - RC@5: 0.5842 - RC@10: 0.6725 - val_loss: 0.7881 - val_NDCG@10: 0.5293 - val_MAE: 0.1558 - val_RC@5: 0.6012 - val_RC@10: 0.6801 - lr: 9.1882e-05 - e_time: 41.8374 - 42s/epoch - 32ms/step
Epoch 83/1000
1292/1292 - 42s - loss: 0.7559 - NDCG@10: 0.5026 - MAE: 0.1748 - RC@5: 0.5843 - RC@10: 0.6727 - val_loss: 0.7880 - val_NDCG@10: 0.5293 - val_MAE: 0.1556 - val_RC@5: 0.6015 - val_RC@10: 0.6801 - lr: 9.1783e-05 - e_time: 41.7993 - 42s/epoch - 33ms/step
Epoch 84/1000
1292/1292 - 42s - loss: 0.7551 - NDCG@10: 0.5025 - MAE: 0.1745 - RC@5: 0.5842 - RC@10: 0.6730 - val_loss: 0.7880 - val_NDCG@10: 0.5297 - val_MAE: 0.1551 - val_RC@5: 0.6012 - val_RC@10: 0.6800 - lr: 9.1684e-05 - e_time: 41.7216 - 42s/epoch - 32ms/step
Epoch 85/1000
1292/1292 - 42s - loss: 0.7546 - NDCG@10: 0.5026 - MAE: 0.1745 - RC@5: 0.5838 - RC@10: 0.6735 - val_loss: 0.7882 - val_NDCG@10: 0.5291 - val_MAE: 0.1538 - val_RC@5: 0.6010 - val_RC@10: 0.6801 - lr: 9.1585e-05 - e_time: 41.7173 - 42s/epoch - 32ms/step
Epoch 86/1000
1292/1292 - 42s - loss: 0.7539 - NDCG@10: 0.5036 - MAE: 0.1741 - RC@5: 0.5857 - RC@10: 0.6749 - val_loss: 0.7878 - val_NDCG@10: 0.5296 - val_MAE: 0.1546 - val_RC@5: 0.6013 - val_RC@10: 0.6806 - lr: 9.1486e-05 - e_time: 41.7419 - 42s/epoch - 32ms/step
Epoch 87/1000
1292/1292 - 42s - loss: 0.7530 - NDCG@10: 0.5039 - MAE: 0.1739 - RC@5: 0.5853 - RC@10: 0.6746 - val_loss: 0.7878 - val_NDCG@10: 0.5299 - val_MAE: 0.1541 - val_RC@5: 0.6021 - val_RC@10: 0.6810 - lr: 9.1387e-05 - e_time: 41.7863 - 42s/epoch - 33ms/step
Epoch 88/1000
1292/1292 - 42s - loss: 0.7530 - NDCG@10: 0.5042 - MAE: 0.1737 - RC@5: 0.5859 - RC@10: 0.6749 - val_loss: 0.7870 - val_NDCG@10: 0.5303 - val_MAE: 0.1555 - val_RC@5: 0.6021 - val_RC@10: 0.6812 - lr: 9.1288e-05 - e_time: 41.7965 - 42s/epoch - 33ms/step
Epoch 89/1000
1292/1292 - 42s - loss: 0.7522 - NDCG@10: 0.5048 - MAE: 0.1736 - RC@5: 0.5866 - RC@10: 0.6755 - val_loss: 0.7873 - val_NDCG@10: 0.5312 - val_MAE: 0.1542 - val_RC@5: 0.6024 - val_RC@10: 0.6817 - lr: 9.1189e-05 - e_time: 41.7039 - 42s/epoch - 32ms/step
Epoch 90/1000
1292/1292 - 42s - loss: 0.7518 - NDCG@10: 0.5050 - MAE: 0.1733 - RC@5: 0.5875 - RC@10: 0.6761 - val_loss: 0.7876 - val_NDCG@10: 0.5308 - val_MAE: 0.1532 - val_RC@5: 0.6025 - val_RC@10: 0.6816 - lr: 9.1090e-05 - e_time: 41.7412 - 42s/epoch - 32ms/step
Epoch 91/1000
1292/1292 - 42s - loss: 0.7516 - NDCG@10: 0.5052 - MAE: 0.1732 - RC@5: 0.5877 - RC@10: 0.6765 - val_loss: 0.7875 - val_NDCG@10: 0.5308 - val_MAE: 0.1531 - val_RC@5: 0.6024 - val_RC@10: 0.6810 - lr: 9.0991e-05 - e_time: 41.8959 - 42s/epoch - 32ms/step
Epoch 92/1000
1292/1292 - 42s - loss: 0.7510 - NDCG@10: 0.5055 - MAE: 0.1729 - RC@5: 0.5876 - RC@10: 0.6767 - val_loss: 0.7866 - val_NDCG@10: 0.5309 - val_MAE: 0.1551 - val_RC@5: 0.6025 - val_RC@10: 0.6812 - lr: 9.0892e-05 - e_time: 41.6823 - 42s/epoch - 32ms/step
Epoch 93/1000
1292/1292 - 42s - loss: 0.7503 - NDCG@10: 0.5062 - MAE: 0.1727 - RC@5: 0.5886 - RC@10: 0.6777 - val_loss: 0.7876 - val_NDCG@10: 0.5296 - val_MAE: 0.1516 - val_RC@5: 0.6016 - val_RC@10: 0.6804 - lr: 9.0793e-05 - e_time: 41.6743 - 42s/epoch - 32ms/step
Epoch 94/1000
1292/1292 - 42s - loss: 0.7500 - NDCG@10: 0.5060 - MAE: 0.1725 - RC@5: 0.5884 - RC@10: 0.6772 - val_loss: 0.7873 - val_NDCG@10: 0.5307 - val_MAE: 0.1520 - val_RC@5: 0.6026 - val_RC@10: 0.6814 - lr: 9.0694e-05 - e_time: 41.7317 - 42s/epoch - 32ms/step
Epoch 95/1000
1292/1292 - 42s - loss: 0.7492 - NDCG@10: 0.5068 - MAE: 0.1723 - RC@5: 0.5893 - RC@10: 0.6778 - val_loss: 0.7872 - val_NDCG@10: 0.5313 - val_MAE: 0.1522 - val_RC@5: 0.6032 - val_RC@10: 0.6818 - lr: 9.0595e-05 - e_time: 41.7897 - 42s/epoch - 32ms/step
Epoch 96/1000
1292/1292 - 42s - loss: 0.7494 - NDCG@10: 0.5060 - MAE: 0.1722 - RC@5: 0.5882 - RC@10: 0.6772 - val_loss: 0.7870 - val_NDCG@10: 0.5314 - val_MAE: 0.1522 - val_RC@5: 0.6036 - val_RC@10: 0.6816 - lr: 9.0496e-05 - e_time: 41.6220 - 42s/epoch - 32ms/step
Epoch 97/1000
1292/1292 - 42s - loss: 0.7484 - NDCG@10: 0.5074 - MAE: 0.1719 - RC@5: 0.5899 - RC@10: 0.6788 - val_loss: 0.7873 - val_NDCG@10: 0.5312 - val_MAE: 0.1513 - val_RC@5: 0.6032 - val_RC@10: 0.6811 - lr: 9.0397e-05 - e_time: 41.6786 - 42s/epoch - 32ms/step
Epoch 98/1000
1292/1292 - 42s - loss: 0.7483 - NDCG@10: 0.5073 - MAE: 0.1718 - RC@5: 0.5897 - RC@10: 0.6788 - val_loss: 0.7866 - val_NDCG@10: 0.5312 - val_MAE: 0.1520 - val_RC@5: 0.6032 - val_RC@10: 0.6814 - lr: 9.0298e-05 - e_time: 41.7725 - 42s/epoch - 32ms/step
Epoch 99/1000
1292/1292 - 42s - loss: 0.7475 - NDCG@10: 0.5081 - MAE: 0.1717 - RC@5: 0.5904 - RC@10: 0.6792 - val_loss: 0.7871 - val_NDCG@10: 0.5313 - val_MAE: 0.1509 - val_RC@5: 0.6035 - val_RC@10: 0.6817 - lr: 9.0199e-05 - e_time: 41.7537 - 42s/epoch - 32ms/step
Epoch 100/1000
1292/1292 - 42s - loss: 0.7474 - NDCG@10: 0.5079 - MAE: 0.1714 - RC@5: 0.5903 - RC@10: 0.6797 - val_loss: 0.7865 - val_NDCG@10: 0.5321 - val_MAE: 0.1518 - val_RC@5: 0.6037 - val_RC@10: 0.6823 - lr: 9.0100e-05 - e_time: 41.7535 - 42s/epoch - 32ms/step
Epoch 101/1000
1292/1292 - 42s - loss: 0.7469 - NDCG@10: 0.5080 - MAE: 0.1713 - RC@5: 0.5907 - RC@10: 0.6794 - val_loss: 0.7873 - val_NDCG@10: 0.5315 - val_MAE: 0.1503 - val_RC@5: 0.6035 - val_RC@10: 0.6814 - lr: 9.0001e-05 - e_time: 41.7983 - 42s/epoch - 32ms/step
Epoch 102/1000
1292/1292 - 42s - loss: 0.7464 - NDCG@10: 0.5091 - MAE: 0.1710 - RC@5: 0.5916 - RC@10: 0.6805 - val_loss: 0.7868 - val_NDCG@10: 0.5316 - val_MAE: 0.1509 - val_RC@5: 0.6035 - val_RC@10: 0.6813 - lr: 8.9902e-05 - e_time: 41.7540 - 42s/epoch - 32ms/step
Epoch 103/1000
1292/1292 - 42s - loss: 0.7458 - NDCG@10: 0.5086 - MAE: 0.1709 - RC@5: 0.5911 - RC@10: 0.6804 - val_loss: 0.7869 - val_NDCG@10: 0.5317 - val_MAE: 0.1508 - val_RC@5: 0.6036 - val_RC@10: 0.6813 - lr: 8.9803e-05 - e_time: 41.7479 - 42s/epoch - 32ms/step
Epoch 104/1000
1292/1292 - 42s - loss: 0.7453 - NDCG@10: 0.5089 - MAE: 0.1707 - RC@5: 0.5915 - RC@10: 0.6806 - val_loss: 0.7863 - val_NDCG@10: 0.5319 - val_MAE: 0.1518 - val_RC@5: 0.6039 - val_RC@10: 0.6817 - lr: 8.9704e-05 - e_time: 41.7317 - 42s/epoch - 32ms/step
Epoch 105/1000
1292/1292 - 42s - loss: 0.7454 - NDCG@10: 0.5092 - MAE: 0.1708 - RC@5: 0.5923 - RC@10: 0.6807 - val_loss: 0.7870 - val_NDCG@10: 0.5315 - val_MAE: 0.1495 - val_RC@5: 0.6028 - val_RC@10: 0.6813 - lr: 8.9605e-05 - e_time: 41.8282 - 42s/epoch - 32ms/step
Epoch 106/1000
1292/1292 - 42s - loss: 0.7445 - NDCG@10: 0.5096 - MAE: 0.1705 - RC@5: 0.5922 - RC@10: 0.6815 - val_loss: 0.7873 - val_NDCG@10: 0.5317 - val_MAE: 0.1485 - val_RC@5: 0.6035 - val_RC@10: 0.6815 - lr: 8.9506e-05 - e_time: 41.8539 - 42s/epoch - 32ms/step
Epoch 107/1000
1292/1292 - 42s - loss: 0.7447 - NDCG@10: 0.5096 - MAE: 0.1705 - RC@5: 0.5930 - RC@10: 0.6818 - val_loss: 0.7864 - val_NDCG@10: 0.5319 - val_MAE: 0.1503 - val_RC@5: 0.6034 - val_RC@10: 0.6817 - lr: 8.9407e-05 - e_time: 41.8038 - 42s/epoch - 32ms/step
Epoch 108/1000
1292/1292 - 42s - loss: 0.7440 - NDCG@10: 0.5099 - MAE: 0.1703 - RC@5: 0.5931 - RC@10: 0.6819 - val_loss: 0.7865 - val_NDCG@10: 0.5318 - val_MAE: 0.1498 - val_RC@5: 0.6029 - val_RC@10: 0.6815 - lr: 8.9308e-05 - e_time: 41.7892 - 42s/epoch - 32ms/step
Epoch 109/1000
2023-05-30 00:01:02.290669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "ATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 200, 128)     1544704     ['input_1[0][0]']                
                                                                                                  
 embedding_1 (Embedding)        (None, 1985, 128)    254080      ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 200, 128)     0           ['embedding[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 128)    0           ['embedding_1[0][0]']            
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 200, 128)     0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 1985, 128)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 200, 1985)    0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 200, 1985)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['lambda_1[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,798,784
Trainable params: 1,798,784
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-05-30 00:01:07.885709: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
2023-05-30 00:01:10.562057: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
Epoch 1/1000
1292/1292 - 44s - loss: 1.9980 - NDCG@10: 0.0575 - MAE: 0.9165 - RC@5: 0.0699 - RC@10: 0.1013 - val_loss: 1.8928 - val_NDCG@10: 0.0949 - val_MAE: 0.7130 - val_RC@5: 0.1036 - val_RC@10: 0.1489 - lr: 9.9901e-05 - e_time: 43.5575 - 44s/epoch - 34ms/step
Epoch 2/1000
1292/1292 - 41s - loss: 1.8475 - NDCG@10: 0.1321 - MAE: 0.7381 - RC@5: 0.1544 - RC@10: 0.2016 - val_loss: 1.7618 - val_NDCG@10: 0.1908 - val_MAE: 0.7388 - val_RC@5: 0.2200 - val_RC@10: 0.2771 - lr: 9.9802e-05 - e_time: 41.1505 - 41s/epoch - 32ms/step
Epoch 3/1000
1292/1292 - 41s - loss: 1.6970 - NDCG@10: 0.2062 - MAE: 0.7284 - RC@5: 0.2430 - RC@10: 0.3081 - val_loss: 1.6082 - val_NDCG@10: 0.2572 - val_MAE: 0.7071 - val_RC@5: 0.2997 - val_RC@10: 0.3728 - lr: 9.9703e-05 - e_time: 41.2504 - 41s/epoch - 32ms/step
Epoch 4/1000
1292/1292 - 42s - loss: 1.5603 - NDCG@10: 0.2556 - MAE: 0.6805 - RC@5: 0.3012 - RC@10: 0.3790 - val_loss: 1.4900 - val_NDCG@10: 0.3021 - val_MAE: 0.6614 - val_RC@5: 0.3516 - val_RC@10: 0.4345 - lr: 9.9604e-05 - e_time: 41.4928 - 42s/epoch - 32ms/step
Epoch 5/1000
1292/1292 - 42s - loss: 1.4618 - NDCG@10: 0.2902 - MAE: 0.6283 - RC@5: 0.3425 - RC@10: 0.4281 - val_loss: 1.4098 - val_NDCG@10: 0.3344 - val_MAE: 0.6143 - val_RC@5: 0.3890 - val_RC@10: 0.4780 - lr: 9.9505e-05 - e_time: 41.6011 - 42s/epoch - 32ms/step
Epoch 6/1000
1292/1292 - 42s - loss: 1.3944 - NDCG@10: 0.3156 - MAE: 0.5833 - RC@5: 0.3726 - RC@10: 0.4626 - val_loss: 1.3544 - val_NDCG@10: 0.3589 - val_MAE: 0.5728 - val_RC@5: 0.4177 - val_RC@10: 0.5086 - lr: 9.9406e-05 - e_time: 41.6036 - 42s/epoch - 32ms/step
Epoch 7/1000
1292/1292 - 42s - loss: 1.3459 - NDCG@10: 0.3360 - MAE: 0.5477 - RC@5: 0.3961 - RC@10: 0.4892 - val_loss: 1.3142 - val_NDCG@10: 0.3779 - val_MAE: 0.5385 - val_RC@5: 0.4399 - val_RC@10: 0.5318 - lr: 9.9307e-05 - e_time: 41.6219 - 42s/epoch - 32ms/step
Epoch 8/1000
1292/1292 - 42s - loss: 1.3096 - NDCG@10: 0.3513 - MAE: 0.5199 - RC@5: 0.4142 - RC@10: 0.5092 - val_loss: 1.2842 - val_NDCG@10: 0.3926 - val_MAE: 0.5128 - val_RC@5: 0.4562 - val_RC@10: 0.5493 - lr: 9.9208e-05 - e_time: 41.5982 - 42s/epoch - 32ms/step
Epoch 9/1000
1292/1292 - 42s - loss: 1.2809 - NDCG@10: 0.3652 - MAE: 0.4970 - RC@5: 0.4301 - RC@10: 0.5263 - val_loss: 1.2606 - val_NDCG@10: 0.4041 - val_MAE: 0.4937 - val_RC@5: 0.4693 - val_RC@10: 0.5626 - lr: 9.9109e-05 - e_time: 41.6344 - 42s/epoch - 32ms/step
Epoch 10/1000
1292/1292 - 42s - loss: 1.2578 - NDCG@10: 0.3760 - MAE: 0.4785 - RC@5: 0.4426 - RC@10: 0.5406 - val_loss: 1.2416 - val_NDCG@10: 0.4160 - val_MAE: 0.4714 - val_RC@5: 0.4822 - val_RC@10: 0.5759 - lr: 9.9010e-05 - e_time: 41.6090 - 42s/epoch - 32ms/step
Epoch 11/1000
1292/1292 - 42s - loss: 1.2381 - NDCG@10: 0.3858 - MAE: 0.4627 - RC@5: 0.4543 - RC@10: 0.5521 - val_loss: 1.2259 - val_NDCG@10: 0.4238 - val_MAE: 0.4567 - val_RC@5: 0.4922 - val_RC@10: 0.5849 - lr: 9.8911e-05 - e_time: 41.6160 - 42s/epoch - 32ms/step
Epoch 12/1000
1292/1292 - 42s - loss: 1.2217 - NDCG@10: 0.3937 - MAE: 0.4497 - RC@5: 0.4635 - RC@10: 0.5614 - val_loss: 1.2129 - val_NDCG@10: 0.4322 - val_MAE: 0.4447 - val_RC@5: 0.5010 - val_RC@10: 0.5951 - lr: 9.8812e-05 - e_time: 41.6077 - 42s/epoch - 32ms/step
Epoch 13/1000
1292/1292 - 42s - loss: 1.2073 - NDCG@10: 0.4006 - MAE: 0.4385 - RC@5: 0.4715 - RC@10: 0.5697 - val_loss: 1.2017 - val_NDCG@10: 0.4393 - val_MAE: 0.4320 - val_RC@5: 0.5090 - val_RC@10: 0.6016 - lr: 9.8713e-05 - e_time: 41.6209 - 42s/epoch - 32ms/step
Epoch 14/1000
1292/1292 - 42s - loss: 1.1949 - NDCG@10: 0.4075 - MAE: 0.4285 - RC@5: 0.4800 - RC@10: 0.5781 - val_loss: 1.1922 - val_NDCG@10: 0.4454 - val_MAE: 0.4204 - val_RC@5: 0.5167 - val_RC@10: 0.6086 - lr: 9.8614e-05 - e_time: 41.6178 - 42s/epoch - 32ms/step
Epoch 15/1000
1292/1292 - 42s - loss: 1.1836 - NDCG@10: 0.4132 - MAE: 0.4200 - RC@5: 0.4861 - RC@10: 0.5847 - val_loss: 1.1840 - val_NDCG@10: 0.4506 - val_MAE: 0.4132 - val_RC@5: 0.5220 - val_RC@10: 0.6139 - lr: 9.8515e-05 - e_time: 41.6138 - 42s/epoch - 32ms/step
Epoch 16/1000
1292/1292 - 42s - loss: 1.1736 - NDCG@10: 0.4186 - MAE: 0.4123 - RC@5: 0.4925 - RC@10: 0.5908 - val_loss: 1.1766 - val_NDCG@10: 0.4546 - val_MAE: 0.4050 - val_RC@5: 0.5275 - val_RC@10: 0.6182 - lr: 9.8416e-05 - e_time: 41.6332 - 42s/epoch - 32ms/step
Epoch 17/1000
1292/1292 - 42s - loss: 1.1641 - NDCG@10: 0.4233 - MAE: 0.4055 - RC@5: 0.4974 - RC@10: 0.5965 - val_loss: 1.1701 - val_NDCG@10: 0.4600 - val_MAE: 0.3988 - val_RC@5: 0.5321 - val_RC@10: 0.6232 - lr: 9.8317e-05 - e_time: 41.6272 - 42s/epoch - 32ms/step
Epoch 18/1000
1292/1292 - 42s - loss: 1.1561 - NDCG@10: 0.4284 - MAE: 0.3992 - RC@5: 0.5029 - RC@10: 0.6016 - val_loss: 1.1644 - val_NDCG@10: 0.4649 - val_MAE: 0.3926 - val_RC@5: 0.5370 - val_RC@10: 0.6278 - lr: 9.8218e-05 - e_time: 41.6467 - 42s/epoch - 32ms/step
Epoch 19/1000
1292/1292 - 42s - loss: 1.1479 - NDCG@10: 0.4319 - MAE: 0.3935 - RC@5: 0.5072 - RC@10: 0.6057 - val_loss: 1.1592 - val_NDCG@10: 0.4676 - val_MAE: 0.3877 - val_RC@5: 0.5398 - val_RC@10: 0.6311 - lr: 9.8119e-05 - e_time: 41.6337 - 42s/epoch - 32ms/step
Epoch 20/1000
1292/1292 - 42s - loss: 1.1403 - NDCG@10: 0.4357 - MAE: 0.3882 - RC@5: 0.5118 - RC@10: 0.6107 - val_loss: 1.1547 - val_NDCG@10: 0.4718 - val_MAE: 0.3794 - val_RC@5: 0.5441 - val_RC@10: 0.6353 - lr: 9.8020e-05 - e_time: 41.6558 - 42s/epoch - 32ms/step
Epoch 21/1000
1292/1292 - 42s - loss: 1.1342 - NDCG@10: 0.4389 - MAE: 0.3835 - RC@5: 0.5156 - RC@10: 0.6139 - val_loss: 1.1504 - val_NDCG@10: 0.4748 - val_MAE: 0.3754 - val_RC@5: 0.5472 - val_RC@10: 0.6379 - lr: 9.7921e-05 - e_time: 41.5705 - 42s/epoch - 32ms/step
Epoch 22/1000
1292/1292 - 42s - loss: 1.1274 - NDCG@10: 0.4422 - MAE: 0.3789 - RC@5: 0.5188 - RC@10: 0.6179 - val_loss: 1.1464 - val_NDCG@10: 0.4775 - val_MAE: 0.3717 - val_RC@5: 0.5506 - val_RC@10: 0.6414 - lr: 9.7822e-05 - e_time: 41.6205 - 42s/epoch - 32ms/step
Epoch 23/1000
1292/1292 - 42s - loss: 1.1221 - NDCG@10: 0.4454 - MAE: 0.3749 - RC@5: 0.5227 - RC@10: 0.6215 - val_loss: 1.1432 - val_NDCG@10: 0.4798 - val_MAE: 0.3646 - val_RC@5: 0.5528 - val_RC@10: 0.6437 - lr: 9.7723e-05 - e_time: 41.6181 - 42s/epoch - 32ms/step
Epoch 24/1000
1292/1292 - 42s - loss: 1.1163 - NDCG@10: 0.4475 - MAE: 0.3712 - RC@5: 0.5256 - RC@10: 0.6240 - val_loss: 1.1400 - val_NDCG@10: 0.4820 - val_MAE: 0.3630 - val_RC@5: 0.5562 - val_RC@10: 0.6462 - lr: 9.7624e-05 - e_time: 41.5549 - 42s/epoch - 32ms/step
Epoch 25/1000
1292/1292 - 41s - loss: 1.1113 - NDCG@10: 0.4502 - MAE: 0.3676 - RC@5: 0.5284 - RC@10: 0.6269 - val_loss: 1.1370 - val_NDCG@10: 0.4853 - val_MAE: 0.3574 - val_RC@5: 0.5590 - val_RC@10: 0.6486 - lr: 9.7525e-05 - e_time: 41.2270 - 41s/epoch - 32ms/step
Epoch 26/1000
1292/1292 - 41s - loss: 1.1066 - NDCG@10: 0.4526 - MAE: 0.3642 - RC@5: 0.5308 - RC@10: 0.6293 - val_loss: 1.1344 - val_NDCG@10: 0.4874 - val_MAE: 0.3543 - val_RC@5: 0.5604 - val_RC@10: 0.6506 - lr: 9.7426e-05 - e_time: 41.1633 - 41s/epoch - 32ms/step
Epoch 27/1000
1292/1292 - 41s - loss: 1.1019 - NDCG@10: 0.4550 - MAE: 0.3612 - RC@5: 0.5336 - RC@10: 0.6321 - val_loss: 1.1322 - val_NDCG@10: 0.4891 - val_MAE: 0.3500 - val_RC@5: 0.5620 - val_RC@10: 0.6521 - lr: 9.7327e-05 - e_time: 41.1222 - 41s/epoch - 32ms/step
Epoch 28/1000
1292/1292 - 41s - loss: 1.0970 - NDCG@10: 0.4577 - MAE: 0.3580 - RC@5: 0.5367 - RC@10: 0.6350 - val_loss: 1.1296 - val_NDCG@10: 0.4912 - val_MAE: 0.3491 - val_RC@5: 0.5649 - val_RC@10: 0.6538 - lr: 9.7228e-05 - e_time: 41.1218 - 41s/epoch - 32ms/step
Epoch 29/1000
1292/1292 - 41s - loss: 1.0928 - NDCG@10: 0.4596 - MAE: 0.3553 - RC@5: 0.5385 - RC@10: 0.6372 - val_loss: 1.1279 - val_NDCG@10: 0.4922 - val_MAE: 0.3429 - val_RC@5: 0.5660 - val_RC@10: 0.6552 - lr: 9.7129e-05 - e_time: 41.1070 - 41s/epoch - 32ms/step
Epoch 30/1000
1292/1292 - 41s - loss: 1.0890 - NDCG@10: 0.4610 - MAE: 0.3524 - RC@5: 0.5405 - RC@10: 0.6386 - val_loss: 1.1258 - val_NDCG@10: 0.4937 - val_MAE: 0.3410 - val_RC@5: 0.5675 - val_RC@10: 0.6571 - lr: 9.7030e-05 - e_time: 41.0181 - 41s/epoch - 32ms/step
Epoch 31/1000
1292/1292 - 41s - loss: 1.0852 - NDCG@10: 0.4621 - MAE: 0.3503 - RC@5: 0.5421 - RC@10: 0.6403 - val_loss: 1.1237 - val_NDCG@10: 0.4950 - val_MAE: 0.3415 - val_RC@5: 0.5690 - val_RC@10: 0.6578 - lr: 9.6931e-05 - e_time: 41.0956 - 41s/epoch - 32ms/step
Epoch 32/1000
1292/1292 - 41s - loss: 1.0812 - NDCG@10: 0.4645 - MAE: 0.3478 - RC@5: 0.5440 - RC@10: 0.6428 - val_loss: 1.1223 - val_NDCG@10: 0.4972 - val_MAE: 0.3370 - val_RC@5: 0.5713 - val_RC@10: 0.6599 - lr: 9.6832e-05 - e_time: 41.0879 - 41s/epoch - 32ms/step
Epoch 33/1000
1292/1292 - 41s - loss: 1.0776 - NDCG@10: 0.4661 - MAE: 0.3456 - RC@5: 0.5463 - RC@10: 0.6445 - val_loss: 1.1209 - val_NDCG@10: 0.4980 - val_MAE: 0.3332 - val_RC@5: 0.5724 - val_RC@10: 0.6605 - lr: 9.6733e-05 - e_time: 41.0776 - 41s/epoch - 32ms/step
Epoch 34/1000
1292/1292 - 41s - loss: 1.0746 - NDCG@10: 0.4675 - MAE: 0.3433 - RC@5: 0.5477 - RC@10: 0.6462 - val_loss: 1.1192 - val_NDCG@10: 0.4994 - val_MAE: 0.3340 - val_RC@5: 0.5740 - val_RC@10: 0.6615 - lr: 9.6634e-05 - e_time: 41.1163 - 41s/epoch - 32ms/step
Epoch 35/1000
1292/1292 - 41s - loss: 1.0711 - NDCG@10: 0.4686 - MAE: 0.3413 - RC@5: 0.5492 - RC@10: 0.6473 - val_loss: 1.1184 - val_NDCG@10: 0.5001 - val_MAE: 0.3275 - val_RC@5: 0.5745 - val_RC@10: 0.6625 - lr: 9.6535e-05 - e_time: 41.1441 - 41s/epoch - 32ms/step
Epoch 36/1000
1292/1292 - 41s - loss: 1.0680 - NDCG@10: 0.4699 - MAE: 0.3393 - RC@5: 0.5509 - RC@10: 0.6488 - val_loss: 1.1170 - val_NDCG@10: 0.5012 - val_MAE: 0.3276 - val_RC@5: 0.5756 - val_RC@10: 0.6632 - lr: 9.6436e-05 - e_time: 41.0771 - 41s/epoch - 32ms/step
Epoch 37/1000
1292/1292 - 41s - loss: 1.0656 - NDCG@10: 0.4720 - MAE: 0.3374 - RC@5: 0.5530 - RC@10: 0.6507 - val_loss: 1.1159 - val_NDCG@10: 0.5023 - val_MAE: 0.3255 - val_RC@5: 0.5770 - val_RC@10: 0.6640 - lr: 9.6337e-05 - e_time: 41.1076 - 41s/epoch - 32ms/step
Epoch 38/1000
1292/1292 - 41s - loss: 1.0622 - NDCG@10: 0.4726 - MAE: 0.3356 - RC@5: 0.5534 - RC@10: 0.6521 - val_loss: 1.1154 - val_NDCG@10: 0.5028 - val_MAE: 0.3217 - val_RC@5: 0.5776 - val_RC@10: 0.6644 - lr: 9.6238e-05 - e_time: 41.0898 - 41s/epoch - 32ms/step
Epoch 39/1000
1292/1292 - 41s - loss: 1.0594 - NDCG@10: 0.4742 - MAE: 0.3337 - RC@5: 0.5560 - RC@10: 0.6533 - val_loss: 1.1144 - val_NDCG@10: 0.5035 - val_MAE: 0.3199 - val_RC@5: 0.5784 - val_RC@10: 0.6652 - lr: 9.6139e-05 - e_time: 41.1231 - 41s/epoch - 32ms/step
Epoch 40/1000
1292/1292 - 41s - loss: 1.0567 - NDCG@10: 0.4747 - MAE: 0.3322 - RC@5: 0.5565 - RC@10: 0.6541 - val_loss: 1.1133 - val_NDCG@10: 0.5048 - val_MAE: 0.3185 - val_RC@5: 0.5797 - val_RC@10: 0.6657 - lr: 9.6040e-05 - e_time: 41.0987 - 41s/epoch - 32ms/step
Epoch 41/1000
1292/1292 - 41s - loss: 1.0541 - NDCG@10: 0.4758 - MAE: 0.3305 - RC@5: 0.5572 - RC@10: 0.6558 - val_loss: 1.1122 - val_NDCG@10: 0.5052 - val_MAE: 0.3184 - val_RC@5: 0.5807 - val_RC@10: 0.6662 - lr: 9.5941e-05 - e_time: 41.1300 - 41s/epoch - 32ms/step
Epoch 42/1000
1292/1292 - 41s - loss: 1.0516 - NDCG@10: 0.4773 - MAE: 0.3290 - RC@5: 0.5584 - RC@10: 0.6571 - val_loss: 1.1112 - val_NDCG@10: 0.5061 - val_MAE: 0.3185 - val_RC@5: 0.5809 - val_RC@10: 0.6670 - lr: 9.5842e-05 - e_time: 41.0885 - 41s/epoch - 32ms/step
Epoch 43/1000
1292/1292 - 41s - loss: 1.0493 - NDCG@10: 0.4779 - MAE: 0.3276 - RC@5: 0.5600 - RC@10: 0.6582 - val_loss: 1.1106 - val_NDCG@10: 0.5066 - val_MAE: 0.3166 - val_RC@5: 0.5822 - val_RC@10: 0.6675 - lr: 9.5743e-05 - e_time: 41.1328 - 41s/epoch - 32ms/step
Epoch 44/1000
1292/1292 - 41s - loss: 1.0463 - NDCG@10: 0.4793 - MAE: 0.3262 - RC@5: 0.5614 - RC@10: 0.6596 - val_loss: 1.1102 - val_NDCG@10: 0.5075 - val_MAE: 0.3132 - val_RC@5: 0.5830 - val_RC@10: 0.6684 - lr: 9.5644e-05 - e_time: 41.0999 - 41s/epoch - 32ms/step
Epoch 45/1000
1292/1292 - 41s - loss: 1.0440 - NDCG@10: 0.4802 - MAE: 0.3245 - RC@5: 0.5627 - RC@10: 0.6604 - val_loss: 1.1095 - val_NDCG@10: 0.5084 - val_MAE: 0.3121 - val_RC@5: 0.5838 - val_RC@10: 0.6688 - lr: 9.5545e-05 - e_time: 41.0929 - 41s/epoch - 32ms/step
Epoch 46/1000
1292/1292 - 41s - loss: 1.0418 - NDCG@10: 0.4811 - MAE: 0.3233 - RC@5: 0.5634 - RC@10: 0.6610 - val_loss: 1.1091 - val_NDCG@10: 0.5092 - val_MAE: 0.3095 - val_RC@5: 0.5843 - val_RC@10: 0.6695 - lr: 9.5446e-05 - e_time: 41.0964 - 41s/epoch - 32ms/step
Epoch 47/1000
1292/1292 - 41s - loss: 1.0401 - NDCG@10: 0.4817 - MAE: 0.3220 - RC@5: 0.5642 - RC@10: 0.6624 - val_loss: 1.1082 - val_NDCG@10: 0.5097 - val_MAE: 0.3091 - val_RC@5: 0.5847 - val_RC@10: 0.6698 - lr: 9.5347e-05 - e_time: 41.1049 - 41s/epoch - 32ms/step
Epoch 48/1000
1292/1292 - 41s - loss: 1.0376 - NDCG@10: 0.4826 - MAE: 0.3206 - RC@5: 0.5649 - RC@10: 0.6631 - val_loss: 1.1079 - val_NDCG@10: 0.5099 - val_MAE: 0.3076 - val_RC@5: 0.5854 - val_RC@10: 0.6699 - lr: 9.5248e-05 - e_time: 41.1173 - 41s/epoch - 32ms/step
Epoch 49/1000
1292/1292 - 41s - loss: 1.0355 - NDCG@10: 0.4836 - MAE: 0.3197 - RC@5: 0.5657 - RC@10: 0.6645 - val_loss: 1.1074 - val_NDCG@10: 0.5108 - val_MAE: 0.3059 - val_RC@5: 0.5860 - val_RC@10: 0.6706 - lr: 9.5149e-05 - e_time: 41.1021 - 41s/epoch - 32ms/step
Epoch 50/1000
1292/1292 - 41s - loss: 1.0333 - NDCG@10: 0.4845 - MAE: 0.3181 - RC@5: 0.5668 - RC@10: 0.6654 - val_loss: 1.1075 - val_NDCG@10: 0.5112 - val_MAE: 0.3028 - val_RC@5: 0.5864 - val_RC@10: 0.6711 - lr: 9.5050e-05 - e_time: 41.0899 - 41s/epoch - 32ms/step
Epoch 51/1000
1292/1292 - 41s - loss: 1.0317 - NDCG@10: 0.4846 - MAE: 0.3174 - RC@5: 0.5671 - RC@10: 0.6654 - val_loss: 1.1070 - val_NDCG@10: 0.5115 - val_MAE: 0.3013 - val_RC@5: 0.5872 - val_RC@10: 0.6715 - lr: 9.4951e-05 - e_time: 41.0681 - 41s/epoch - 32ms/step
Epoch 52/1000
1292/1292 - 41s - loss: 1.0304 - NDCG@10: 0.4851 - MAE: 0.3163 - RC@5: 0.5681 - RC@10: 0.6663 - val_loss: 1.1062 - val_NDCG@10: 0.5116 - val_MAE: 0.3023 - val_RC@5: 0.5873 - val_RC@10: 0.6713 - lr: 9.4852e-05 - e_time: 41.1177 - 41s/epoch - 32ms/step
Epoch 53/1000
1292/1292 - 41s - loss: 1.0277 - NDCG@10: 0.4860 - MAE: 0.3148 - RC@5: 0.5694 - RC@10: 0.6675 - val_loss: 1.1062 - val_NDCG@10: 0.5125 - val_MAE: 0.2993 - val_RC@5: 0.5882 - val_RC@10: 0.6726 - lr: 9.4753e-05 - e_time: 41.1241 - 41s/epoch - 32ms/step
Epoch 54/1000
1292/1292 - 41s - loss: 1.0262 - NDCG@10: 0.4871 - MAE: 0.3141 - RC@5: 0.5703 - RC@10: 0.6682 - val_loss: 1.1059 - val_NDCG@10: 0.5130 - val_MAE: 0.2986 - val_RC@5: 0.5880 - val_RC@10: 0.6730 - lr: 9.4654e-05 - e_time: 41.1064 - 41s/epoch - 32ms/step
Epoch 55/1000
1292/1292 - 41s - loss: 1.0245 - NDCG@10: 0.4876 - MAE: 0.3131 - RC@5: 0.5706 - RC@10: 0.6689 - val_loss: 1.1061 - val_NDCG@10: 0.5128 - val_MAE: 0.2959 - val_RC@5: 0.5885 - val_RC@10: 0.6722 - lr: 9.4555e-05 - e_time: 41.0714 - 41s/epoch - 32ms/step
Epoch 56/1000
1292/1292 - 41s - loss: 1.0229 - NDCG@10: 0.4878 - MAE: 0.3121 - RC@5: 0.5712 - RC@10: 0.6691 - val_loss: 1.1055 - val_NDCG@10: 0.5133 - val_MAE: 0.2964 - val_RC@5: 0.5890 - val_RC@10: 0.6726 - lr: 9.4456e-05 - e_time: 41.0685 - 41s/epoch - 32ms/step
Epoch 57/1000
1292/1292 - 41s - loss: 1.0213 - NDCG@10: 0.4883 - MAE: 0.3113 - RC@5: 0.5714 - RC@10: 0.6702 - val_loss: 1.1058 - val_NDCG@10: 0.5129 - val_MAE: 0.2936 - val_RC@5: 0.5895 - val_RC@10: 0.6729 - lr: 9.4357e-05 - e_time: 41.0757 - 41s/epoch - 32ms/step
Epoch 58/1000
1292/1292 - 41s - loss: 1.0190 - NDCG@10: 0.4892 - MAE: 0.3098 - RC@5: 0.5728 - RC@10: 0.6716 - val_loss: 1.1059 - val_NDCG@10: 0.5134 - val_MAE: 0.2920 - val_RC@5: 0.5887 - val_RC@10: 0.6729 - lr: 9.4258e-05 - e_time: 41.0692 - 41s/epoch - 32ms/step
Epoch 59/1000
1292/1292 - 41s - loss: 1.0177 - NDCG@10: 0.4894 - MAE: 0.3094 - RC@5: 0.5730 - RC@10: 0.6713 - val_loss: 1.1049 - val_NDCG@10: 0.5134 - val_MAE: 0.2937 - val_RC@5: 0.5882 - val_RC@10: 0.6730 - lr: 9.4159e-05 - e_time: 41.0797 - 41s/epoch - 32ms/step
Epoch 60/1000
1292/1292 - 41s - loss: 1.0158 - NDCG@10: 0.4905 - MAE: 0.3083 - RC@5: 0.5743 - RC@10: 0.6728 - val_loss: 1.1046 - val_NDCG@10: 0.5144 - val_MAE: 0.2938 - val_RC@5: 0.5892 - val_RC@10: 0.6734 - lr: 9.4060e-05 - e_time: 41.1053 - 41s/epoch - 32ms/step
Epoch 61/1000
1292/1292 - 41s - loss: 1.0147 - NDCG@10: 0.4907 - MAE: 0.3075 - RC@5: 0.5739 - RC@10: 0.6730 - val_loss: 1.1049 - val_NDCG@10: 0.5145 - val_MAE: 0.2913 - val_RC@5: 0.5899 - val_RC@10: 0.6740 - lr: 9.3961e-05 - e_time: 41.1061 - 41s/epoch - 32ms/step
Epoch 62/1000
1292/1292 - 41s - loss: 1.0130 - NDCG@10: 0.4908 - MAE: 0.3068 - RC@5: 0.5746 - RC@10: 0.6731 - val_loss: 1.1049 - val_NDCG@10: 0.5153 - val_MAE: 0.2894 - val_RC@5: 0.5909 - val_RC@10: 0.6744 - lr: 9.3862e-05 - e_time: 41.0817 - 41s/epoch - 32ms/step
Epoch 63/1000
1292/1292 - 41s - loss: 1.0116 - NDCG@10: 0.4918 - MAE: 0.3056 - RC@5: 0.5759 - RC@10: 0.6745 - val_loss: 1.1043 - val_NDCG@10: 0.5151 - val_MAE: 0.2894 - val_RC@5: 0.5912 - val_RC@10: 0.6739 - lr: 9.3763e-05 - e_time: 41.0678 - 41s/epoch - 32ms/step
Epoch 64/1000
1292/1292 - 41s - loss: 1.0108 - NDCG@10: 0.4918 - MAE: 0.3052 - RC@5: 0.5762 - RC@10: 0.6741 - val_loss: 1.1050 - val_NDCG@10: 0.5148 - val_MAE: 0.2863 - val_RC@5: 0.5905 - val_RC@10: 0.6740 - lr: 9.3664e-05 - e_time: 41.0981 - 41s/epoch - 32ms/step
Epoch 65/1000
1292/1292 - 41s - loss: 1.0085 - NDCG@10: 0.4928 - MAE: 0.3040 - RC@5: 0.5769 - RC@10: 0.6759 - val_loss: 1.1041 - val_NDCG@10: 0.5157 - val_MAE: 0.2880 - val_RC@5: 0.5912 - val_RC@10: 0.6751 - lr: 9.3565e-05 - e_time: 41.0759 - 41s/epoch - 32ms/step
Epoch 66/1000
1292/1292 - 41s - loss: 1.0079 - NDCG@10: 0.4927 - MAE: 0.3037 - RC@5: 0.5770 - RC@10: 0.6752 - val_loss: 1.1045 - val_NDCG@10: 0.5154 - val_MAE: 0.2862 - val_RC@5: 0.5909 - val_RC@10: 0.6741 - lr: 9.3466e-05 - e_time: 41.0938 - 41s/epoch - 32ms/step
Epoch 67/1000
1292/1292 - 41s - loss: 1.0062 - NDCG@10: 0.4938 - MAE: 0.3027 - RC@5: 0.5779 - RC@10: 0.6768 - val_loss: 1.1039 - val_NDCG@10: 0.5155 - val_MAE: 0.2865 - val_RC@5: 0.5906 - val_RC@10: 0.6745 - lr: 9.3367e-05 - e_time: 41.0777 - 41s/epoch - 32ms/step
Epoch 68/1000
1292/1292 - 41s - loss: 1.0047 - NDCG@10: 0.4940 - MAE: 0.3019 - RC@5: 0.5779 - RC@10: 0.6768 - val_loss: 1.1038 - val_NDCG@10: 0.5154 - val_MAE: 0.2855 - val_RC@5: 0.5907 - val_RC@10: 0.6742 - lr: 9.3268e-05 - e_time: 41.1327 - 41s/epoch - 32ms/step
Epoch 69/1000
1292/1292 - 41s - loss: 1.0038 - NDCG@10: 0.4944 - MAE: 0.3010 - RC@5: 0.5789 - RC@10: 0.6771 - val_loss: 1.1047 - val_NDCG@10: 0.5153 - val_MAE: 0.2823 - val_RC@5: 0.5901 - val_RC@10: 0.6744 - lr: 9.3169e-05 - e_time: 41.0854 - 41s/epoch - 32ms/step
Epoch 70/1000
1292/1292 - 41s - loss: 1.0024 - NDCG@10: 0.4943 - MAE: 0.3005 - RC@5: 0.5787 - RC@10: 0.6778 - val_loss: 1.1038 - val_NDCG@10: 0.5158 - val_MAE: 0.2845 - val_RC@5: 0.5910 - val_RC@10: 0.6749 - lr: 9.3070e-05 - e_time: 41.0771 - 41s/epoch - 32ms/step
Epoch 71/1000
1292/1292 - 41s - loss: 1.0015 - NDCG@10: 0.4950 - MAE: 0.2997 - RC@5: 0.5796 - RC@10: 0.6781 - val_loss: 1.1042 - val_NDCG@10: 0.5162 - val_MAE: 0.2822 - val_RC@5: 0.5912 - val_RC@10: 0.6748 - lr: 9.2971e-05 - e_time: 41.0662 - 41s/epoch - 32ms/step
Epoch 72/1000
1292/1292 - 41s - loss: 0.9997 - NDCG@10: 0.4954 - MAE: 0.2990 - RC@5: 0.5800 - RC@10: 0.6782 - val_loss: 1.1041 - val_NDCG@10: 0.5161 - val_MAE: 0.2822 - val_RC@5: 0.5915 - val_RC@10: 0.6755 - lr: 9.2872e-05 - e_time: 41.0792 - 41s/epoch - 32ms/step
Epoch 73/1000
1292/1292 - 41s - loss: 0.9989 - NDCG@10: 0.4957 - MAE: 0.2982 - RC@5: 0.5804 - RC@10: 0.6789 - val_loss: 1.1047 - val_NDCG@10: 0.5160 - val_MAE: 0.2793 - val_RC@5: 0.5908 - val_RC@10: 0.6749 - lr: 9.2773e-05 - e_time: 41.0757 - 41s/epoch - 32ms/step
Epoch 74/1000
1292/1292 - 41s - loss: 0.9978 - NDCG@10: 0.4963 - MAE: 0.2978 - RC@5: 0.5817 - RC@10: 0.6793 - val_loss: 1.1047 - val_NDCG@10: 0.5156 - val_MAE: 0.2793 - val_RC@5: 0.5907 - val_RC@10: 0.6752 - lr: 9.2674e-05 - e_time: 41.0711 - 41s/epoch - 32ms/step
Epoch 75/1000
1292/1292 - 41s - loss: 0.9968 - NDCG@10: 0.4968 - MAE: 0.2974 - RC@5: 0.5818 - RC@10: 0.6800 - val_loss: 1.1045 - val_NDCG@10: 0.5160 - val_MAE: 0.2790 - val_RC@5: 0.5909 - val_RC@10: 0.6746 - lr: 9.2575e-05 - e_time: 41.0615 - 41s/epoch - 32ms/step
Epoch 76/1000
1292/1292 - 41s - loss: 0.9957 - NDCG@10: 0.4967 - MAE: 0.2965 - RC@5: 0.5817 - RC@10: 0.6800 - val_loss: 1.1047 - val_NDCG@10: 0.5161 - val_MAE: 0.2773 - val_RC@5: 0.5914 - val_RC@10: 0.6751 - lr: 9.2476e-05 - e_time: 41.0825 - 41s/epoch - 32ms/step
Epoch 77/1000
1292/1292 - 41s - loss: 0.9942 - NDCG@10: 0.4973 - MAE: 0.2958 - RC@5: 0.5826 - RC@10: 0.6809 - val_loss: 1.1044 - val_NDCG@10: 0.5176 - val_MAE: 0.2779 - val_RC@5: 0.5926 - val_RC@10: 0.6763 - lr: 9.2377e-05 - e_time: 41.0915 - 41s/epoch - 32ms/step
Epoch 78/1000
1292/1292 - 41s - loss: 0.9934 - NDCG@10: 0.4976 - MAE: 0.2952 - RC@5: 0.5832 - RC@10: 0.6808 - val_loss: 1.1041 - val_NDCG@10: 0.5167 - val_MAE: 0.2784 - val_RC@5: 0.5918 - val_RC@10: 0.6751 - lr: 9.2278e-05 - e_time: 41.0870 - 41s/epoch - 32ms/step
Epoch 78: early stopping
[92m[INFO] Loading best model...[0m
WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name='input_1'), name='input_1', description="created by layer 'input_1'"), but it was called on an input with incompatible shape (None,).
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:991: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:991: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(
(1985, 2)
[92m[QUERY] 'Quiero comer un arroz con bogavante y con buenas vistas'[0m
Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 78, in <module>
    mdl.evaluate_text("Quiero comer un arroz con bogavante y con buenas vistas")
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/ATT2VAL.py", line 201, in evaluate_text
    lstm_text_complete = [list(map(lambda x: self.DATASET.DATA["TEXT_TOKENIZER"].word_index[x], text_prepro.split(" ")))] 
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/ATT2VAL.py", line 201, in <lambda>
    lstm_text_complete = [list(map(lambda x: self.DATASET.DATA["TEXT_TOKENIZER"].word_index[x], text_prepro.split(" ")))] 
KeyError: 'quiero'
2023-05-30 18:53:52.128162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
[93m[WARNING] Model folder already exists...[0m
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "ATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 200, 200)     2413600     ['input_1[0][0]']                
                                                                                                  
 embedding_1 (Embedding)        (None, 1985, 200)    397000      ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 200, 200)     0           ['embedding[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 200)    0           ['embedding_1[0][0]']            
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 200, 200)     0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 1985, 200)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 200, 1985)    0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 200, 1985)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['lambda_1[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,810,600
Trainable params: 2,810,600
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-05-30 18:53:59.153517: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
[92m[INFO] Model already trained. Loading weights...[0m
Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 98, in <module>
    mdl.train(dev=True, save_model=True, callbacks=[])
  File "/media/nas/pperez/code/TAVtext/src/models/KerasModelClass.py", line 50, in train
    self.__train_model__(train_sequence=train_seq, dev_sequence=dev_seq, save_model=save_model, train_cfg=train_cfg, callbacks=callbacks)
  File "/media/nas/pperez/code/TAVtext/src/models/KerasModelClass.py", line 99, in __train_model__
    self.MODEL.load_weights(self.MODEL_PATH + final_folder + "weights")
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 139, in restore
    raise ValueError(
ValueError: Received incompatible tensor with shape (12068, 128) when attempting to restore variable with shape (12068, 200) and name layer_with_weights-0/embeddings/.ATTRIBUTES/VARIABLE_VALUE.
2023-05-30 18:54:27.042526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "ATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 embedding (Embedding)          (None, 200, 200)     2413600     ['input_1[0][0]']                
                                                                                                  
 embedding_1 (Embedding)        (None, 1985, 200)    397000      ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 200, 200)     0           ['embedding[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 200)    0           ['embedding_1[0][0]']            
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 200, 200)     0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 1985, 200)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 lambda (Lambda)                (None, 200, 1985)    0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 lambda_1 (Lambda)              (None, 200, 1985)    0           ['lambda[0][0]',                 
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['lambda_1[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 2,810,600
Trainable params: 2,810,600
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-05-30 18:54:35.021994: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
2023-05-30 18:54:38.497736: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
Epoch 1/1000
1292/1292 - 57s - loss: 1.9626 - NDCG@10: 0.0863 - MAE: 0.8755 - RC@5: 0.1011 - RC@10: 0.1364 - val_loss: 1.8432 - val_NDCG@10: 0.1464 - val_MAE: 0.7066 - val_RC@5: 0.1690 - val_RC@10: 0.2145 - lr: 9.9901e-05 - e_time: 56.5729 - 57s/epoch - 44ms/step
Epoch 2/1000
1292/1292 - 55s - loss: 1.7584 - NDCG@10: 0.2020 - MAE: 0.7194 - RC@5: 0.2335 - RC@10: 0.2895 - val_loss: 1.6544 - val_NDCG@10: 0.2534 - val_MAE: 0.7108 - val_RC@5: 0.2943 - val_RC@10: 0.3608 - lr: 9.9802e-05 - e_time: 54.6601 - 55s/epoch - 43ms/step
Epoch 3/1000
1292/1292 - 55s - loss: 1.5741 - NDCG@10: 0.2812 - MAE: 0.6853 - RC@5: 0.3270 - RC@10: 0.4006 - val_loss: 1.4934 - val_NDCG@10: 0.3196 - val_MAE: 0.6695 - val_RC@5: 0.3709 - val_RC@10: 0.4496 - lr: 9.9703e-05 - e_time: 54.6925 - 55s/epoch - 43ms/step
Epoch 4/1000
1292/1292 - 55s - loss: 1.4368 - NDCG@10: 0.3331 - MAE: 0.6222 - RC@5: 0.3887 - RC@10: 0.4715 - val_loss: 1.3892 - val_NDCG@10: 0.3618 - val_MAE: 0.6129 - val_RC@5: 0.4200 - val_RC@10: 0.5046 - lr: 9.9604e-05 - e_time: 54.7675 - 55s/epoch - 43ms/step
Epoch 5/1000
1292/1292 - 55s - loss: 1.3479 - NDCG@10: 0.3687 - MAE: 0.5642 - RC@5: 0.4293 - RC@10: 0.5176 - val_loss: 1.3232 - val_NDCG@10: 0.3898 - val_MAE: 0.5616 - val_RC@5: 0.4513 - val_RC@10: 0.5392 - lr: 9.9505e-05 - e_time: 55.0194 - 55s/epoch - 43ms/step
Epoch 6/1000
1292/1292 - 55s - loss: 1.2878 - NDCG@10: 0.3945 - MAE: 0.5202 - RC@5: 0.4592 - RC@10: 0.5500 - val_loss: 1.2790 - val_NDCG@10: 0.4108 - val_MAE: 0.5244 - val_RC@5: 0.4752 - val_RC@10: 0.5648 - lr: 9.9406e-05 - e_time: 55.0588 - 55s/epoch - 43ms/step
Epoch 7/1000
1292/1292 - 56s - loss: 1.2448 - NDCG@10: 0.4144 - MAE: 0.4868 - RC@5: 0.4816 - RC@10: 0.5746 - val_loss: 1.2472 - val_NDCG@10: 0.4266 - val_MAE: 0.4920 - val_RC@5: 0.4920 - val_RC@10: 0.5823 - lr: 9.9307e-05 - e_time: 55.1728 - 56s/epoch - 43ms/step
Epoch 8/1000
1292/1292 - 56s - loss: 1.2121 - NDCG@10: 0.4302 - MAE: 0.4609 - RC@5: 0.4995 - RC@10: 0.5928 - val_loss: 1.2235 - val_NDCG@10: 0.4396 - val_MAE: 0.4677 - val_RC@5: 0.5060 - val_RC@10: 0.5975 - lr: 9.9208e-05 - e_time: 55.2549 - 56s/epoch - 43ms/step
Epoch 9/1000
1292/1292 - 56s - loss: 1.1854 - NDCG@10: 0.4425 - MAE: 0.4403 - RC@5: 0.5143 - RC@10: 0.6070 - val_loss: 1.2051 - val_NDCG@10: 0.4495 - val_MAE: 0.4479 - val_RC@5: 0.5176 - val_RC@10: 0.6082 - lr: 9.9109e-05 - e_time: 55.2999 - 56s/epoch - 43ms/step
Epoch 10/1000
1292/1292 - 56s - loss: 1.1632 - NDCG@10: 0.4536 - MAE: 0.4236 - RC@5: 0.5259 - RC@10: 0.6191 - val_loss: 1.1902 - val_NDCG@10: 0.4587 - val_MAE: 0.4319 - val_RC@5: 0.5274 - val_RC@10: 0.6181 - lr: 9.9010e-05 - e_time: 55.3142 - 56s/epoch - 43ms/step
Epoch 11/1000
1292/1292 - 56s - loss: 1.1441 - NDCG@10: 0.4629 - MAE: 0.4094 - RC@5: 0.5369 - RC@10: 0.6302 - val_loss: 1.1780 - val_NDCG@10: 0.4652 - val_MAE: 0.4155 - val_RC@5: 0.5357 - val_RC@10: 0.6246 - lr: 9.8911e-05 - e_time: 55.2695 - 56s/epoch - 43ms/step
Epoch 12/1000
1292/1292 - 56s - loss: 1.1274 - NDCG@10: 0.4707 - MAE: 0.3975 - RC@5: 0.5454 - RC@10: 0.6388 - val_loss: 1.1678 - val_NDCG@10: 0.4714 - val_MAE: 0.4053 - val_RC@5: 0.5427 - val_RC@10: 0.6315 - lr: 9.8812e-05 - e_time: 55.3526 - 56s/epoch - 43ms/step
Epoch 13/1000
1292/1292 - 56s - loss: 1.1124 - NDCG@10: 0.4779 - MAE: 0.3872 - RC@5: 0.5535 - RC@10: 0.6472 - val_loss: 1.1595 - val_NDCG@10: 0.4770 - val_MAE: 0.3925 - val_RC@5: 0.5485 - val_RC@10: 0.6368 - lr: 9.8713e-05 - e_time: 55.3317 - 56s/epoch - 43ms/step
Epoch 14/1000
1292/1292 - 56s - loss: 1.0987 - NDCG@10: 0.4844 - MAE: 0.3779 - RC@5: 0.5612 - RC@10: 0.6544 - val_loss: 1.1523 - val_NDCG@10: 0.4805 - val_MAE: 0.3832 - val_RC@5: 0.5531 - val_RC@10: 0.6405 - lr: 9.8614e-05 - e_time: 55.3151 - 56s/epoch - 43ms/step
Epoch 15/1000
1292/1292 - 55s - loss: 1.0868 - NDCG@10: 0.4894 - MAE: 0.3698 - RC@5: 0.5665 - RC@10: 0.6605 - val_loss: 1.1463 - val_NDCG@10: 0.4849 - val_MAE: 0.3754 - val_RC@5: 0.5576 - val_RC@10: 0.6445 - lr: 9.8515e-05 - e_time: 55.0465 - 55s/epoch - 43ms/step
Epoch 16/1000
1292/1292 - 55s - loss: 1.0754 - NDCG@10: 0.4947 - MAE: 0.3624 - RC@5: 0.5728 - RC@10: 0.6657 - val_loss: 1.1411 - val_NDCG@10: 0.4878 - val_MAE: 0.3669 - val_RC@5: 0.5604 - val_RC@10: 0.6481 - lr: 9.8416e-05 - e_time: 55.0917 - 55s/epoch - 43ms/step
Epoch 17/1000
1292/1292 - 55s - loss: 1.0654 - NDCG@10: 0.4993 - MAE: 0.3558 - RC@5: 0.5781 - RC@10: 0.6710 - val_loss: 1.1364 - val_NDCG@10: 0.4921 - val_MAE: 0.3604 - val_RC@5: 0.5640 - val_RC@10: 0.6518 - lr: 9.8317e-05 - e_time: 54.8667 - 55s/epoch - 43ms/step
Epoch 18/1000
1292/1292 - 55s - loss: 1.0556 - NDCG@10: 0.5036 - MAE: 0.3497 - RC@5: 0.5831 - RC@10: 0.6760 - val_loss: 1.1326 - val_NDCG@10: 0.4944 - val_MAE: 0.3546 - val_RC@5: 0.5667 - val_RC@10: 0.6537 - lr: 9.8218e-05 - e_time: 54.7501 - 55s/epoch - 43ms/step
Epoch 19/1000
1292/1292 - 55s - loss: 1.0461 - NDCG@10: 0.5072 - MAE: 0.3439 - RC@5: 0.5870 - RC@10: 0.6805 - val_loss: 1.1291 - val_NDCG@10: 0.4971 - val_MAE: 0.3494 - val_RC@5: 0.5700 - val_RC@10: 0.6566 - lr: 9.8119e-05 - e_time: 54.7532 - 55s/epoch - 43ms/step
Epoch 20/1000
1292/1292 - 55s - loss: 1.0377 - NDCG@10: 0.5110 - MAE: 0.3388 - RC@5: 0.5913 - RC@10: 0.6839 - val_loss: 1.1263 - val_NDCG@10: 0.4988 - val_MAE: 0.3426 - val_RC@5: 0.5722 - val_RC@10: 0.6577 - lr: 9.8020e-05 - e_time: 54.7190 - 55s/epoch - 43ms/step
Epoch 21/1000
1292/1292 - 55s - loss: 1.0290 - NDCG@10: 0.5142 - MAE: 0.3337 - RC@5: 0.5952 - RC@10: 0.6875 - val_loss: 1.1235 - val_NDCG@10: 0.5007 - val_MAE: 0.3385 - val_RC@5: 0.5742 - val_RC@10: 0.6598 - lr: 9.7921e-05 - e_time: 54.7375 - 55s/epoch - 43ms/step
Epoch 22/1000
1292/1292 - 55s - loss: 1.0219 - NDCG@10: 0.5173 - MAE: 0.3294 - RC@5: 0.5982 - RC@10: 0.6916 - val_loss: 1.1216 - val_NDCG@10: 0.5022 - val_MAE: 0.3322 - val_RC@5: 0.5758 - val_RC@10: 0.6613 - lr: 9.7822e-05 - e_time: 54.7440 - 55s/epoch - 43ms/step
Epoch 23/1000
1292/1292 - 55s - loss: 1.0145 - NDCG@10: 0.5194 - MAE: 0.3251 - RC@5: 0.6008 - RC@10: 0.6941 - val_loss: 1.1196 - val_NDCG@10: 0.5035 - val_MAE: 0.3288 - val_RC@5: 0.5765 - val_RC@10: 0.6623 - lr: 9.7723e-05 - e_time: 54.7109 - 55s/epoch - 43ms/step
Epoch 24/1000
1292/1292 - 55s - loss: 1.0076 - NDCG@10: 0.5224 - MAE: 0.3211 - RC@5: 0.6048 - RC@10: 0.6971 - val_loss: 1.1174 - val_NDCG@10: 0.5052 - val_MAE: 0.3270 - val_RC@5: 0.5785 - val_RC@10: 0.6634 - lr: 9.7624e-05 - e_time: 54.7874 - 55s/epoch - 43ms/step
Epoch 25/1000
1292/1292 - 55s - loss: 1.0008 - NDCG@10: 0.5248 - MAE: 0.3172 - RC@5: 0.6069 - RC@10: 0.7002 - val_loss: 1.1168 - val_NDCG@10: 0.5065 - val_MAE: 0.3199 - val_RC@5: 0.5800 - val_RC@10: 0.6646 - lr: 9.7525e-05 - e_time: 54.8689 - 55s/epoch - 43ms/step
Epoch 26/1000
1292/1292 - 55s - loss: 0.9943 - NDCG@10: 0.5269 - MAE: 0.3134 - RC@5: 0.6097 - RC@10: 0.7028 - val_loss: 1.1154 - val_NDCG@10: 0.5077 - val_MAE: 0.3174 - val_RC@5: 0.5811 - val_RC@10: 0.6657 - lr: 9.7426e-05 - e_time: 54.8463 - 55s/epoch - 43ms/step
Epoch 27/1000
1292/1292 - 55s - loss: 0.9883 - NDCG@10: 0.5291 - MAE: 0.3101 - RC@5: 0.6122 - RC@10: 0.7053 - val_loss: 1.1144 - val_NDCG@10: 0.5086 - val_MAE: 0.3135 - val_RC@5: 0.5819 - val_RC@10: 0.6664 - lr: 9.7327e-05 - e_time: 54.8281 - 55s/epoch - 43ms/step
Epoch 28/1000
1292/1292 - 55s - loss: 0.9823 - NDCG@10: 0.5309 - MAE: 0.3070 - RC@5: 0.6146 - RC@10: 0.7075 - val_loss: 1.1133 - val_NDCG@10: 0.5093 - val_MAE: 0.3107 - val_RC@5: 0.5829 - val_RC@10: 0.6673 - lr: 9.7228e-05 - e_time: 54.8405 - 55s/epoch - 43ms/step
Epoch 29/1000
1292/1292 - 55s - loss: 0.9769 - NDCG@10: 0.5328 - MAE: 0.3039 - RC@5: 0.6166 - RC@10: 0.7097 - val_loss: 1.1126 - val_NDCG@10: 0.5099 - val_MAE: 0.3062 - val_RC@5: 0.5836 - val_RC@10: 0.6680 - lr: 9.7129e-05 - e_time: 54.7279 - 55s/epoch - 43ms/step
Epoch 30/1000
1292/1292 - 55s - loss: 0.9713 - NDCG@10: 0.5345 - MAE: 0.3009 - RC@5: 0.6181 - RC@10: 0.7117 - val_loss: 1.1119 - val_NDCG@10: 0.5105 - val_MAE: 0.3037 - val_RC@5: 0.5848 - val_RC@10: 0.6683 - lr: 9.7030e-05 - e_time: 54.6776 - 55s/epoch - 43ms/step
Epoch 31/1000
1292/1292 - 55s - loss: 0.9662 - NDCG@10: 0.5356 - MAE: 0.2980 - RC@5: 0.6200 - RC@10: 0.7135 - val_loss: 1.1117 - val_NDCG@10: 0.5107 - val_MAE: 0.3002 - val_RC@5: 0.5844 - val_RC@10: 0.6685 - lr: 9.6931e-05 - e_time: 54.6594 - 55s/epoch - 43ms/step
Epoch 32/1000
1292/1292 - 55s - loss: 0.9613 - NDCG@10: 0.5377 - MAE: 0.2953 - RC@5: 0.6223 - RC@10: 0.7156 - val_loss: 1.1112 - val_NDCG@10: 0.5112 - val_MAE: 0.2980 - val_RC@5: 0.5853 - val_RC@10: 0.6691 - lr: 9.6832e-05 - e_time: 54.5707 - 55s/epoch - 43ms/step
Epoch 33/1000
1292/1292 - 55s - loss: 0.9560 - NDCG@10: 0.5391 - MAE: 0.2925 - RC@5: 0.6239 - RC@10: 0.7176 - val_loss: 1.1113 - val_NDCG@10: 0.5116 - val_MAE: 0.2945 - val_RC@5: 0.5861 - val_RC@10: 0.6694 - lr: 9.6733e-05 - e_time: 54.6899 - 55s/epoch - 42ms/step
Epoch 34/1000
1292/1292 - 55s - loss: 0.9513 - NDCG@10: 0.5408 - MAE: 0.2901 - RC@5: 0.6263 - RC@10: 0.7198 - val_loss: 1.1113 - val_NDCG@10: 0.5121 - val_MAE: 0.2917 - val_RC@5: 0.5873 - val_RC@10: 0.6699 - lr: 9.6634e-05 - e_time: 54.8321 - 55s/epoch - 42ms/step
Epoch 35/1000
1292/1292 - 55s - loss: 0.9471 - NDCG@10: 0.5418 - MAE: 0.2877 - RC@5: 0.6276 - RC@10: 0.7211 - val_loss: 1.1119 - val_NDCG@10: 0.5120 - val_MAE: 0.2877 - val_RC@5: 0.5867 - val_RC@10: 0.6698 - lr: 9.6535e-05 - e_time: 54.7729 - 55s/epoch - 42ms/step
Epoch 36/1000
1292/1292 - 55s - loss: 0.9425 - NDCG@10: 0.5428 - MAE: 0.2853 - RC@5: 0.6287 - RC@10: 0.7224 - val_loss: 1.1110 - val_NDCG@10: 0.5114 - val_MAE: 0.2875 - val_RC@5: 0.5859 - val_RC@10: 0.6700 - lr: 9.6436e-05 - e_time: 54.7977 - 55s/epoch - 43ms/step
Epoch 37/1000
1292/1292 - 55s - loss: 0.9383 - NDCG@10: 0.5442 - MAE: 0.2832 - RC@5: 0.6301 - RC@10: 0.7242 - val_loss: 1.1115 - val_NDCG@10: 0.5127 - val_MAE: 0.2842 - val_RC@5: 0.5869 - val_RC@10: 0.6710 - lr: 9.6337e-05 - e_time: 54.7937 - 55s/epoch - 42ms/step
Epoch 38/1000
1292/1292 - 55s - loss: 0.9343 - NDCG@10: 0.5456 - MAE: 0.2809 - RC@5: 0.6321 - RC@10: 0.7259 - val_loss: 1.1128 - val_NDCG@10: 0.5117 - val_MAE: 0.2799 - val_RC@5: 0.5861 - val_RC@10: 0.6697 - lr: 9.6238e-05 - e_time: 54.8064 - 55s/epoch - 42ms/step
Epoch 39/1000
1292/1292 - 55s - loss: 0.9301 - NDCG@10: 0.5462 - MAE: 0.2788 - RC@5: 0.6333 - RC@10: 0.7272 - val_loss: 1.1121 - val_NDCG@10: 0.5118 - val_MAE: 0.2797 - val_RC@5: 0.5869 - val_RC@10: 0.6702 - lr: 9.6139e-05 - e_time: 54.8086 - 55s/epoch - 42ms/step
Epoch 40/1000
1292/1292 - 55s - loss: 0.9265 - NDCG@10: 0.5473 - MAE: 0.2769 - RC@5: 0.6342 - RC@10: 0.7279 - val_loss: 1.1125 - val_NDCG@10: 0.5124 - val_MAE: 0.2777 - val_RC@5: 0.5870 - val_RC@10: 0.6704 - lr: 9.6040e-05 - e_time: 54.8090 - 55s/epoch - 42ms/step
Epoch 41/1000
1292/1292 - 55s - loss: 0.9229 - NDCG@10: 0.5483 - MAE: 0.2748 - RC@5: 0.6356 - RC@10: 0.7298 - val_loss: 1.1128 - val_NDCG@10: 0.5126 - val_MAE: 0.2750 - val_RC@5: 0.5872 - val_RC@10: 0.6700 - lr: 9.5941e-05 - e_time: 54.7928 - 55s/epoch - 42ms/step
Epoch 42/1000
1292/1292 - 55s - loss: 0.9189 - NDCG@10: 0.5494 - MAE: 0.2730 - RC@5: 0.6365 - RC@10: 0.7312 - val_loss: 1.1130 - val_NDCG@10: 0.5125 - val_MAE: 0.2741 - val_RC@5: 0.5866 - val_RC@10: 0.6700 - lr: 9.5842e-05 - e_time: 54.7294 - 55s/epoch - 42ms/step
Epoch 43/1000
1292/1292 - 55s - loss: 0.9158 - NDCG@10: 0.5500 - MAE: 0.2713 - RC@5: 0.6373 - RC@10: 0.7322 - val_loss: 1.1145 - val_NDCG@10: 0.5128 - val_MAE: 0.2703 - val_RC@5: 0.5867 - val_RC@10: 0.6705 - lr: 9.5743e-05 - e_time: 54.6719 - 55s/epoch - 42ms/step
Epoch 44/1000
1292/1292 - 55s - loss: 0.9118 - NDCG@10: 0.5514 - MAE: 0.2693 - RC@5: 0.6395 - RC@10: 0.7339 - val_loss: 1.1141 - val_NDCG@10: 0.5129 - val_MAE: 0.2702 - val_RC@5: 0.5871 - val_RC@10: 0.6706 - lr: 9.5644e-05 - e_time: 54.6936 - 55s/epoch - 42ms/step
Epoch 45/1000
1292/1292 - 55s - loss: 0.9088 - NDCG@10: 0.5518 - MAE: 0.2678 - RC@5: 0.6400 - RC@10: 0.7344 - val_loss: 1.1148 - val_NDCG@10: 0.5127 - val_MAE: 0.2674 - val_RC@5: 0.5872 - val_RC@10: 0.6703 - lr: 9.5545e-05 - e_time: 54.7250 - 55s/epoch - 42ms/step
Epoch 46/1000
1292/1292 - 55s - loss: 0.9057 - NDCG@10: 0.5533 - MAE: 0.2663 - RC@5: 0.6415 - RC@10: 0.7360 - val_loss: 1.1146 - val_NDCG@10: 0.5127 - val_MAE: 0.2667 - val_RC@5: 0.5867 - val_RC@10: 0.6696 - lr: 9.5446e-05 - e_time: 54.7357 - 55s/epoch - 42ms/step
Epoch 46: early stopping
[92m[INFO] Loading best model...[0m
WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name='input_1'), name='input_1', description="created by layer 'input_1'"), but it was called on an input with incompatible shape (None,).
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:991: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:991: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(
(1985, 2)
[92m[QUERY] 'Quiero comer un arroz con bogavante y con buenas vistas'[0m
Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 101, in <module>
    mdl.evaluate_text("Quiero comer un arroz con bogavante y con buenas vistas")
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/ATT2VAL.py", line 210, in evaluate_text
    lstm_text_complete = [list(map(lambda x: self.DATASET.DATA["TEXT_TOKENIZER"].word_index[x], text_prepro.split(" ")))]
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/ATT2VAL.py", line 210, in <lambda>
    lstm_text_complete = [list(map(lambda x: self.DATASET.DATA["TEXT_TOKENIZER"].word_index[x], text_prepro.split(" ")))]
KeyError: 'quiero'
2023-05-30 23:23:03.237099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "ATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 all_words (Embedding)          (None, 200, 128)     1544704     ['input_1[0][0]']                
                                                                                                  
 all_items (Embedding)          (None, 1985, 128)    254080      ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 200, 128)     0           ['all_words[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 128)    0           ['all_items[0][0]']              
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 200, 128)     0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 1985, 128)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 dot_mul (Lambda)               (None, 200, 1985)    0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 dot_mask (Lambda)              (None, 200, 1985)    0           ['dot_mul[0][0]',                
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['dot_mask[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 1,798,784
Trainable params: 1,798,784
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-05-30 23:23:09.768592: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
2023-05-30 23:23:13.326288: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
Epoch 1/1000
1292/1292 - 44s - loss: 2.5407 - NDCG@10: 0.0495 - MAE: 0.5336 - RC@5: 0.0597 - RC@10: 0.0875 - val_loss: 2.3531 - val_NDCG@10: 0.0670 - val_MAE: 0.6240 - val_RC@5: 0.0774 - val_RC@10: 0.1118 - lr: 4.9951e-05 - e_time: 44.0152 - 44s/epoch - 34ms/step
Epoch 2/1000
1292/1292 - 42s - loss: 2.2545 - NDCG@10: 0.0886 - MAE: 0.6466 - RC@5: 0.1020 - RC@10: 0.1482 - val_loss: 2.1256 - val_NDCG@10: 0.1263 - val_MAE: 0.6260 - val_RC@5: 0.1436 - val_RC@10: 0.1986 - lr: 4.9901e-05 - e_time: 41.5407 - 42s/epoch - 32ms/step
Epoch 3/1000
1292/1292 - 42s - loss: 2.0386 - NDCG@10: 0.1603 - MAE: 0.6091 - RC@5: 0.1850 - RC@10: 0.2502 - val_loss: 1.9317 - val_NDCG@10: 0.2048 - val_MAE: 0.5663 - val_RC@5: 0.2343 - val_RC@10: 0.3077 - lr: 4.9852e-05 - e_time: 41.6108 - 42s/epoch - 32ms/step
Epoch 4/1000
1292/1292 - 42s - loss: 1.8546 - NDCG@10: 0.2281 - MAE: 0.5636 - RC@5: 0.2658 - RC@10: 0.3433 - val_loss: 1.7756 - val_NDCG@10: 0.2614 - val_MAE: 0.5274 - val_RC@5: 0.3021 - val_RC@10: 0.3840 - lr: 4.9802e-05 - e_time: 41.6588 - 42s/epoch - 32ms/step
Epoch 5/1000
1292/1292 - 42s - loss: 1.7230 - NDCG@10: 0.2713 - MAE: 0.5158 - RC@5: 0.3175 - RC@10: 0.4033 - val_loss: 1.6764 - val_NDCG@10: 0.2963 - val_MAE: 0.4765 - val_RC@5: 0.3440 - val_RC@10: 0.4324 - lr: 4.9753e-05 - e_time: 41.7012 - 42s/epoch - 32ms/step
Epoch 6/1000
1292/1292 - 42s - loss: 1.6336 - NDCG@10: 0.3007 - MAE: 0.4664 - RC@5: 0.3526 - RC@10: 0.4444 - val_loss: 1.6073 - val_NDCG@10: 0.3222 - val_MAE: 0.4309 - val_RC@5: 0.3751 - val_RC@10: 0.4671 - lr: 4.9703e-05 - e_time: 41.7149 - 42s/epoch - 32ms/step
Epoch 7/1000
1292/1292 - 42s - loss: 1.5678 - NDCG@10: 0.3237 - MAE: 0.4262 - RC@5: 0.3803 - RC@10: 0.4752 - val_loss: 1.5564 - val_NDCG@10: 0.3415 - val_MAE: 0.3953 - val_RC@5: 0.3976 - val_RC@10: 0.4929 - lr: 4.9654e-05 - e_time: 41.6390 - 42s/epoch - 32ms/step
Epoch 8/1000
1292/1292 - 42s - loss: 1.5163 - NDCG@10: 0.3410 - MAE: 0.3957 - RC@5: 0.4009 - RC@10: 0.4981 - val_loss: 1.5172 - val_NDCG@10: 0.3571 - val_MAE: 0.3684 - val_RC@5: 0.4169 - val_RC@10: 0.5129 - lr: 4.9604e-05 - e_time: 41.7517 - 42s/epoch - 32ms/step
Epoch 9/1000
1292/1292 - 42s - loss: 1.4749 - NDCG@10: 0.3560 - MAE: 0.3713 - RC@5: 0.4192 - RC@10: 0.5178 - val_loss: 1.4861 - val_NDCG@10: 0.3702 - val_MAE: 0.3461 - val_RC@5: 0.4322 - val_RC@10: 0.5295 - lr: 4.9555e-05 - e_time: 41.9530 - 42s/epoch - 33ms/step
Epoch 10/1000
1292/1292 - 42s - loss: 1.4405 - NDCG@10: 0.3688 - MAE: 0.3513 - RC@5: 0.4336 - RC@10: 0.5343 - val_loss: 1.4610 - val_NDCG@10: 0.3815 - val_MAE: 0.3275 - val_RC@5: 0.4455 - val_RC@10: 0.5433 - lr: 4.9505e-05 - e_time: 42.0290 - 42s/epoch - 33ms/step
Epoch 11/1000
1292/1292 - 42s - loss: 1.4114 - NDCG@10: 0.3795 - MAE: 0.3348 - RC@5: 0.4461 - RC@10: 0.5474 - val_loss: 1.4397 - val_NDCG@10: 0.3910 - val_MAE: 0.3148 - val_RC@5: 0.4560 - val_RC@10: 0.5542 - lr: 4.9456e-05 - e_time: 41.9959 - 42s/epoch - 33ms/step
Epoch 12/1000
1292/1292 - 42s - loss: 1.3852 - NDCG@10: 0.3891 - MAE: 0.3205 - RC@5: 0.4572 - RC@10: 0.5592 - val_loss: 1.4223 - val_NDCG@10: 0.4002 - val_MAE: 0.3008 - val_RC@5: 0.4670 - val_RC@10: 0.5644 - lr: 4.9406e-05 - e_time: 42.0191 - 42s/epoch - 33ms/step
Epoch 13/1000
1292/1292 - 42s - loss: 1.3629 - NDCG@10: 0.3976 - MAE: 0.3081 - RC@5: 0.4668 - RC@10: 0.5692 - val_loss: 1.4076 - val_NDCG@10: 0.4076 - val_MAE: 0.2875 - val_RC@5: 0.4756 - val_RC@10: 0.5727 - lr: 4.9357e-05 - e_time: 42.1510 - 42s/epoch - 33ms/step
Epoch 14/1000
1292/1292 - 42s - loss: 1.3427 - NDCG@10: 0.4055 - MAE: 0.2977 - RC@5: 0.4757 - RC@10: 0.5788 - val_loss: 1.3947 - val_NDCG@10: 0.4144 - val_MAE: 0.2789 - val_RC@5: 0.4843 - val_RC@10: 0.5796 - lr: 4.9307e-05 - e_time: 42.0690 - 42s/epoch - 33ms/step
Epoch 15/1000
1292/1292 - 42s - loss: 1.3249 - NDCG@10: 0.4123 - MAE: 0.2879 - RC@5: 0.4836 - RC@10: 0.5861 - val_loss: 1.3838 - val_NDCG@10: 0.4203 - val_MAE: 0.2702 - val_RC@5: 0.4909 - val_RC@10: 0.5860 - lr: 4.9258e-05 - e_time: 42.0200 - 42s/epoch - 33ms/step
Epoch 16/1000
1292/1292 - 42s - loss: 1.3080 - NDCG@10: 0.4185 - MAE: 0.2799 - RC@5: 0.4906 - RC@10: 0.5938 - val_loss: 1.3739 - val_NDCG@10: 0.4253 - val_MAE: 0.2644 - val_RC@5: 0.4962 - val_RC@10: 0.5915 - lr: 4.9208e-05 - e_time: 42.1180 - 42s/epoch - 33ms/step
Epoch 17/1000
1292/1292 - 42s - loss: 1.2931 - NDCG@10: 0.4238 - MAE: 0.2721 - RC@5: 0.4968 - RC@10: 0.6002 - val_loss: 1.3660 - val_NDCG@10: 0.4299 - val_MAE: 0.2556 - val_RC@5: 0.5010 - val_RC@10: 0.5966 - lr: 4.9159e-05 - e_time: 42.0167 - 42s/epoch - 33ms/step
Epoch 18/1000
1292/1292 - 42s - loss: 1.2794 - NDCG@10: 0.4290 - MAE: 0.2657 - RC@5: 0.5031 - RC@10: 0.6059 - val_loss: 1.3584 - val_NDCG@10: 0.4350 - val_MAE: 0.2501 - val_RC@5: 0.5051 - val_RC@10: 0.6015 - lr: 4.9109e-05 - e_time: 41.8707 - 42s/epoch - 33ms/step
Epoch 19/1000
1292/1292 - 42s - loss: 1.2666 - NDCG@10: 0.4342 - MAE: 0.2595 - RC@5: 0.5084 - RC@10: 0.6118 - val_loss: 1.3523 - val_NDCG@10: 0.4388 - val_MAE: 0.2430 - val_RC@5: 0.5101 - val_RC@10: 0.6051 - lr: 4.9060e-05 - e_time: 41.8325 - 42s/epoch - 33ms/step
Epoch 20/1000
1292/1292 - 42s - loss: 1.2548 - NDCG@10: 0.4378 - MAE: 0.2539 - RC@5: 0.5129 - RC@10: 0.6165 - val_loss: 1.3464 - val_NDCG@10: 0.4429 - val_MAE: 0.2392 - val_RC@5: 0.5142 - val_RC@10: 0.6095 - lr: 4.9010e-05 - e_time: 41.6469 - 42s/epoch - 32ms/step
Epoch 21/1000
1292/1292 - 42s - loss: 1.2438 - NDCG@10: 0.4422 - MAE: 0.2489 - RC@5: 0.5175 - RC@10: 0.6214 - val_loss: 1.3415 - val_NDCG@10: 0.4468 - val_MAE: 0.2335 - val_RC@5: 0.5182 - val_RC@10: 0.6129 - lr: 4.8961e-05 - e_time: 41.6498 - 42s/epoch - 32ms/step
Epoch 22/1000
1292/1292 - 42s - loss: 1.2330 - NDCG@10: 0.4460 - MAE: 0.2442 - RC@5: 0.5219 - RC@10: 0.6255 - val_loss: 1.3363 - val_NDCG@10: 0.4493 - val_MAE: 0.2309 - val_RC@5: 0.5214 - val_RC@10: 0.6154 - lr: 4.8911e-05 - e_time: 41.5956 - 42s/epoch - 32ms/step
Epoch 23/1000
1292/1292 - 42s - loss: 1.2234 - NDCG@10: 0.4499 - MAE: 0.2398 - RC@5: 0.5264 - RC@10: 0.6299 - val_loss: 1.3327 - val_NDCG@10: 0.4525 - val_MAE: 0.2260 - val_RC@5: 0.5248 - val_RC@10: 0.6180 - lr: 4.8862e-05 - e_time: 41.6737 - 42s/epoch - 32ms/step
Epoch 24/1000
1292/1292 - 42s - loss: 1.2139 - NDCG@10: 0.4531 - MAE: 0.2355 - RC@5: 0.5301 - RC@10: 0.6332 - val_loss: 1.3297 - val_NDCG@10: 0.4558 - val_MAE: 0.2200 - val_RC@5: 0.5275 - val_RC@10: 0.6210 - lr: 4.8812e-05 - e_time: 41.5907 - 42s/epoch - 32ms/step
Epoch 25/1000
1292/1292 - 42s - loss: 1.2050 - NDCG@10: 0.4567 - MAE: 0.2319 - RC@5: 0.5337 - RC@10: 0.6374 - val_loss: 1.3266 - val_NDCG@10: 0.4588 - val_MAE: 0.2166 - val_RC@5: 0.5304 - val_RC@10: 0.6234 - lr: 4.8763e-05 - e_time: 41.6713 - 42s/epoch - 32ms/step
Epoch 26/1000
1292/1292 - 42s - loss: 1.1965 - NDCG@10: 0.4597 - MAE: 0.2285 - RC@5: 0.5369 - RC@10: 0.6404 - val_loss: 1.3231 - val_NDCG@10: 0.4605 - val_MAE: 0.2153 - val_RC@5: 0.5323 - val_RC@10: 0.6254 - lr: 4.8713e-05 - e_time: 41.5954 - 42s/epoch - 32ms/step
Epoch 27/1000
1292/1292 - 42s - loss: 1.1886 - NDCG@10: 0.4618 - MAE: 0.2251 - RC@5: 0.5397 - RC@10: 0.6431 - val_loss: 1.3208 - val_NDCG@10: 0.4626 - val_MAE: 0.2120 - val_RC@5: 0.5351 - val_RC@10: 0.6278 - lr: 4.8664e-05 - e_time: 41.6174 - 42s/epoch - 32ms/step
Epoch 28/1000
1292/1292 - 42s - loss: 1.1804 - NDCG@10: 0.4645 - MAE: 0.2217 - RC@5: 0.5425 - RC@10: 0.6459 - val_loss: 1.3190 - val_NDCG@10: 0.4650 - val_MAE: 0.2079 - val_RC@5: 0.5374 - val_RC@10: 0.6301 - lr: 4.8614e-05 - e_time: 41.6751 - 42s/epoch - 32ms/step
Epoch 29/1000
1292/1292 - 42s - loss: 1.1732 - NDCG@10: 0.4669 - MAE: 0.2187 - RC@5: 0.5450 - RC@10: 0.6492 - val_loss: 1.3166 - val_NDCG@10: 0.4663 - val_MAE: 0.2057 - val_RC@5: 0.5387 - val_RC@10: 0.6313 - lr: 4.8565e-05 - e_time: 41.6155 - 42s/epoch - 32ms/step
Epoch 30/1000
1292/1292 - 42s - loss: 1.1662 - NDCG@10: 0.4695 - MAE: 0.2158 - RC@5: 0.5482 - RC@10: 0.6517 - val_loss: 1.3158 - val_NDCG@10: 0.4685 - val_MAE: 0.2013 - val_RC@5: 0.5411 - val_RC@10: 0.6332 - lr: 4.8515e-05 - e_time: 41.6883 - 42s/epoch - 32ms/step
Epoch 31/1000
1292/1292 - 42s - loss: 1.1589 - NDCG@10: 0.4715 - MAE: 0.2132 - RC@5: 0.5504 - RC@10: 0.6537 - val_loss: 1.3137 - val_NDCG@10: 0.4700 - val_MAE: 0.1997 - val_RC@5: 0.5427 - val_RC@10: 0.6342 - lr: 4.8466e-05 - e_time: 41.5917 - 42s/epoch - 32ms/step
Epoch 32/1000
1292/1292 - 42s - loss: 1.1528 - NDCG@10: 0.4737 - MAE: 0.2107 - RC@5: 0.5528 - RC@10: 0.6563 - val_loss: 1.3130 - val_NDCG@10: 0.4724 - val_MAE: 0.1961 - val_RC@5: 0.5446 - val_RC@10: 0.6361 - lr: 4.8416e-05 - e_time: 41.5959 - 42s/epoch - 32ms/step
Epoch 33/1000
1292/1292 - 42s - loss: 1.1464 - NDCG@10: 0.4754 - MAE: 0.2081 - RC@5: 0.5547 - RC@10: 0.6581 - val_loss: 1.3104 - val_NDCG@10: 0.4729 - val_MAE: 0.1972 - val_RC@5: 0.5448 - val_RC@10: 0.6372 - lr: 4.8367e-05 - e_time: 41.6049 - 42s/epoch - 32ms/step
Epoch 34/1000
1292/1292 - 42s - loss: 1.1403 - NDCG@10: 0.4775 - MAE: 0.2059 - RC@5: 0.5566 - RC@10: 0.6606 - val_loss: 1.3100 - val_NDCG@10: 0.4745 - val_MAE: 0.1930 - val_RC@5: 0.5467 - val_RC@10: 0.6381 - lr: 4.8317e-05 - e_time: 41.5222 - 42s/epoch - 32ms/step
Epoch 35/1000
1292/1292 - 42s - loss: 1.1345 - NDCG@10: 0.4795 - MAE: 0.2033 - RC@5: 0.5588 - RC@10: 0.6625 - val_loss: 1.3101 - val_NDCG@10: 0.4765 - val_MAE: 0.1894 - val_RC@5: 0.5484 - val_RC@10: 0.6399 - lr: 4.8268e-05 - e_time: 41.6343 - 42s/epoch - 32ms/step
Epoch 36/1000
1292/1292 - 42s - loss: 1.1293 - NDCG@10: 0.4810 - MAE: 0.2014 - RC@5: 0.5607 - RC@10: 0.6643 - val_loss: 1.3089 - val_NDCG@10: 0.4776 - val_MAE: 0.1881 - val_RC@5: 0.5502 - val_RC@10: 0.6407 - lr: 4.8218e-05 - e_time: 41.7435 - 42s/epoch - 32ms/step
Epoch 37/1000
1292/1292 - 42s - loss: 1.1242 - NDCG@10: 0.4826 - MAE: 0.1998 - RC@5: 0.5625 - RC@10: 0.6662 - val_loss: 1.3083 - val_NDCG@10: 0.4782 - val_MAE: 0.1856 - val_RC@5: 0.5504 - val_RC@10: 0.6418 - lr: 4.8169e-05 - e_time: 41.6385 - 42s/epoch - 32ms/step
Epoch 38/1000
1292/1292 - 42s - loss: 1.1187 - NDCG@10: 0.4839 - MAE: 0.1973 - RC@5: 0.5639 - RC@10: 0.6678 - val_loss: 1.3072 - val_NDCG@10: 0.4791 - val_MAE: 0.1854 - val_RC@5: 0.5516 - val_RC@10: 0.6424 - lr: 4.8119e-05 - e_time: 41.6462 - 42s/epoch - 32ms/step
Epoch 39/1000
1292/1292 - 42s - loss: 1.1139 - NDCG@10: 0.4856 - MAE: 0.1956 - RC@5: 0.5664 - RC@10: 0.6696 - val_loss: 1.3073 - val_NDCG@10: 0.4805 - val_MAE: 0.1828 - val_RC@5: 0.5528 - val_RC@10: 0.6435 - lr: 4.8070e-05 - e_time: 41.6411 - 42s/epoch - 32ms/step
Epoch 40/1000
2023-05-30 23:53:50.512893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Model: "ATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 all_words (Embedding)          (None, 200, 256)     3089408     ['input_1[0][0]']                
                                                                                                  
 all_items (Embedding)          (None, 1985, 256)    508160      ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 word_emb (Lambda)              (None, 200, 256)     0           ['all_words[0][0]']              
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 256)    0           ['all_items[0][0]']              
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 dropout (Dropout)              (None, 200, 256)     0           ['word_emb[0][0]']               
                                                                                                  
 dropout_1 (Dropout)            (None, 1985, 256)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 dot_mul (Lambda)               (None, 200, 1985)    0           ['dropout[0][0]',                
                                                                  'dropout_1[0][0]']              
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 dot_mask (Lambda)              (None, 200, 1985)    0           ['dot_mul[0][0]',                
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['dot_mask[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,597,568
Trainable params: 3,597,568
Non-trainable params: 0
__________________________________________________________________________________________________
None
2023-05-30 23:53:57.277156: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
2023-05-30 23:54:00.809924: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
Epoch 1/1000
1292/1292 - 65s - loss: 2.4922 - NDCG@10: 0.0621 - MAE: 0.5163 - RC@5: 0.0720 - RC@10: 0.1021 - val_loss: 2.2710 - val_NDCG@10: 0.0913 - val_MAE: 0.5710 - val_RC@5: 0.1039 - val_RC@10: 0.1465 - lr: 4.9951e-05 - e_time: 64.6515 - 65s/epoch - 50ms/step
Epoch 2/1000
1292/1292 - 63s - loss: 2.1391 - NDCG@10: 0.1426 - MAE: 0.5836 - RC@5: 0.1637 - RC@10: 0.2216 - val_loss: 1.9926 - val_NDCG@10: 0.2030 - val_MAE: 0.5440 - val_RC@5: 0.2326 - val_RC@10: 0.3007 - lr: 4.9901e-05 - e_time: 62.1875 - 63s/epoch - 48ms/step
Epoch 3/1000
1292/1292 - 63s - loss: 1.8894 - NDCG@10: 0.2403 - MAE: 0.5371 - RC@5: 0.2797 - RC@10: 0.3538 - val_loss: 1.7887 - val_NDCG@10: 0.2840 - val_MAE: 0.4980 - val_RC@5: 0.3290 - val_RC@10: 0.4068 - lr: 4.9852e-05 - e_time: 62.5849 - 63s/epoch - 49ms/step
Epoch 4/1000
1292/1292 - 63s - loss: 1.7065 - NDCG@10: 0.3025 - MAE: 0.4896 - RC@5: 0.3528 - RC@10: 0.4371 - val_loss: 1.6553 - val_NDCG@10: 0.3281 - val_MAE: 0.4437 - val_RC@5: 0.3800 - val_RC@10: 0.4659 - lr: 4.9802e-05 - e_time: 62.9088 - 63s/epoch - 49ms/step
Epoch 5/1000
1292/1292 - 64s - loss: 1.5890 - NDCG@10: 0.3396 - MAE: 0.4313 - RC@5: 0.3967 - RC@10: 0.4879 - val_loss: 1.5737 - val_NDCG@10: 0.3573 - val_MAE: 0.3899 - val_RC@5: 0.4145 - val_RC@10: 0.5042 - lr: 4.9753e-05 - e_time: 63.1107 - 64s/epoch - 49ms/step
Epoch 6/1000
1292/1292 - 63s - loss: 1.5085 - NDCG@10: 0.3658 - MAE: 0.3869 - RC@5: 0.4277 - RC@10: 0.5218 - val_loss: 1.5178 - val_NDCG@10: 0.3775 - val_MAE: 0.3542 - val_RC@5: 0.4386 - val_RC@10: 0.5302 - lr: 4.9703e-05 - e_time: 62.9440 - 63s/epoch - 49ms/step
Epoch 7/1000
1292/1292 - 63s - loss: 1.4481 - NDCG@10: 0.3867 - MAE: 0.3536 - RC@5: 0.4522 - RC@10: 0.5489 - val_loss: 1.4774 - val_NDCG@10: 0.3948 - val_MAE: 0.3246 - val_RC@5: 0.4579 - val_RC@10: 0.5517 - lr: 4.9654e-05 - e_time: 62.9204 - 63s/epoch - 49ms/step
Epoch 8/1000
1292/1292 - 64s - loss: 1.4002 - NDCG@10: 0.4035 - MAE: 0.3279 - RC@5: 0.4718 - RC@10: 0.5691 - val_loss: 1.4458 - val_NDCG@10: 0.4080 - val_MAE: 0.3046 - val_RC@5: 0.4730 - val_RC@10: 0.5673 - lr: 4.9604e-05 - e_time: 63.0941 - 64s/epoch - 49ms/step
Epoch 9/1000
1292/1292 - 64s - loss: 1.3608 - NDCG@10: 0.4175 - MAE: 0.3071 - RC@5: 0.4874 - RC@10: 0.5864 - val_loss: 1.4217 - val_NDCG@10: 0.4196 - val_MAE: 0.2846 - val_RC@5: 0.4850 - val_RC@10: 0.5801 - lr: 4.9555e-05 - e_time: 63.0952 - 64s/epoch - 49ms/step
Epoch 10/1000
1292/1292 - 64s - loss: 1.3269 - NDCG@10: 0.4300 - MAE: 0.2900 - RC@5: 0.5013 - RC@10: 0.6007 - val_loss: 1.4024 - val_NDCG@10: 0.4290 - val_MAE: 0.2690 - val_RC@5: 0.4972 - val_RC@10: 0.5898 - lr: 4.9505e-05 - e_time: 63.1934 - 64s/epoch - 49ms/step
Epoch 11/1000
1292/1292 - 64s - loss: 1.2978 - NDCG@10: 0.4402 - MAE: 0.2755 - RC@5: 0.5127 - RC@10: 0.6128 - val_loss: 1.3862 - val_NDCG@10: 0.4365 - val_MAE: 0.2582 - val_RC@5: 0.5055 - val_RC@10: 0.5982 - lr: 4.9456e-05 - e_time: 63.1537 - 64s/epoch - 49ms/step
Epoch 12/1000
1292/1292 - 64s - loss: 1.2717 - NDCG@10: 0.4489 - MAE: 0.2635 - RC@5: 0.5228 - RC@10: 0.6232 - val_loss: 1.3736 - val_NDCG@10: 0.4434 - val_MAE: 0.2464 - val_RC@5: 0.5135 - val_RC@10: 0.6060 - lr: 4.9406e-05 - e_time: 63.1368 - 64s/epoch - 49ms/step
Epoch 13/1000
1292/1292 - 63s - loss: 1.2482 - NDCG@10: 0.4571 - MAE: 0.2526 - RC@5: 0.5318 - RC@10: 0.6326 - val_loss: 1.3630 - val_NDCG@10: 0.4502 - val_MAE: 0.2368 - val_RC@5: 0.5205 - val_RC@10: 0.6124 - lr: 4.9357e-05 - e_time: 62.8917 - 63s/epoch - 49ms/step
Epoch 14/1000
1292/1292 - 63s - loss: 1.2274 - NDCG@10: 0.4644 - MAE: 0.2433 - RC@5: 0.5399 - RC@10: 0.6408 - val_loss: 1.3543 - val_NDCG@10: 0.4556 - val_MAE: 0.2284 - val_RC@5: 0.5266 - val_RC@10: 0.6172 - lr: 4.9307e-05 - e_time: 62.8460 - 63s/epoch - 49ms/step
Epoch 15/1000
1292/1292 - 63s - loss: 1.2077 - NDCG@10: 0.4712 - MAE: 0.2348 - RC@5: 0.5477 - RC@10: 0.6483 - val_loss: 1.3467 - val_NDCG@10: 0.4607 - val_MAE: 0.2218 - val_RC@5: 0.5319 - val_RC@10: 0.6221 - lr: 4.9258e-05 - e_time: 62.5966 - 63s/epoch - 49ms/step
Epoch 16/1000
1292/1292 - 63s - loss: 1.1903 - NDCG@10: 0.4774 - MAE: 0.2276 - RC@5: 0.5545 - RC@10: 0.6551 - val_loss: 1.3413 - val_NDCG@10: 0.4647 - val_MAE: 0.2138 - val_RC@5: 0.5361 - val_RC@10: 0.6262 - lr: 4.9208e-05 - e_time: 62.3052 - 63s/epoch - 49ms/step
Epoch 17/1000
1292/1292 - 63s - loss: 1.1736 - NDCG@10: 0.4828 - MAE: 0.2208 - RC@5: 0.5608 - RC@10: 0.6619 - val_loss: 1.3360 - val_NDCG@10: 0.4681 - val_MAE: 0.2081 - val_RC@5: 0.5396 - val_RC@10: 0.6295 - lr: 4.9159e-05 - e_time: 62.4782 - 63s/epoch - 49ms/step
Epoch 18/1000
1292/1292 - 63s - loss: 1.1580 - NDCG@10: 0.4874 - MAE: 0.2145 - RC@5: 0.5655 - RC@10: 0.6670 - val_loss: 1.3322 - val_NDCG@10: 0.4722 - val_MAE: 0.2020 - val_RC@5: 0.5437 - val_RC@10: 0.6336 - lr: 4.9109e-05 - e_time: 62.4534 - 63s/epoch - 49ms/step
Epoch 19/1000
1292/1292 - 63s - loss: 1.1442 - NDCG@10: 0.4919 - MAE: 0.2091 - RC@5: 0.5705 - RC@10: 0.6719 - val_loss: 1.3291 - val_NDCG@10: 0.4744 - val_MAE: 0.1965 - val_RC@5: 0.5461 - val_RC@10: 0.6354 - lr: 4.9060e-05 - e_time: 62.4984 - 63s/epoch - 49ms/step
Epoch 20/1000
1292/1292 - 63s - loss: 1.1307 - NDCG@10: 0.4959 - MAE: 0.2040 - RC@5: 0.5753 - RC@10: 0.6769 - val_loss: 1.3266 - val_NDCG@10: 0.4777 - val_MAE: 0.1915 - val_RC@5: 0.5487 - val_RC@10: 0.6386 - lr: 4.9010e-05 - e_time: 62.5765 - 63s/epoch - 49ms/step
Epoch 21/1000
1292/1292 - 63s - loss: 1.1181 - NDCG@10: 0.4996 - MAE: 0.1993 - RC@5: 0.5799 - RC@10: 0.6811 - val_loss: 1.3237 - val_NDCG@10: 0.4801 - val_MAE: 0.1881 - val_RC@5: 0.5510 - val_RC@10: 0.6409 - lr: 4.8961e-05 - e_time: 62.5845 - 63s/epoch - 49ms/step
Epoch 22/1000
1292/1292 - 63s - loss: 1.1059 - NDCG@10: 0.5034 - MAE: 0.1946 - RC@5: 0.5835 - RC@10: 0.6855 - val_loss: 1.3215 - val_NDCG@10: 0.4822 - val_MAE: 0.1855 - val_RC@5: 0.5532 - val_RC@10: 0.6430 - lr: 4.8911e-05 - e_time: 62.4856 - 63s/epoch - 49ms/step
Epoch 23/1000
1292/1292 - 62s - loss: 1.0944 - NDCG@10: 0.5067 - MAE: 0.1902 - RC@5: 0.5873 - RC@10: 0.6890 - val_loss: 1.3228 - val_NDCG@10: 0.4851 - val_MAE: 0.1778 - val_RC@5: 0.5558 - val_RC@10: 0.6449 - lr: 4.8862e-05 - e_time: 62.4373 - 62s/epoch - 48ms/step
Epoch 24/1000
1292/1292 - 63s - loss: 1.0835 - NDCG@10: 0.5102 - MAE: 0.1864 - RC@5: 0.5910 - RC@10: 0.6930 - val_loss: 1.3214 - val_NDCG@10: 0.4864 - val_MAE: 0.1746 - val_RC@5: 0.5569 - val_RC@10: 0.6459 - lr: 4.8812e-05 - e_time: 62.3972 - 63s/epoch - 49ms/step
Epoch 25/1000
1292/1292 - 63s - loss: 1.0731 - NDCG@10: 0.5130 - MAE: 0.1826 - RC@5: 0.5950 - RC@10: 0.6964 - val_loss: 1.3206 - val_NDCG@10: 0.4882 - val_MAE: 0.1714 - val_RC@5: 0.5589 - val_RC@10: 0.6477 - lr: 4.8763e-05 - e_time: 62.3599 - 63s/epoch - 49ms/step
Epoch 26/1000
1292/1292 - 62s - loss: 1.0631 - NDCG@10: 0.5160 - MAE: 0.1791 - RC@5: 0.5984 - RC@10: 0.6998 - val_loss: 1.3211 - val_NDCG@10: 0.4899 - val_MAE: 0.1674 - val_RC@5: 0.5606 - val_RC@10: 0.6493 - lr: 4.8713e-05 - e_time: 62.1455 - 62s/epoch - 48ms/step
Epoch 27/1000
1292/1292 - 62s - loss: 1.0545 - NDCG@10: 0.5182 - MAE: 0.1760 - RC@5: 0.6007 - RC@10: 0.7029 - val_loss: 1.3212 - val_NDCG@10: 0.4908 - val_MAE: 0.1645 - val_RC@5: 0.5621 - val_RC@10: 0.6500 - lr: 4.8664e-05 - e_time: 62.4639 - 62s/epoch - 48ms/step
Epoch 28/1000
1292/1292 - 63s - loss: 1.0451 - NDCG@10: 0.5213 - MAE: 0.1728 - RC@5: 0.6044 - RC@10: 0.7066 - val_loss: 1.3212 - val_NDCG@10: 0.4921 - val_MAE: 0.1618 - val_RC@5: 0.5635 - val_RC@10: 0.6511 - lr: 4.8614e-05 - e_time: 62.6788 - 63s/epoch - 49ms/step
Epoch 29/1000
1292/1292 - 63s - loss: 1.0368 - NDCG@10: 0.5236 - MAE: 0.1699 - RC@5: 0.6065 - RC@10: 0.7090 - val_loss: 1.3227 - val_NDCG@10: 0.4927 - val_MAE: 0.1579 - val_RC@5: 0.5638 - val_RC@10: 0.6514 - lr: 4.8565e-05 - e_time: 62.5175 - 63s/epoch - 48ms/step
Epoch 30/1000
1292/1292 - 63s - loss: 1.0287 - NDCG@10: 0.5255 - MAE: 0.1670 - RC@5: 0.6089 - RC@10: 0.7111 - val_loss: 1.3220 - val_NDCG@10: 0.4942 - val_MAE: 0.1567 - val_RC@5: 0.5647 - val_RC@10: 0.6525 - lr: 4.8515e-05 - e_time: 62.5131 - 63s/epoch - 48ms/step
Epoch 31/1000
1292/1292 - 62s - loss: 1.0208 - NDCG@10: 0.5278 - MAE: 0.1644 - RC@5: 0.6124 - RC@10: 0.7139 - val_loss: 1.3239 - val_NDCG@10: 0.4959 - val_MAE: 0.1529 - val_RC@5: 0.5668 - val_RC@10: 0.6536 - lr: 4.8466e-05 - e_time: 62.3637 - 62s/epoch - 48ms/step
Epoch 32/1000
1292/1292 - 62s - loss: 1.0134 - NDCG@10: 0.5298 - MAE: 0.1619 - RC@5: 0.6136 - RC@10: 0.7163 - val_loss: 1.3246 - val_NDCG@10: 0.4962 - val_MAE: 0.1509 - val_RC@5: 0.5671 - val_RC@10: 0.6546 - lr: 4.8416e-05 - e_time: 62.3749 - 62s/epoch - 48ms/step
Epoch 33/1000
1292/1292 - 62s - loss: 1.0063 - NDCG@10: 0.5316 - MAE: 0.1595 - RC@5: 0.6159 - RC@10: 0.7187 - val_loss: 1.3245 - val_NDCG@10: 0.4969 - val_MAE: 0.1495 - val_RC@5: 0.5681 - val_RC@10: 0.6549 - lr: 4.8367e-05 - e_time: 62.4092 - 62s/epoch - 48ms/step
Epoch 34/1000
1292/1292 - 63s - loss: 0.9993 - NDCG@10: 0.5334 - MAE: 0.1572 - RC@5: 0.6185 - RC@10: 0.7210 - val_loss: 1.3272 - val_NDCG@10: 0.4986 - val_MAE: 0.1455 - val_RC@5: 0.5699 - val_RC@10: 0.6559 - lr: 4.8317e-05 - e_time: 62.5403 - 63s/epoch - 48ms/step
Epoch 35/1000
1292/1292 - 62s - loss: 0.9926 - NDCG@10: 0.5353 - MAE: 0.1548 - RC@5: 0.6202 - RC@10: 0.7232 - val_loss: 1.3277 - val_NDCG@10: 0.4988 - val_MAE: 0.1440 - val_RC@5: 0.5694 - val_RC@10: 0.6560 - lr: 4.8268e-05 - e_time: 62.3398 - 62s/epoch - 48ms/step
Epoch 35: early stopping
[92m[INFO] Loading best model...[0m
WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name='input_1'), name='input_1', description="created by layer 'input_1'"), but it was called on an input with incompatible shape (None,).
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:991: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:991: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.
  warnings.warn(
(1985, 2)
[92m[QUERY] 'Quiero comer un arroz con bogavante y con buenas vistas'[0m
Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 102, in <module>
    mdl.evaluate_text("Quiero comer un arroz con bogavante y con buenas vistas")
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/ATT2VAL.py", line 213, in evaluate_text
    lstm_text_complete = [list(map(lambda x: self.DATASET.DATA["TEXT_TOKENIZER"].word_index[x], text_prepro.split(" ")))] 
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/ATT2VAL.py", line 213, in <lambda>
    lstm_text_complete = [list(map(lambda x: self.DATASET.DATA["TEXT_TOKENIZER"].word_index[x], text_prepro.split(" ")))] 
KeyError: 'quiero'
2023-05-31 18:23:49.556585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6
Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
Model: "BERTATT2VAL_0"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 200)]        0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1985)]       0           []                               
                                                                                                  
 all_items (Embedding)          (None, 1985, 768)    1524480     ['input_2[0][0]']                
                                                                                                  
 tf.math.not_equal (TFOpLambda)  (None, 200)         0           ['input_1[0][0]']                
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  167356416   ['input_1[0][0]']                
                                thPoolingAndCrossAt                                               
                                tentions(last_hidde                                               
                                n_state=(None, 200,                                               
                                 768),                                                            
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 rest_emb (Lambda)              (None, 1985, 768)    0           ['all_items[0][0]']              
                                                                                                  
 tf.cast (TFOpLambda)           (None, 200)          0           ['tf.math.not_equal[0][0]']      
                                                                                                  
 word_emb (Lambda)              (None, 200, 768)     0           ['tf_bert_model[0][0]']          
                                                                                                  
 dropout_37 (Dropout)           (None, 1985, 768)    0           ['rest_emb[0][0]']               
                                                                                                  
 tf.expand_dims (TFOpLambda)    (None, 200, 1)       0           ['tf.cast[0][0]']                
                                                                                                  
 dot_mul (Lambda)               (None, 200, 1985)    0           ['word_emb[0][0]',               
                                                                  'dropout_37[0][0]']             
                                                                                                  
 tf.tile (TFOpLambda)           (None, 200, 1985)    0           ['tf.expand_dims[0][0]']         
                                                                                                  
 dot_mask (Lambda)              (None, 200, 1985)    0           ['dot_mul[0][0]',                
                                                                  'tf.tile[0][0]']                
                                                                                                  
 dotprod (Activation)           (None, 200, 1985)    0           ['dot_mask[0][0]']               
                                                                                                  
 sum (Lambda)                   (None, 1985)         0           ['dotprod[0][0]']                
                                                                                                  
 activation (Activation)        (None, 1985)         0           ['sum[0][0]']                    
                                                                                                  
 out (Activation)               (None, 1985)         0           ['activation[0][0]']             
                                                                                                  
==================================================================================================
Total params: 168,880,896
Trainable params: 1,524,480
Non-trainable params: 167,356,416
__________________________________________________________________________________________________
None
2023-05-31 18:32:56.923572: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
2023-05-31 18:33:00.945381: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 5250348820 exceeds 10% of free system memory.
Epoch 1/1000
2584/2584 - 1421s - loss: 2.3395 - NDCG@10: 0.0255 - MAE: 0.0427 - RC@5: 0.0313 - RC@10: 0.0523 - val_loss: 2.3278 - val_NDCG@10: 0.0263 - val_MAE: 0.0278 - val_RC@5: 0.0302 - val_RC@10: 0.0557 - lr: 9.9901e-05 - e_time: 1414.6844 - 1421s/epoch - 550ms/step
Epoch 2/1000
2584/2584 - 1405s - loss: 2.3223 - NDCG@10: 0.0279 - MAE: 0.0281 - RC@5: 0.0329 - RC@10: 0.0591 - val_loss: 2.3300 - val_NDCG@10: 0.0258 - val_MAE: 0.0257 - val_RC@5: 0.0315 - val_RC@10: 0.0533 - lr: 9.9802e-05 - e_time: 1405.2208 - 1405s/epoch - 544ms/step
Epoch 3/1000
2584/2584 - 1404s - loss: 2.3225 - NDCG@10: 0.0281 - MAE: 0.0258 - RC@5: 0.0347 - RC@10: 0.0579 - val_loss: 2.3311 - val_NDCG@10: 0.0253 - val_MAE: 0.0241 - val_RC@5: 0.0318 - val_RC@10: 0.0518 - lr: 9.9703e-05 - e_time: 1404.0728 - 1404s/epoch - 543ms/step
Epoch 4/1000
2584/2584 - 1404s - loss: 2.3198 - NDCG@10: 0.0291 - MAE: 0.0253 - RC@5: 0.0361 - RC@10: 0.0599 - val_loss: 2.3299 - val_NDCG@10: 0.0257 - val_MAE: 0.0242 - val_RC@5: 0.0318 - val_RC@10: 0.0526 - lr: 9.9604e-05 - e_time: 1403.9409 - 1404s/epoch - 543ms/step
Epoch 5/1000
2584/2584 - 1404s - loss: 2.3177 - NDCG@10: 0.0301 - MAE: 0.0245 - RC@5: 0.0378 - RC@10: 0.0614 - val_loss: 2.3304 - val_NDCG@10: 0.0256 - val_MAE: 0.0241 - val_RC@5: 0.0320 - val_RC@10: 0.0515 - lr: 9.9505e-05 - e_time: 1404.2541 - 1404s/epoch - 543ms/step
Epoch 6/1000
2584/2584 - 1405s - loss: 2.3201 - NDCG@10: 0.0297 - MAE: 0.0219 - RC@5: 0.0399 - RC@10: 0.0583 - val_loss: 2.3354 - val_NDCG@10: 0.0242 - val_MAE: 0.0211 - val_RC@5: 0.0338 - val_RC@10: 0.0465 - lr: 9.9406e-05 - e_time: 1404.5252 - 1405s/epoch - 544ms/step
Epoch 7/1000
2584/2584 - 1405s - loss: 2.3210 - NDCG@10: 0.0296 - MAE: 0.0208 - RC@5: 0.0408 - RC@10: 0.0568 - val_loss: 2.3358 - val_NDCG@10: 0.0234 - val_MAE: 0.0225 - val_RC@5: 0.0297 - val_RC@10: 0.0468 - lr: 9.9307e-05 - e_time: 1404.5278 - 1405s/epoch - 544ms/step
Epoch 8/1000
2584/2584 - 1403s - loss: 2.3202 - NDCG@10: 0.0299 - MAE: 0.0218 - RC@5: 0.0401 - RC@10: 0.0583 - val_loss: 2.3371 - val_NDCG@10: 0.0233 - val_MAE: 0.0207 - val_RC@5: 0.0324 - val_RC@10: 0.0450 - lr: 9.9208e-05 - e_time: 1403.2796 - 1403s/epoch - 543ms/step
Epoch 9/1000
2584/2584 - 1404s - loss: 2.3198 - NDCG@10: 0.0305 - MAE: 0.0211 - RC@5: 0.0420 - RC@10: 0.0582 - val_loss: 2.3396 - val_NDCG@10: 0.0215 - val_MAE: 0.0164 - val_RC@5: 0.0310 - val_RC@10: 0.0392 - lr: 9.9109e-05 - e_time: 1403.7567 - 1404s/epoch - 543ms/step
Epoch 10/1000
2584/2584 - 1404s - loss: 2.3196 - NDCG@10: 0.0316 - MAE: 0.0182 - RC@5: 0.0448 - RC@10: 0.0561 - val_loss: 2.3377 - val_NDCG@10: 0.0228 - val_MAE: 0.0177 - val_RC@5: 0.0320 - val_RC@10: 0.0416 - lr: 9.9010e-05 - e_time: 1403.9057 - 1404s/epoch - 543ms/step
Epoch 11/1000
2584/2584 - 1404s - loss: 2.3184 - NDCG@10: 0.0314 - MAE: 0.0193 - RC@5: 0.0449 - RC@10: 0.0577 - val_loss: 2.3402 - val_NDCG@10: 0.0215 - val_MAE: 0.0182 - val_RC@5: 0.0297 - val_RC@10: 0.0399 - lr: 9.8911e-05 - e_time: 1404.0290 - 1404s/epoch - 543ms/step
Epoch 11: early stopping
[92m[INFO] Loading best model...[0m
WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.int32, name='input_1'), name='input_1', description="created by layer 'input_1'"), but it was called on an input with incompatible shape (None,).
Traceback (most recent call last):
  File "/media/nas/pperez/code/TAVtext/Pruebas.py", line 106, in <module>
    mdl.emb_tsne()
  File "/media/nas/pperez/code/TAVtext/src/models/text_models/new/BERTATT2VAL.py", line 140, in emb_tsne
    wrd_embs = wrd_embs.predict(list(range(self.DATASET.DATA["VOCAB_SIZE"])), verbose=0).numpy()
  File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/tmp/__autograph_generated_filebya69zcd.py", line 15, in tf__predict_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
  File "/tmp/__autograph_generated_filemilvdugn.py", line 36, in tf__run_call_with_unpacked_inputs
    retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)
  File "/tmp/__autograph_generated_filei_g34srk.py", line 11, in tf__call
    outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)
  File "/tmp/__autograph_generated_filemilvdugn.py", line 36, in tf__run_call_with_unpacked_inputs
    retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)
  File "/tmp/__autograph_generated_filetdip3mph.py", line 75, in tf__call
    (batch_size, seq_length) = ag__.ld(input_shape)
ValueError: in user code:

    File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/engine/training.py", line 2137, in predict_function  *
        return step_function(self, iterator)
    File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/engine/training.py", line 2123, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/engine/training.py", line 2111, in run_step  **
        outputs = model.predict_step(data)
    File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/engine/training.py", line 2079, in predict_step
        return self(x, training=False)
    File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File "/tmp/__autograph_generated_filemilvdugn.py", line 36, in tf__run_call_with_unpacked_inputs
        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)
    File "/tmp/__autograph_generated_filei_g34srk.py", line 11, in tf__call
        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), encoder_hidden_states=ag__.ld(encoder_hidden_states), encoder_attention_mask=ag__.ld(encoder_attention_mask), past_key_values=ag__.ld(past_key_values), use_cache=ag__.ld(use_cache), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)
    File "/tmp/__autograph_generated_filemilvdugn.py", line 36, in tf__run_call_with_unpacked_inputs
        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)
    File "/tmp/__autograph_generated_filetdip3mph.py", line 75, in tf__call
        (batch_size, seq_length) = ag__.ld(input_shape)

    ValueError: Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).
    
    in user code:
    
        File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/transformers/modeling_tf_utils.py", line 1090, in run_call_with_unpacked_inputs  *
            return func(self, **unpacked_inputs)
        File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py", line 1118, in call  *
            outputs = self.bert(
        File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler  **
            raise e.with_traceback(filtered_tb) from None
        File "/tmp/__autograph_generated_filemilvdugn.py", line 36, in tf__run_call_with_unpacked_inputs
            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)
        File "/tmp/__autograph_generated_filetdip3mph.py", line 75, in tf__call
            (batch_size, seq_length) = ag__.ld(input_shape)
    
        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).
        
        in user code:
        
            File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/transformers/modeling_tf_utils.py", line 1090, in run_call_with_unpacked_inputs  *
                return func(self, **unpacked_inputs)
            File "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/transformers/models/bert/modeling_tf_bert.py", line 777, in call  *
                batch_size, seq_length = input_shape
        
            ValueError: not enough values to unpack (expected 2, got 1)
        
        
        Call arguments received by layer 'bert' (type TFBertMainLayer):
           self=tf.Tensor(shape=(None,), dtype=int64)
           input_ids=None
           attention_mask=None
           token_type_ids=None
           position_ids=None
           head_mask=None
           inputs_embeds=None
           encoder_hidden_states=None
           encoder_attention_mask=None
           past_key_values=None
           use_cache=True
           output_attentions=False
           output_hidden_states=False
           return_dict=True
           training=False
    
    
    Call arguments received by layer 'tf_bert_model' (type TFBertModel):
       self=tf.Tensor(shape=(None,), dtype=int32)
       input_ids=None
       attention_mask=None
       token_type_ids=None
       position_ids=None
       head_mask=None
       inputs_embeds=None
       encoder_hidden_states=None
       encoder_attention_mask=None
       past_key_values=None
       use_cache=None
       output_attentions=None
       output_hidden_states=None
       return_dict=None
       training=False

