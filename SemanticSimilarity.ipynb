{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import os\n",
    "\n",
    "gpu = np.argmin([g[\"mem_used_percent\"] for g in nvgpu.gpu_info()])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = TFAutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 174s 1s/step\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/RestaurantDataset/417c135d6d69672cd3dccf9f3d7bc369/ALL_DATA'\n",
    "all_data = pd.read_pickle(data_path)\n",
    "text_data = all_data[\"text\"]\n",
    "text_seqs = tokenizer.batch_encode_plus(text_data.tolist(),  padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "text_enco = text_seqs[\"input_ids\"].numpy()\n",
    "text_mask = text_seqs[\"attention_mask\"].numpy()\n",
    "\n",
    "text_enco_ds = tf.data.Dataset.from_tensor_slices(text_enco)\n",
    "text_mask_ds = tf.data.Dataset.from_tensor_slices(text_mask)\n",
    "\n",
    "text_dataset = tf.data.Dataset.zip((text_enco_ds, text_mask_ds))\n",
    "text_dataset = text_dataset.map(lambda x, y: ((x, y),))\n",
    "\n",
    "# text_dataset = text_enco_ds\n",
    "\n",
    "# Modelo\n",
    "text_in = tf.keras.Input(shape=(text_enco.shape[-1],), dtype='int32', name=\"in_text\")\n",
    "text_mask_in = tf.keras.Input(shape=(text_mask.shape[-1],), dtype='int32', name=\"in_mask\")\n",
    "\n",
    "# Un embedding por palabra\n",
    "token_embeddings = model((text_in, text_mask_in))[0]\n",
    "\n",
    "# Repetir la máscara para el tamaño del embedding\n",
    "input_mask_expanded = tf.expand_dims(text_mask_in, axis=-1)\n",
    "input_mask_expanded = tf.cast(input_mask_expanded, dtype=tf.float32)\n",
    "input_mask_expanded = tf.tile(input_mask_expanded, [1, 1, token_embeddings.shape[-1]])\n",
    "\n",
    "# Multiplicar cada embedding por la máscara (anular embeddings que sean 0)\n",
    "masked_token_embeddings = token_embeddings * input_mask_expanded\n",
    "# Sumar todos los embeddings de las palabras restantes en un solo vector\n",
    "summed_token_embeddings = tf.reduce_sum(masked_token_embeddings, axis=1)\n",
    "mask_sum = tf.reduce_sum(input_mask_expanded, axis=1)\n",
    "mask_sum = tf.maximum(mask_sum, 1e-9)\n",
    "\n",
    "mean_pooled_embeddings = summed_token_embeddings / mask_sum\n",
    "\n",
    "keras_model = tf.keras.models.Model(inputs=[text_in, text_mask_in], outputs=tf.reduce_sum(token_embeddings, axis=1), name=\"my_model\")\n",
    "\n",
    "batch_size = 300\n",
    "text_dataset = text_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "pred = keras_model.predict(text_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SIDRERIA CANDASU] -> uno ambiente agradable buen precio plato abundante el sidra mucho buen y el pincho gratis \n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Texto de ejemplo a codificar y buscar\n",
    "input_text = \"quiero comer un cachopo barato\"\n",
    "\n",
    "text_seq = tokenizer.batch_encode_plus([input_text],  max_length=text_enco.shape[-1], truncation=True, padding='max_length', return_tensors='tf')\n",
    "text_enco = text_seq[\"input_ids\"].numpy()\n",
    "text_mask = text_seq[\"attention_mask\"].numpy()\n",
    "text_encoded = keras_model.predict([text_enco, text_mask], verbose=0)\n",
    "\n",
    "distances = cdist(text_encoded, pred, metric='cosine')\n",
    "min_distance_idx = np.argmin(distances)\n",
    "print(f'[{all_data[\"name\"][min_distance_idx]}] -> {text_data[min_distance_idx]} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁this',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁sample',\n",
       " '▁phrase',\n",
       " '▁with',\n",
       " '▁eight',\n",
       " 'y',\n",
       " '▁words',\n",
       " '</s>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['this is a sample phrase with eighty words']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "# Compute token embeddings\n",
    "model_output = model(encoded_input)\n",
    "\n",
    "model_output[0][0].shape\n",
    "encoded_input[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " average_pooling2d_4 (Averag  (None, 16, 16, 32)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " average_pooling2d_5 (Averag  (None, 8, 8, 64)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8, 8, 2024)        131560    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8, 8, 1024)        2073600   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8, 8, 512)         524800    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8, 8, 1)           513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,796,041\n",
      "Trainable params: 2,796,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(32, 32,3)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(2024))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
