{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nas/pperez/miniconda3/envs/TAVtext/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-04 13:05:14.388445: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from src.Common import get_pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nvgpu\n",
    "import os\n",
    "\n",
    "gpu = np.argmin([g[\"mem_used_percent\"] for g in nvgpu.gpu_info()])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = TFAutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un tf.data a partir de train_dev o test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_tfdata(base_path, test=False, padding=None):\n",
    "    data_path = f'{base_path}/ALL_DATA'\n",
    "\n",
    "    all_data = pd.read_pickle(data_path)\n",
    "    if test: all_data = all_data[all_data[\"test\"]==1]\n",
    "    else: all_data = all_data[all_data[\"test\"]!=1]\n",
    "\n",
    "    text_data = all_data[\"text\"]\n",
    "    text_seqs = tokenizer.batch_encode_plus(text_data.tolist(), max_length=padding, padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "\n",
    "    text_enco = text_seqs[\"input_ids\"].numpy()\n",
    "    text_mask = text_seqs[\"attention_mask\"].numpy()\n",
    "\n",
    "    text_enco_ds = tf.data.Dataset.from_tensor_slices(text_enco)\n",
    "    text_mask_ds = tf.data.Dataset.from_tensor_slices(text_mask)\n",
    "\n",
    "    text_dataset = tf.data.Dataset.zip((text_enco_ds, text_mask_ds))\n",
    "    text_dataset = text_dataset.map(lambda x, y: ((x, y),))\n",
    "\n",
    "    return all_data, text_dataset, text_enco.shape[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener los embeddings de todos los textos que hay en train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959/959 [==============================] - 1322s 1s/step\n"
     ]
    }
   ],
   "source": [
    "base_path = \"data/RestaurantDataset/c82f2182f12c05a57df709ccac06cf14/\" # Gijón sin lematización\n",
    "base_path = \"data/RestaurantDataset/573535f43b572904548c57e8661d9e3a/\" # Barcelona sin lematización\n",
    "\n",
    "# padding = get_pickle(base_path, \"MAX_LEN_PADDING\") *2\n",
    "train_dev_data, train_dev_dataset, padding = pickle_to_tfdata(base_path)\n",
    "\n",
    "# Modelo\n",
    "text_in = tf.keras.Input(shape=(padding,), dtype='int32', name=\"in_text\")\n",
    "text_mask_in = tf.keras.Input(shape=(padding,), dtype='int32', name=\"in_mask\")\n",
    "\n",
    "# Un embedding por palabra\n",
    "token_embeddings = model((text_in, text_mask_in))[0]\n",
    "\n",
    "# Repetir la máscara para el tamaño del embedding\n",
    "input_mask_expanded = tf.expand_dims(text_mask_in, axis=-1)\n",
    "input_mask_expanded = tf.cast(input_mask_expanded, dtype=tf.float32)\n",
    "input_mask_expanded = tf.tile(input_mask_expanded, [1, 1, token_embeddings.shape[-1]])\n",
    "\n",
    "# Multiplicar cada embedding por la máscara (anular embeddings que sean 0)\n",
    "masked_token_embeddings = token_embeddings * input_mask_expanded\n",
    "# Sumar todos los embeddings de las palabras restantes en un solo vector\n",
    "summed_token_embeddings = tf.reduce_sum(masked_token_embeddings, axis=1)\n",
    "mask_sum = tf.reduce_sum(input_mask_expanded, axis=1)\n",
    "mask_sum = tf.maximum(mask_sum, 1e-9)\n",
    "\n",
    "mean_pooled_embeddings = summed_token_embeddings / mask_sum\n",
    "\n",
    "keras_model = tf.keras.models.Model(inputs=[text_in, text_mask_in], outputs=tf.reduce_sum(token_embeddings, axis=1), name=\"my_model\")\n",
    "\n",
    "batch_size = 300\n",
    "train_dev_dataset = train_dev_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "pred = keras_model.predict(train_dev_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 144s 1s/step\n"
     ]
    }
   ],
   "source": [
    "test_data, test_dataset, _= pickle_to_tfdata(base_path, test=True, padding=padding)\n",
    "test_dataset = test_dataset.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_preds = keras_model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31291/31291 [02:29<00:00, 209.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07987630767533206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_chunked, ndcg_score\n",
    "\n",
    "# Calcular distancias de forma paralela\n",
    "distances = pairwise_distances_chunked(test_preds, pred, metric='cosine', n_jobs=-1, working_memory=500)\n",
    "\n",
    "true_targets_list = []\n",
    "pred_item_ids_list = []\n",
    "\n",
    "tst_id = 0\n",
    "with tqdm(total=len(test_preds)) as pbar:\n",
    "    for dist_chunk in distances:\n",
    "        for dist_item in range(len(dist_chunk)):\n",
    "            distances = dist_chunk[dist_item]       \n",
    "            train_dev_data[\"cosine_prox\"] = 1-distances # El NDCG ordena de mayor a menor\n",
    "            pred_item_dist = train_dev_data.groupby(\"id_item\")[\"cosine_prox\"].mean().values\n",
    "\n",
    "            tst_item_id = test_data.iloc[tst_id][\"id_item\"]\n",
    "            true_target = np.zeros(len(pred_item_dist)) \n",
    "            true_target[tst_item_id]=1\n",
    "            true_targets_list.append(true_target)\n",
    "            pred_item_ids_list.append(pred_item_dist)\n",
    "        \n",
    "            tst_id+=1\n",
    "            pbar.update(1)\n",
    "\n",
    "true_targets = np.array(true_targets_list)\n",
    "pred_item_ids = np.array(pred_item_ids_list)\n",
    "\n",
    "ndcg = ndcg_score(true_targets, pred_item_ids, k=10)\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(y_true, y_pred_ranked, k):\n",
    "    # Obtener los índices de los k elementos principales\n",
    "    top_k_indices = y_pred_ranked[:k]\n",
    "    # Obtener las relevancias correspondientes a los índices\n",
    "    relevances = y_true[top_k_indices]\n",
    "    # Calcular los descuentos y la posición ideal de relevancia\n",
    "    discounts = np.log2(np.arange(2, len(relevances) + 2))\n",
    "    ideal_relevances = np.sort(y_true)[::-1]\n",
    "    ideal_dcg = np.sum(ideal_relevances[:k] / discounts)\n",
    "    # Calcular el nDCG\n",
    "    dcg = np.sum(relevances / discounts)\n",
    "    ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3934/3934 [01:22<00:00, 47.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17459540526516643"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "k = 10\n",
    "\n",
    "true_targets_list = []\n",
    "pred_item_ids_list = []\n",
    "\n",
    "for tst_id in tqdm(range(len(test_preds))):\n",
    "    text_encoded = test_preds[tst_id]\n",
    "\n",
    "    distances = cdist([text_encoded], pred, metric='cosine')\n",
    "    # sorted_ids = np.argsort(distances)[0][::-1] # Ordenamos de peor a mejor distancia\n",
    "    # pred_item_id = train_dev_data.iloc[sorted_ids][\"id_item\"].drop_duplicates().values\n",
    "    train_dev_data[\"cosine_prox\"] = 1-distances[0] # El NDCG ordena de mayor a menor\n",
    "    pred_item_dist = train_dev_data.groupby(\"id_item\")[\"cosine_prox\"].mean().values\n",
    "\n",
    "    tst_item_id = test_data.iloc[tst_id][\"id_item\"]\n",
    "\n",
    "    true_target = np.zeros(len(pred_item_dist)) \n",
    "    true_target[tst_item_id]=1\n",
    "    true_targets_list.append(true_target)\n",
    "    # true_targets_list.append(pred_item_id == tst_item_id)\n",
    "    pred_item_ids_list.append(pred_item_dist)\n",
    "\n",
    "true_targets = np.array(true_targets_list)\n",
    "pred_item_ids = np.array(pred_item_ids_list)\n",
    "\n",
    "# Calcular nDCG at 10 utilizando la función ndcg_score de scikit-learn\n",
    "ndcg_scores = ndcg_score(true_targets, pred_item_ids, k=k)\n",
    "ndcg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06][Living Barcelona 1925] -> solomillo de ternera con queso de cabra exelente sabor y presentacion sitio en la rambla de barelona para no perdelseo (Distancia: 0.06)\n",
      "[0.07][La Pizza del Born] -> les recomiendo la de perejil y queso de cabra o pesto la comida esta riquisima al igual que ka sangria gracias (Distancia: 0.07)\n",
      "[0.07][Bruc33Tapas] -> deliciosas tapas las croquetas de rabo exquisitas muy buena ensalada de queso de cabra y los dados de ternera suaves y jugosos buen lugar para repetir acogedor y agradable (Distancia: 0.07)\n",
      "[0.08][Bar Bodega l'Electricitat] -> pedimos ensalada de queso de cabra carpaccio de ternera y espinacas todo resulto estar riquisimo recomiendo el lugar (Distancia: 0.08)\n",
      "[0.08][Restaurant Vegetalia (Plaza Fossar de les Moreres)] -> la comida estupenda la fajita de espinacas con queso de cabra y el librito de seitan muy buenos buena atencion y trato un sitio para repetir sin duda (Distancia: 0.08)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Texto de ejemplo a codificar y buscar\n",
    "input_text = \"quiero comer un cachopo con cecina y queso de cabra\"\n",
    "\n",
    "text_seq = tokenizer.batch_encode_plus([input_text],  max_length=padding, truncation=True, padding='max_length', return_tensors='tf')\n",
    "text_enco = text_seq[\"input_ids\"].numpy()\n",
    "text_mask = text_seq[\"attention_mask\"].numpy()\n",
    "text_encoded = keras_model.predict([text_enco, text_mask], verbose=0)\n",
    "\n",
    "distances = cdist(text_encoded, pred, metric='cosine')\n",
    "min_distance_indices = np.argsort(distances)[0][:5]  # Obtiene los índices de los 5 elementos más cercanos\n",
    "\n",
    "for idx in min_distance_indices:\n",
    "    distance = round(distances[0][idx], 2)  # Redondea la distancia a 2 dígitos\n",
    "    print(f'[{distances[0][idx]:0.2f}][{train_dev_data[\"name\"][idx]}] -> {train_dev_data.iloc[idx][\"text\"]} (Distancia: {distance})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁this',\n",
       " '▁is',\n",
       " '▁a',\n",
       " '▁sample',\n",
       " '▁phrase',\n",
       " '▁with',\n",
       " '▁eight',\n",
       " 'y',\n",
       " '▁words',\n",
       " '</s>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['this is a sample phrase with eighty words']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "# Compute token embeddings\n",
    "model_output = model(encoded_input)\n",
    "\n",
    "model_output[0][0].shape\n",
    "encoded_input[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " average_pooling2d_4 (Averag  (None, 16, 16, 32)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " average_pooling2d_5 (Averag  (None, 8, 8, 64)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8, 8, 2024)        131560    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8, 8, 1024)        2073600   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8, 8, 512)         524800    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8, 8, 1)           513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,796,041\n",
      "Trainable params: 2,796,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(32, 32,3)))\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.AveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(2024))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001))\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAVtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
